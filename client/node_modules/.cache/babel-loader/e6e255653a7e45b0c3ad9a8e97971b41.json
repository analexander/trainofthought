{"ast":null,"code":"// import React, { Component } from \"react\"\n// import \"./face_expression_model-weights_manifest.json\"\n// import \"./tiny_face_detector_model-weights_manifest.json\"\n// class Video extends Component {\n//     render() {\n//         const main = async () => {\n//             // load the face detection model\n//             await faceapi.nets.tinyFaceDetector.load('/');\n//             // load the face expression detection model\n//             await faceapi.loadFaceExpressionModel('/');\n//             // get canvas and video elements\n//             const canvasElement = document.getElementById(\"overlay\");\n//             const videoElement = document.querySelector(\"video\");\n//             // get Webcam video stream\n//             const constraints = { audio: false, video: {} };\n//             const stream = await navigator.mediaDevices.getUserMedia(constraints);\n//             // what to do when the video stream is available\n//             // AKA facial recogition \n//             const onPlay = async () => {\n//                 const options = new faceapi.TinyFaceDetectorOptions({ inputSize: 512, scoreThreshold: 0.5 });\n//                 const result = await faceapi.detectSingleFace(videoElement, options).withFaceExpressions();\n//                 if (result) {\n//                     const dims = faceapi.matchDimensions(canvasElement, videoElement, true);\n//                     const resizedResult = faceapi.resizeResults(result, dims);\n//                     const minConfidence = 0.05;\n//                     faceapi.draw.drawDetections(canvasElement, resizedResult);\n//                     faceapi.draw.drawFaceExpressions(canvasElement, resizedResult, minConfidence);\n//                 }\n//                 setTimeout(() => onPlay());\n//             }\n//             videoElement.srcObject = stream;\n//             videoElement.onloadedmetadata = () => {\n//                 videoElement.play();\n//             };\n//             videoElement.onplay = onPlay;\n//         }        \n//     }\n// }\n// export default Video","map":{"version":3,"sources":["/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/src/component/facerec/Video.js"],"names":[],"mappings":"AAAA;AACA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AACA;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AAEA;AACA;AAEA;AACA;AACA;AACA;AACA;AAEA;AACA;AACA;AAEA","sourcesContent":["// import React, { Component } from \"react\"\n// import \"./face_expression_model-weights_manifest.json\"\n// import \"./tiny_face_detector_model-weights_manifest.json\"\n\n// class Video extends Component {\n//     render() {\n//         const main = async () => {\n//             // load the face detection model\n//             await faceapi.nets.tinyFaceDetector.load('/');\n//             // load the face expression detection model\n//             await faceapi.loadFaceExpressionModel('/');\n        \n//             // get canvas and video elements\n//             const canvasElement = document.getElementById(\"overlay\");\n//             const videoElement = document.querySelector(\"video\");\n        \n//             // get Webcam video stream\n//             const constraints = { audio: false, video: {} };\n//             const stream = await navigator.mediaDevices.getUserMedia(constraints);\n        \n//             // what to do when the video stream is available\n//             // AKA facial recogition \n//             const onPlay = async () => {\n        \n//                 const options = new faceapi.TinyFaceDetectorOptions({ inputSize: 512, scoreThreshold: 0.5 });\n//                 const result = await faceapi.detectSingleFace(videoElement, options).withFaceExpressions();\n        \n//                 if (result) {\n//                     const dims = faceapi.matchDimensions(canvasElement, videoElement, true);\n//                     const resizedResult = faceapi.resizeResults(result, dims);\n//                     const minConfidence = 0.05;\n//                     faceapi.draw.drawDetections(canvasElement, resizedResult);\n//                     faceapi.draw.drawFaceExpressions(canvasElement, resizedResult, minConfidence);\n//                 }\n        \n//                 setTimeout(() => onPlay());\n//             }\n        \n//             videoElement.srcObject = stream;\n//             videoElement.onloadedmetadata = () => {\n//                 videoElement.play();\n//             };\n//             videoElement.onplay = onPlay;\n        \n//         }        \n//     }\n// }\n\n// export default Video"]},"metadata":{},"sourceType":"module"}