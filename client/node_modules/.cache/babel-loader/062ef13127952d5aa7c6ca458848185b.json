{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Optimizers.\n */\nimport { train } from '@tensorflow/tfjs-core';\nimport { epsilon } from './backend/common';\nimport { ValueError } from './errors'; // Add (de)serialize()\n// Porting note: This diverges from the PyKeras implementation and may need to\n// change based on (de)serialization requirements.\n\nexport function getOptimizer(identifier) {\n  var optimizerMap = {\n    'Adagrad': function Adagrad() {\n      return train.adagrad(0.01);\n    },\n    'Adadelta': function Adadelta() {\n      return train.adadelta(1, 0.95, epsilon());\n    },\n    'Adam': function Adam() {\n      return train.adam(0.001, 0.9, 0.999, epsilon());\n    },\n    'Adamax': function Adamax() {\n      return train.adamax(0.002, 0.9, 0.999, epsilon(), 0);\n    },\n    'RMSProp': function RMSProp() {\n      return train.rmsprop(0.001, 0.9, 0, epsilon());\n    },\n    'SGD': function SGD() {\n      return train.sgd(0.01);\n    }\n  };\n  optimizerMap['adagrad'] = optimizerMap['Adagrad'];\n  optimizerMap['adadelta'] = optimizerMap['Adadelta'];\n  optimizerMap['adam'] = optimizerMap['Adam'];\n  optimizerMap['adamax'] = optimizerMap['Adamax'];\n  optimizerMap['rmsprop'] = optimizerMap['RMSProp'];\n  optimizerMap['sgd'] = optimizerMap['SGD'];\n\n  if (identifier in optimizerMap) {\n    return optimizerMap[identifier]();\n  }\n\n  throw new ValueError(\"Unknown Optimizer \".concat(identifier));\n}","map":null,"metadata":{},"sourceType":"module"}