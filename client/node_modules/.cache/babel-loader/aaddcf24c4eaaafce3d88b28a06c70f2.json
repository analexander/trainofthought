{"ast":null,"code":"import _regeneratorRuntime from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/regenerator\";\nimport _asyncToGenerator from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\nimport _classCallCheck from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass\";\nimport _possibleConstructorReturn from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";\nimport _getPrototypeOf from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";\nimport _inherits from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/inherits\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { dispose as _dispose } from '../globals';\nimport { variableGrads } from '../gradients';\nimport { scalar } from '../ops/ops';\nimport { Serializable } from '../serialization';\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\n\nexport var Optimizer = /*#__PURE__*/function (_Serializable) {\n  _inherits(Optimizer, _Serializable);\n\n  function Optimizer() {\n    _classCallCheck(this, Optimizer);\n\n    return _possibleConstructorReturn(this, _getPrototypeOf(Optimizer).apply(this, arguments));\n  }\n\n  _createClass(Optimizer, [{\n    key: \"minimize\",\n\n    /**\n     * Executes `f()` and minimizes the scalar output of `f()` by computing\n     * gradients of y with respect to the list of trainable variables provided by\n     * `varList`. If no list is provided, it defaults to all trainable variables.\n     *\n     * @param f The function to execute and whose output to minimize.\n     * @param returnCost Whether to return the scalar cost value produced by\n     * executing `f()`.\n     * @param varList An optional list of variables to update. If specified, only\n     * the trainable variables in varList will be updated by minimize. Defaults to\n     * all trainable variables.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\n     */\n    value: function minimize(f) {\n      var returnCost = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : false;\n      var varList = arguments.length > 2 ? arguments[2] : undefined;\n\n      var _this$computeGradient = this.computeGradients(f, varList),\n          value = _this$computeGradient.value,\n          grads = _this$computeGradient.grads;\n\n      if (varList != null) {\n        var gradArray = varList.map(function (v) {\n          return {\n            name: v.name,\n            tensor: grads[v.name]\n          };\n        });\n        this.applyGradients(gradArray);\n      } else {\n        this.applyGradients(grads);\n      } // Dispose gradients.\n\n\n      _dispose(grads);\n\n      if (returnCost) {\n        return value;\n      } else {\n        value.dispose();\n        return null;\n      }\n    }\n    /**\n     * The number of iterations that this optimizer instance has been invoked for.\n     */\n\n  }, {\n    key: \"incrementIterations\",\n    value: function incrementIterations() {\n      this.iterations_ = this.iterations + 1;\n    }\n    /**\n     * Executes f() and computes the gradient of the scalar output of f() with\n     * respect to the list of trainable variables provided by `varList`. If no\n     * list is provided, it defaults to all trainable variables.\n     *\n     * @param f The function to execute and whose output to use for computing\n     * gradients with respect to variables.\n     * @param varList An optional list of variables to compute gradients with\n     * respect to. If specified, only the trainable variables in varList will have\n     * gradients computed with respect to. Defaults to all trainable variables.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\n     */\n\n  }, {\n    key: \"computeGradients\",\n    value: function computeGradients(f, varList) {\n      return variableGrads(f, varList);\n    }\n    /**\n     * Dispose the variables (if any) owned by this optimizer instance.\n     */\n\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      if (this.iterations_ != null) {\n        _dispose(this.iterations_);\n      }\n    }\n  }, {\n    key: \"saveIterations\",\n    value: function () {\n      var _saveIterations = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee() {\n        return _regeneratorRuntime.wrap(function _callee$(_context) {\n          while (1) {\n            switch (_context.prev = _context.next) {\n              case 0:\n                if (this.iterations_ == null) {\n                  this.iterations_ = 0;\n                }\n\n                return _context.abrupt(\"return\", {\n                  name: 'iter',\n                  // TODO(cais): Use 'int64' type when available.\n                  tensor: scalar(this.iterations_, 'int32')\n                });\n\n              case 2:\n              case \"end\":\n                return _context.stop();\n            }\n          }\n        }, _callee, this);\n      }));\n\n      function saveIterations() {\n        return _saveIterations.apply(this, arguments);\n      }\n\n      return saveIterations;\n    }()\n  }, {\n    key: \"getWeights\",\n    value: function () {\n      var _getWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2() {\n        return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n          while (1) {\n            switch (_context2.prev = _context2.next) {\n              case 0:\n                throw new Error('getWeights() is not implemented for this optimizer yet.');\n\n              case 1:\n              case \"end\":\n                return _context2.stop();\n            }\n          }\n        }, _callee2);\n      }));\n\n      function getWeights() {\n        return _getWeights.apply(this, arguments);\n      }\n\n      return getWeights;\n    }()\n  }, {\n    key: \"setWeights\",\n    value: function () {\n      var _setWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee3(weightValues) {\n        return _regeneratorRuntime.wrap(function _callee3$(_context3) {\n          while (1) {\n            switch (_context3.prev = _context3.next) {\n              case 0:\n                throw new Error(\"setWeights() is not implemented for this optimizer class \" + \"\".concat(this.getClassName()));\n\n              case 1:\n              case \"end\":\n                return _context3.stop();\n            }\n          }\n        }, _callee3, this);\n      }));\n\n      function setWeights(_x) {\n        return _setWeights.apply(this, arguments);\n      }\n\n      return setWeights;\n    }()\n    /**\n     * Extract the first element of the weight values and set it\n     * as the iterations counter variable of this instance of optimizer.\n     *\n     * @param weightValues\n     * @returns Weight values with the first element consumed and excluded.\n     */\n\n  }, {\n    key: \"extractIterations\",\n    value: function () {\n      var _extractIterations = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee4(weightValues) {\n        return _regeneratorRuntime.wrap(function _callee4$(_context4) {\n          while (1) {\n            switch (_context4.prev = _context4.next) {\n              case 0:\n                _context4.next = 2;\n                return weightValues[0].tensor.data();\n\n              case 2:\n                this.iterations_ = _context4.sent[0];\n                return _context4.abrupt(\"return\", weightValues.slice(1));\n\n              case 4:\n              case \"end\":\n                return _context4.stop();\n            }\n          }\n        }, _callee4, this);\n      }));\n\n      function extractIterations(_x2) {\n        return _extractIterations.apply(this, arguments);\n      }\n\n      return extractIterations;\n    }()\n  }, {\n    key: \"iterations\",\n    get: function get() {\n      if (this.iterations_ == null) {\n        this.iterations_ = 0;\n      }\n\n      return this.iterations_;\n    }\n  }]);\n\n  return Optimizer;\n}(Serializable);\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n  value: function value(instance) {\n    return instance.minimize != null && instance.computeGradients != null && instance.applyGradients != null;\n  }\n});","map":null,"metadata":{},"sourceType":"module"}