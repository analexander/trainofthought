{"ast":null,"code":"import _toConsumableArray from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/toConsumableArray\";\nimport _possibleConstructorReturn from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";\nimport _getPrototypeOf from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";\nimport _inherits from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/inherits\";\nimport _createClass from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass\";\nimport _classCallCheck from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\n\nfunction _createForOfIteratorHelper(o, allowArrayLike) { var it; if (typeof Symbol === \"undefined\" || o[Symbol.iterator] == null) { if (Array.isArray(o) || (it = _unsupportedIterableToArray(o)) || allowArrayLike && o && typeof o.length === \"number\") { if (it) o = it; var i = 0; var F = function F() {}; return { s: F, n: function n() { if (i >= o.length) return { done: true }; return { done: false, value: o[i++] }; }, e: function e(_e) { throw _e; }, f: F }; } throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\"); } var normalCompletion = true, didErr = false, err; return { s: function s() { it = o[Symbol.iterator](); }, n: function n() { var step = it.next(); normalCompletion = step.done; return step; }, e: function e(_e2) { didErr = true; err = _e2; }, f: function f() { try { if (!normalCompletion && it.return != null) it.return(); } finally { if (didErr) throw err; } } }; }\n\nfunction _unsupportedIterableToArray(o, minLen) { if (!o) return; if (typeof o === \"string\") return _arrayLikeToArray(o, minLen); var n = Object.prototype.toString.call(o).slice(8, -1); if (n === \"Object\" && o.constructor) n = o.constructor.name; if (n === \"Map\" || n === \"Set\") return Array.from(o); if (n === \"Arguments\" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return _arrayLikeToArray(o, minLen); }\n\nfunction _arrayLikeToArray(arr, len) { if (len == null || len > arr.length) len = arr.length; for (var i = 0, arr2 = new Array(len); i < len; i++) { arr2[i] = arr[i]; } return arr2; }\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/* Original source: keras/engine/topology.py */\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getNextUniqueTensorId, getUid } from '../backend/state';\nimport { getScopedTensorName, getUniqueTensorName, nameScope } from '../common';\nimport { AttributeError, NotImplementedError, RuntimeError, ValueError } from '../errors';\nimport { getInitializer } from '../initializers';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as types_utils from '../utils/types_utils';\nimport * as variable_utils from '../utils/variable_utils';\nimport { batchGetValue, batchSetValue, LayerVariable } from '../variables';\n/**\n * Specifies the ndim, dtype and shape of every input to a layer.\n *\n * Every layer should expose (if appropriate) an `inputSpec` attribute:\n * a list of instances of InputSpec (one per input tensor).\n *\n * A null entry in a shape is compatible with any dimension,\n * a null shape is compatible with any shape.\n */\n\nexport var InputSpec = function InputSpec(args) {\n  _classCallCheck(this, InputSpec);\n\n  this.dtype = args.dtype;\n  this.shape = args.shape;\n  /*\n    TODO(michaelterry): Could throw error if ndim and shape are both defined\n      (then backport).\n  */\n\n  if (args.shape != null) {\n    this.ndim = args.shape.length;\n  } else {\n    this.ndim = args.ndim;\n  }\n\n  this.maxNDim = args.maxNDim;\n  this.minNDim = args.minNDim;\n  this.axes = args.axes || {};\n};\n/**\n * `tf.SymbolicTensor` is a placeholder for a Tensor without any concrete value.\n *\n * They are most often encountered when building a graph of `Layer`s for a\n * a `tf.LayersModel` and the input data's shape, but not values are known.\n *\n * @doc {heading: 'Models', 'subheading': 'Classes'}\n */\n\nexport var SymbolicTensor =\n/**\n *\n * @param dtype\n * @param shape\n * @param sourceLayer The Layer that produced this symbolic tensor.\n * @param inputs The inputs passed to sourceLayer's __call__() method.\n * @param nodeIndex\n * @param tensorIndex\n * @param callArgs The keyword arguments passed to the __call__() method.\n * @param name\n * @param outputTensorIndex The index of this tensor in the list of outputs\n *   returned by apply().\n */\nfunction SymbolicTensor(dtype, shape, sourceLayer, inputs, callArgs, name, outputTensorIndex) {\n  _classCallCheck(this, SymbolicTensor);\n\n  this.dtype = dtype;\n  this.shape = shape;\n  this.sourceLayer = sourceLayer;\n  this.inputs = inputs;\n  this.callArgs = callArgs;\n  this.outputTensorIndex = outputTensorIndex;\n  this.id = getNextUniqueTensorId();\n\n  if (name != null) {\n    this.originalName = getScopedTensorName(name);\n    this.name = getUniqueTensorName(this.originalName);\n  }\n\n  this.rank = shape.length;\n};\nvar _nextNodeID = 0;\n/**\n * A `Node` describes the connectivity between two layers.\n *\n * Each time a layer is connected to some new input,\n * a node is added to `layer.inboundNodes`.\n *\n * Each time the output of a layer is used by another layer,\n * a node is added to `layer.outboundNodes`.\n *\n * `nodeIndices` and `tensorIndices` are basically fine-grained coordinates\n * describing the origin of the `inputTensors`, verifying the following:\n *\n * `inputTensors[i] ==\n * inboundLayers[i].inboundNodes[nodeIndices[i]].outputTensors[\n *   tensorIndices[i]]`\n *\n * A node from layer A to layer B is added to:\n *     A.outboundNodes\n *     B.inboundNodes\n */\n\nexport var Node = /*#__PURE__*/function () {\n  function Node(args, // TODO(michaelterry): Define actual type for this.\n  callArgs) {\n    _classCallCheck(this, Node);\n\n    this.callArgs = callArgs;\n    this.id = _nextNodeID++;\n    /*\n      Layer instance (NOT a list).\n      this is the layer that takes a list of input tensors\n      and turns them into a list of output tensors.\n      the current node will be added to\n      the inboundNodes of outboundLayer.\n    */\n\n    this.outboundLayer = args.outboundLayer;\n    /*\n        The following 3 properties describe where\n        the input tensors come from: which layers,\n        and for each layer, which node and which\n        tensor output of each node.\n    */\n    // List of layer instances.\n\n    this.inboundLayers = args.inboundLayers; // List of integers, 1:1 mapping with inboundLayers.\n\n    this.nodeIndices = args.nodeIndices; // List of integers, 1:1 mapping with inboundLayers.\n\n    this.tensorIndices = args.tensorIndices;\n    /*\n        Following 2 properties:\n        tensor inputs and outputs of outboundLayer.\n    */\n    // List of tensors. 1:1 mapping with inboundLayers.\n\n    this.inputTensors = args.inputTensors; // List of tensors, created by outboundLayer.call().\n\n    this.outputTensors = args.outputTensors;\n    /*\n        Following 2 properties: input and output masks.\n        List of tensors, 1:1 mapping with inputTensor.\n    */\n\n    this.inputMasks = args.inputMasks; // List of tensors, created by outboundLayer.computeMask().\n\n    this.outputMasks = args.outputMasks; // Following 2 properties: input and output shapes.\n    // List of shape tuples, shapes of inputTensors.\n\n    this.inputShapes = args.inputShapes; // List of shape tuples, shapes of outputTensors.\n\n    this.outputShapes = args.outputShapes; // Add nodes to all layers involved.\n\n    var _iterator = _createForOfIteratorHelper(args.inboundLayers),\n        _step;\n\n    try {\n      for (_iterator.s(); !(_step = _iterator.n()).done;) {\n        var layer = _step.value;\n\n        if (layer != null) {\n          layer.outboundNodes.push(this);\n        }\n      }\n    } catch (err) {\n      _iterator.e(err);\n    } finally {\n      _iterator.f();\n    }\n\n    args.outboundLayer.inboundNodes.push(this);\n  }\n\n  _createClass(Node, [{\n    key: \"getConfig\",\n    value: function getConfig() {\n      var inboundNames = [];\n\n      var _iterator2 = _createForOfIteratorHelper(this.inboundLayers),\n          _step2;\n\n      try {\n        for (_iterator2.s(); !(_step2 = _iterator2.n()).done;) {\n          var layer = _step2.value;\n\n          if (layer != null) {\n            inboundNames.push(layer.name);\n          } else {\n            inboundNames.push(null);\n          }\n        }\n      } catch (err) {\n        _iterator2.e(err);\n      } finally {\n        _iterator2.f();\n      }\n\n      return {\n        outboundLayer: this.outboundLayer ? this.outboundLayer.name : null,\n        inboundLayers: inboundNames,\n        nodeIndices: this.nodeIndices,\n        tensorIndices: this.tensorIndices\n      };\n    }\n  }]);\n\n  return Node;\n}();\nvar _nextLayerID = 0;\n/**\n * A layer is a grouping of operations and weights that can be composed to\n * create a `tf.LayersModel`.\n *\n * Layers are constructed by using the functions under the\n * [tf.layers](#Layers-Basic) namespace.\n *\n * @doc {heading: 'Layers', subheading: 'Classes', namespace: 'layers'}\n */\n\nexport var Layer = /*#__PURE__*/function (_serialization$Serial) {\n  _inherits(Layer, _serialization$Serial);\n\n  function Layer() {\n    var _this;\n\n    var args = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : {};\n\n    _classCallCheck(this, Layer);\n\n    _this = _possibleConstructorReturn(this, _getPrototypeOf(Layer).call(this));\n    _this._callHook = null;\n    _this._addedWeightNames = []; // Porting Notes: PyKeras does not have this property in this base Layer\n    //   class. Instead lets Layer subclass set it dynamically and checks the\n    //   value with `hasattr`. In tfjs-layers, we let this be a member of this\n    //   base class.\n\n    _this._stateful = false;\n    _this.id = _nextLayerID++;\n    _this.activityRegularizer = null;\n    _this.inputSpec = null;\n    _this.supportsMasking = false; // These properties will be set upon call of this.build()\n\n    _this._trainableWeights = [];\n    _this._nonTrainableWeights = [];\n    _this._losses = [];\n    _this._updates = [];\n    _this._built = false;\n    /*\n      These lists will be filled via successive calls\n      to this.addInboundNode().\n     */\n\n    _this.inboundNodes = [];\n    _this.outboundNodes = [];\n    var name = args.name;\n\n    if (!name) {\n      var prefix = _this.getClassName();\n\n      name = generic_utils.toSnakeCase(prefix) + '_' + getUid(prefix);\n    }\n\n    _this.name = name;\n    _this.trainable_ = args.trainable == null ? true : args.trainable;\n\n    if (args.inputShape != null || args.batchInputShape != null) {\n      /*\n        In this case we will later create an input layer\n        to insert before the current layer\n       */\n      var batchInputShape;\n\n      if (args.batchInputShape != null) {\n        batchInputShape = args.batchInputShape;\n      } else if (args.inputShape != null) {\n        var batchSize = null;\n\n        if (args.batchSize != null) {\n          batchSize = args.batchSize;\n        }\n\n        batchInputShape = [batchSize].concat(args.inputShape);\n      }\n\n      _this.batchInputShape = batchInputShape; // Set dtype.\n\n      var dtype = args.dtype;\n\n      if (dtype == null) {\n        dtype = args.inputDType;\n      }\n\n      if (dtype == null) {\n        dtype = 'float32';\n      }\n\n      _this.dtype = dtype;\n    }\n\n    if (args.weights != null) {\n      _this.initialWeights = args.weights;\n    } else {\n      _this.initialWeights = null;\n    } // The value of `_refCount` is initialized to null. When the layer is used\n    // in a symbolic way for the first time, it will be set to 1.\n\n\n    _this._refCount = null;\n    _this.fastWeightInitDuringBuild = false;\n    return _this;\n  }\n  /**\n   * Converts a layer and its index to a unique (immutable type) name.\n   * This function is used internally with `this.containerNodes`.\n   * @param layer The layer.\n   * @param nodeIndex The layer's position (e.g. via enumerate) in a list of\n   *   nodes.\n   *\n   * @returns The unique name.\n   */\n\n\n  _createClass(Layer, [{\n    key: \"getNodeAtIndex\",\n\n    /**\n     * Returns this.inboundNode at index nodeIndex.\n     *\n     * Porting note: This is a replacement for _get_node_attribute_at_index()\n     * @param nodeIndex\n     * @param attrName The name of the attribute related to request for this node.\n     */\n    value: function getNodeAtIndex(nodeIndex, attrName) {\n      if (this.inboundNodes.length === 0) {\n        throw new RuntimeError('The layer has never been called ' + \"and thus has no defined \".concat(attrName, \".\"));\n      }\n\n      if (this.inboundNodes.length <= nodeIndex) {\n        throw new ValueError(\"Asked to get \".concat(attrName, \" at node \").concat(nodeIndex, \", \") + \"but the layer has only \".concat(this.inboundNodes.length, \" inbound nodes.\"));\n      }\n\n      return this.inboundNodes[nodeIndex];\n    }\n    /**\n     * Retrieves the input tensor(s) of a layer at a given node.\n     *\n     * @param nodeIndex Integer, index of the node from which to retrieve the\n     *   attribute. E.g. `nodeIndex=0` will correspond to the first time the layer\n     *   was called.\n     *\n     * @return A tensor (or list of tensors if the layer has multiple inputs).\n     */\n\n  }, {\n    key: \"getInputAt\",\n    value: function getInputAt(nodeIndex) {\n      return generic_utils.singletonOrArray(this.getNodeAtIndex(nodeIndex, 'input').inputTensors);\n    }\n    /**\n     * Retrieves the output tensor(s) of a layer at a given node.\n     *\n     * @param nodeIndex Integer, index of the node from which to retrieve the\n     *   attribute. E.g. `nodeIndex=0` will correspond to the first time the layer\n     *   was called.\n     *\n     * @return A tensor (or list of tensors if the layer has multiple outputs).\n     */\n\n  }, {\n    key: \"getOutputAt\",\n    value: function getOutputAt(nodeIndex) {\n      return generic_utils.singletonOrArray(this.getNodeAtIndex(nodeIndex, 'output').outputTensors);\n    } // Properties\n\n    /**\n     * Retrieves the input tensor(s) of a layer.\n     *\n     * Only applicable if the layer has exactly one inbound node,\n     * i.e. if it is connected to one incoming layer.\n     *\n     * @return Input tensor or list of input tensors.\n     *\n     * @exception AttributeError if the layer is connected to more than one\n     *   incoming layers.\n     */\n\n  }, {\n    key: \"calculateLosses\",\n\n    /**\n     * Retrieves the Layer's current loss values.\n     *\n     * Used for regularizers during training.\n     */\n    value: function calculateLosses() {\n      // Porting Node: This is an augmentation to Layer.loss in PyKeras.\n      //   In PyKeras, Layer.loss returns symbolic tensors. Here a concrete\n      //   Tensor (specifically Scalar) values are returned. This is due to the\n      //   imperative backend.\n      return this.losses.map(function (lossFn) {\n        return lossFn();\n      });\n    }\n  }, {\n    key: \"resetStates\",\n\n    /**\n     * Reset the states of the layer.\n     *\n     * This method of the base Layer class is essentially a no-op.\n     * Subclasses that are stateful (e.g., stateful RNNs) should override this\n     * method.\n     */\n    value: function resetStates() {\n      if (!this.stateful) {\n        throw new Error('Cannot call the resetStates() method of a non-stateful Layer ' + 'object.');\n      }\n    }\n    /**\n     * Checks compatibility between the layer and provided inputs.\n     *\n     * This checks that the tensor(s) `input`\n     * verify the input assumptions of the layer\n     * (if any). If not, exceptions are raised.\n     *\n     * @param inputs Input tensor or list of input tensors.\n     *\n     * @exception ValueError in case of mismatch between\n     *   the provided inputs and the expectations of the layer.\n     */\n\n  }, {\n    key: \"assertInputCompatibility\",\n    value: function assertInputCompatibility(inputs) {\n      inputs = generic_utils.toList(inputs);\n\n      if (this.inputSpec == null || this.inputSpec.length === 0) {\n        return;\n      }\n\n      var inputSpec = generic_utils.toList(this.inputSpec);\n\n      if (inputs.length !== inputSpec.length) {\n        throw new ValueError(\"Layer \".concat(this.name, \" expects \").concat(inputSpec.length, \" inputs, \") + \"but it received \".concat(inputs.length, \" input tensors. \") + \"Input received: \".concat(inputs));\n      }\n\n      for (var inputIndex = 0; inputIndex < inputs.length; inputIndex++) {\n        var x = inputs[inputIndex];\n        var spec = inputSpec[inputIndex];\n\n        if (spec == null) {\n          continue;\n        } // Check ndim.\n\n\n        var ndim = x.rank;\n\n        if (spec.ndim != null) {\n          if (ndim !== spec.ndim) {\n            throw new ValueError(\"Input \".concat(inputIndex, \" is incompatible with layer \").concat(this.name, \": \") + \"expected ndim=\".concat(spec.ndim, \", found ndim=\").concat(ndim));\n          }\n        }\n\n        if (spec.maxNDim != null) {\n          if (ndim > spec.maxNDim) {\n            throw new ValueError(\"Input \".concat(inputIndex, \" is incompatible with layer \").concat(this.name) + \": expected max_ndim=\".concat(spec.maxNDim, \", found ndim=\").concat(ndim));\n          }\n        }\n\n        if (spec.minNDim != null) {\n          if (ndim < spec.minNDim) {\n            throw new ValueError(\"Input \".concat(inputIndex, \" is incompatible with layer \").concat(this.name) + \": expected min_ndim=\".concat(spec.minNDim, \", found ndim=\").concat(ndim, \".\"));\n          }\n        } // Check dtype.\n\n\n        if (spec.dtype != null) {\n          if (x.dtype !== spec.dtype) {\n            throw new ValueError(\"Input \".concat(inputIndex, \" is incompatible with layer \").concat(this.name, \" \") + \": expected dtype=\".concat(spec.dtype, \", found dtype=\").concat(x.dtype, \".\"));\n          }\n        } // Check specific shape axes.\n\n\n        if (spec.axes) {\n          var xShape = x.shape;\n\n          for (var key in spec.axes) {\n            var axis = Number(key);\n            var value = spec.axes[key]; // Perform Python-style slicing in case axis < 0;\n            // TODO(cais): Use https://github.com/alvivi/typescript-underscore to\n            // ensure type safety through Underscore calls.\n\n            var xShapeAtAxis = axis >= 0 ? xShape[axis] : xShape[xShape.length + axis];\n\n            if (value != null && [value, null].indexOf(xShapeAtAxis) === -1) {\n              throw new ValueError(\"Input \".concat(inputIndex, \" is incompatible with layer \") + \"\".concat(this.name, \": expected axis \").concat(axis, \" of input shape to \") + \"have value \".concat(value, \" but got shape \").concat(xShape, \".\"));\n            }\n          }\n        } // Check shape.\n\n\n        if (spec.shape != null) {\n          for (var i = 0; i < spec.shape.length; ++i) {\n            var specDim = spec.shape[i];\n            var dim = x.shape[i];\n\n            if (specDim != null && dim != null) {\n              if (specDim !== dim) {\n                throw new ValueError(\"Input \".concat(inputIndex, \" is incompatible with layer \") + \"\".concat(this.name, \": expected shape=\").concat(spec.shape, \", \") + \"found shape=\".concat(x.shape, \".\"));\n              }\n            }\n          }\n        }\n      }\n    }\n    /**\n     * This is where the layer's logic lives.\n     *\n     * @param inputs Input tensor, or list/tuple of input tensors.\n     * @param kwargs Additional keyword arguments.\n     *\n     * @return A tensor or list/tuple of tensors.\n     */\n\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      return inputs;\n    }\n  }, {\n    key: \"invokeCallHook\",\n    value: function invokeCallHook(inputs, kwargs) {\n      if (this._callHook != null) {\n        this._callHook(inputs, kwargs);\n      }\n    }\n    /**\n     * Set call hook.\n     * This is currently used for testing only.\n     * @param callHook\n     */\n\n  }, {\n    key: \"setCallHook\",\n    value: function setCallHook(callHook) {\n      this._callHook = callHook;\n    }\n    /**\n     * Clear call hook.\n     * This is currently used for testing only.\n     */\n\n  }, {\n    key: \"clearCallHook\",\n    value: function clearCallHook() {\n      this._callHook = null;\n    }\n    /**\n     * Builds or executes a `Layer's logic.\n     *\n     * When called with `tf.Tensor`(s), execute the `Layer`s computation and\n     * return Tensor(s). For example:\n     *\n     * ```js\n     * const denseLayer = tf.layers.dense({\n     *   units: 1,\n     *   kernelInitializer: 'zeros',\n     *   useBias: false\n     * });\n     *\n     * // Invoke the layer's apply() method with a `tf.Tensor` (with concrete\n     * // numeric values).\n     * const input = tf.ones([2, 2]);\n     * const output = denseLayer.apply(input);\n     *\n     * // The output's value is expected to be [[0], [0]], due to the fact that\n     * // the dense layer has a kernel initialized to all-zeros and does not have\n     * // a bias.\n     * output.print();\n     * ```\n     *\n     * When called with `tf.SymbolicTensor`(s), this will prepare the layer for\n     * future execution.  This entails internal book-keeping on shapes of\n     * expected Tensors, wiring layers together, and initializing weights.\n     *\n     * Calling `apply` with `tf.SymbolicTensor`s are typically used during the\n     * building of non-`tf.Sequential` models. For example:\n     *\n     * ```js\n     * const flattenLayer = tf.layers.flatten();\n     * const denseLayer = tf.layers.dense({units: 1});\n     *\n     * // Use tf.layers.input() to obtain a SymbolicTensor as input to apply().\n     * const input = tf.input({shape: [2, 2]});\n     * const output1 = flattenLayer.apply(input);\n     *\n     * // output1.shape is [null, 4]. The first dimension is the undetermined\n     * // batch size. The second dimension comes from flattening the [2, 2]\n     * // shape.\n     * console.log(JSON.stringify(output1.shape));\n     *\n     * // The output SymbolicTensor of the flatten layer can be used to call\n     * // the apply() of the dense layer:\n     * const output2 = denseLayer.apply(output1);\n     *\n     * // output2.shape is [null, 1]. The first dimension is the undetermined\n     * // batch size. The second dimension matches the number of units of the\n     * // dense layer.\n     * console.log(JSON.stringify(output2.shape));\n     *\n     * // The input and output and be used to construct a model that consists\n     * // of the flatten and dense layers.\n     * const model = tf.model({inputs: input, outputs: output2});\n     * ```\n     *\n     * @param inputs a `tf.Tensor` or `tf.SymbolicTensor` or an Array of them.\n     * @param kwargs Additional keyword arguments to be passed to `call()`.\n     *\n     * @return Output of the layer's `call` method.\n     *\n     * @exception ValueError error in case the layer is missing shape information\n     *   for its `build` call.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n    // Porting Note: This is a replacement for __call__() in Python.\n\n  }, {\n    key: \"apply\",\n    value: function apply(inputs, kwargs) {\n      var _this2 = this;\n\n      kwargs = kwargs || {};\n      this.assertNotDisposed(); // Ensure inputs are all the same type.\n\n      var inputsList = generic_utils.toList(inputs);\n      var allAreSymbolic = true;\n\n      var _iterator3 = _createForOfIteratorHelper(inputsList),\n          _step3;\n\n      try {\n        for (_iterator3.s(); !(_step3 = _iterator3.n()).done;) {\n          var input = _step3.value;\n\n          if (!(input instanceof SymbolicTensor)) {\n            allAreSymbolic = false;\n            break;\n          }\n        }\n      } catch (err) {\n        _iterator3.e(err);\n      } finally {\n        _iterator3.f();\n      }\n\n      var noneAreSymbolic = true;\n\n      var _iterator4 = _createForOfIteratorHelper(inputsList),\n          _step4;\n\n      try {\n        for (_iterator4.s(); !(_step4 = _iterator4.n()).done;) {\n          var _input = _step4.value;\n\n          if (_input instanceof SymbolicTensor) {\n            noneAreSymbolic = false;\n            break;\n          }\n        }\n      } catch (err) {\n        _iterator4.e(err);\n      } finally {\n        _iterator4.f();\n      }\n\n      if (allAreSymbolic === noneAreSymbolic) {\n        throw new ValueError('Arguments to apply() must be all ' + 'SymbolicTensors or all Tensors');\n      } // TODO(michaelterry): nameScope() may not be necessary.\n\n\n      return nameScope(this.name, function () {\n        // Handle laying building (weight creating, input spec locking).\n        if (!_this2.built) {\n          /*\n            Throw exceptions in case the input is not compatible\n            with the inputSpec specified in the layer constructor.\n           */\n          _this2.assertInputCompatibility(inputs); // Collect input shapes to build layer.\n\n\n          var inputShapes = [];\n\n          var _iterator5 = _createForOfIteratorHelper(generic_utils.toList(inputs)),\n              _step5;\n\n          try {\n            for (_iterator5.s(); !(_step5 = _iterator5.n()).done;) {\n              var xElem = _step5.value;\n              inputShapes.push(xElem.shape);\n            }\n          } catch (err) {\n            _iterator5.e(err);\n          } finally {\n            _iterator5.f();\n          }\n\n          _this2.build(generic_utils.singletonOrArray(inputShapes));\n\n          _this2.built = true; // Load weights that were specified at layer instantiation.\n\n          if (_this2.initialWeights) {\n            _this2.setWeights(_this2.initialWeights);\n          }\n\n          if (_this2._refCount === null && noneAreSymbolic) {\n            // The first use of this layer is a non-symbolic call, set ref count\n            // to 1 so the Layer can be properly disposed if its dispose() method\n            // is called.\n            _this2._refCount = 1;\n          }\n        }\n        /*\n          Throw exceptions in case the input is not compatible\n          with the inputSpec set at build time.\n        */\n\n\n        _this2.assertInputCompatibility(inputs); // Handle mask propagation.\n        // TODO(michaelterry): Mask propagation not currently implemented.\n        // Actually call the layer, collecting output(s), mask(s), and shape(s).\n\n\n        if (noneAreSymbolic) {\n          var output = _this2.call(inputs, kwargs); // TODO(michaelterry): Compute the outputMask\n          // If the layer returns tensors from its inputs, unmodified,\n          // we copy them to avoid loss of tensor metadata.\n\n\n          var outputList = generic_utils.toList(output);\n          var outputListCopy = []; // TODO(michaelterry): This copying may not be necessary given our eager\n          // backend.\n\n          var _iterator6 = _createForOfIteratorHelper(outputList),\n              _step6;\n\n          try {\n            for (_iterator6.s(); !(_step6 = _iterator6.n()).done;) {\n              var x = _step6.value;\n\n              if (inputsList.indexOf(x) !== -1) {\n                x = x.clone();\n              }\n\n              outputListCopy.push(x);\n            }\n          } catch (err) {\n            _iterator6.e(err);\n          } finally {\n            _iterator6.f();\n          }\n\n          output = generic_utils.singletonOrArray(outputListCopy);\n\n          if (_this2.activityRegularizer != null) {\n            throw new NotImplementedError('Layer invocation in the presence of activity ' + 'regularizer(s) is not supported yet.');\n          } // TODO(michaelterry): Call addInboundNode()?\n\n\n          return output;\n        } else {\n          var inputShape = collectInputShape(inputs);\n\n          var outputShape = _this2.computeOutputShape(inputShape);\n\n          var _output;\n\n          var outputDType = guessOutputDType(inputs);\n\n          _this2.warnOnIncompatibleInputShape(Array.isArray(inputs) ? inputShape[0] : inputShape);\n\n          if (outputShape != null && outputShape.length > 0 && Array.isArray(outputShape[0])) {\n            // We have multiple output shapes. Create multiple output tensors.\n            _output = outputShape.map(function (shape, index) {\n              return new SymbolicTensor(outputDType, shape, _this2, generic_utils.toList(inputs), kwargs, _this2.name, index);\n            });\n          } else {\n            _output = new SymbolicTensor(outputDType, outputShape, _this2, generic_utils.toList(inputs), kwargs, _this2.name);\n          }\n          /*\n            Add an inbound node to the layer, so that it keeps track\n            of the call and of all new variables created during the call.\n            This also updates the layer history of the output tensor(s).\n            If the input tensor(s) had no previous history,\n            this does nothing.\n          */\n\n\n          _this2.addInboundNode(inputs, _output, null, null, inputShape, outputShape, kwargs);\n\n          _this2._refCount++;\n\n          if (_this2.activityRegularizer != null) {\n            throw new NotImplementedError('Layer invocation in the presence of activity ' + 'regularizer(s) is not supported yet.');\n          }\n\n          return _output;\n        }\n      });\n    }\n    /**\n     * Check compatibility between input shape and this layer's batchInputShape.\n     *\n     * Print warning if any incompatibility is found.\n     *\n     * @param inputShape Input shape to be checked.\n     */\n\n  }, {\n    key: \"warnOnIncompatibleInputShape\",\n    value: function warnOnIncompatibleInputShape(inputShape) {\n      if (this.batchInputShape == null) {\n        return;\n      } else if (inputShape.length !== this.batchInputShape.length) {\n        console.warn(\"The rank of the input tensor provided (shape: \" + \"\".concat(JSON.stringify(inputShape), \") does not match that of the \") + \"batchInputShape (\".concat(JSON.stringify(this.batchInputShape), \") \") + \"of the layer \".concat(this.name));\n      } else {\n        var dimMismatch = false;\n        this.batchInputShape.forEach(function (dimension, i) {\n          if (dimension != null && inputShape[i] != null && inputShape[i] !== dimension) {\n            dimMismatch = true;\n          }\n        });\n\n        if (dimMismatch) {\n          console.warn(\"The shape of the input tensor \" + \"(\".concat(JSON.stringify(inputShape), \") does not \") + \"match the expectation of layer \".concat(this.name, \": \") + \"\".concat(JSON.stringify(this.batchInputShape)));\n        }\n      }\n    }\n    /**\n     * Retrieves the output shape(s) of a layer.\n     *\n     * Only applicable if the layer has only one inbound node, or if all inbound\n     * nodes have the same output shape.\n     *\n     * @returns Output shape or shapes.\n     * @throws AttributeError: if the layer is connected to more than one incoming\n     *   nodes.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n\n  }, {\n    key: \"countParams\",\n\n    /**\n     * Counts the total number of numbers (e.g., float32, int32) in the\n     * weights.\n     *\n     * @returns An integer count.\n     * @throws RuntimeError: If the layer is not built yet (in which case its\n     *   weights are not defined yet.)\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n    value: function countParams() {\n      if (!this.built) {\n        throw new RuntimeError(\"You tried to call countParams() on \".concat(this.name, \", \") + \"but the layer is not built yet. Build it first by calling \" + \"build(batchInputShape).\");\n      }\n\n      return variable_utils.countParamsInWeights(this.weights);\n    }\n    /**\n     * Creates the layer weights.\n     *\n     * Must be implemented on all layers that have weights.\n     *\n     * Called when apply() is called to construct the weights.\n     *\n     * @param inputShape A `Shape` or array of `Shape` (unused).\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n\n  }, {\n    key: \"build\",\n    value: function build(inputShape) {\n      this.built = true;\n    }\n    /**\n     * Returns the current values of the weights of the layer.\n     *\n     * @param trainableOnly Whether to get the values of only trainable weights.\n     * @returns Weight values as an `Array` of `tf.Tensor`s.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n\n  }, {\n    key: \"getWeights\",\n    value: function getWeights() {\n      var trainableOnly = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : false;\n      return batchGetValue(trainableOnly ? this.trainableWeights : this.weights);\n    }\n    /**\n     * Sets the weights of the layer, from Tensors.\n     *\n     * @param weights a list of Tensors. The number of arrays and their shape\n     *   must match number of the dimensions of the weights of the layer (i.e.\n     *   it should match the output of `getWeights`).\n     *\n     * @exception ValueError If the provided weights list does not match the\n     *   layer's specifications.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weights) {\n      var _this3 = this;\n\n      tidy(function () {\n        var params = _this3.weights;\n\n        if (params.length !== weights.length) {\n          // TODO(cais): Restore the following and use `providedWeights`, instead\n          // of `weights` in the error message, once the deeplearn.js bug is\n          // fixed: https://github.com/PAIR-code/deeplearnjs/issues/498 const\n          // providedWeights = JSON.stringify(weights).substr(0, 50);\n          throw new ValueError(\"You called setWeights(weights) on layer \\\"\".concat(_this3.name, \"\\\" \") + \"with a weight list of length \".concat(weights.length, \", \") + \"but the layer was expecting \".concat(params.length, \" weights. \") + \"Provided weights: \".concat(weights, \"...\"));\n        }\n\n        if (params.length === 0) {\n          return;\n        }\n\n        var weightValueTuples = [];\n        var paramValues = batchGetValue(params);\n\n        for (var i = 0; i < paramValues.length; ++i) {\n          var pv = paramValues[i];\n          var p = params[i];\n          var w = weights[i];\n\n          if (!util.arraysEqual(pv.shape, w.shape)) {\n            throw new ValueError(\"Layer weight shape \".concat(pv.shape, \" \") + \"not compatible with provided weight shape \".concat(w.shape));\n          }\n\n          weightValueTuples.push([p, w]);\n        }\n\n        batchSetValue(weightValueTuples);\n      });\n    }\n    /**\n     * Adds a weight variable to the layer.\n     *\n     * @param name Name of the new weight variable.\n     * @param shape The shape of the weight.\n     * @param dtype The dtype of the weight.\n     * @param initializer An initializer instance.\n     * @param regularizer A regularizer instance.\n     * @param trainable Whether the weight should be trained via backprop or not\n     *   (assuming that the layer itself is also trainable).\n     * @param constraint An optional trainable.\n     * @return The created weight variable.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n\n  }, {\n    key: \"addWeight\",\n    value: function addWeight(name, shape, dtype, initializer, regularizer, trainable, constraint) {\n      // Reject duplicate weight names.\n      if (this._addedWeightNames.indexOf(name) !== -1) {\n        throw new ValueError(\"Duplicate weight name \".concat(name, \" for layer \").concat(this.name));\n      }\n\n      this._addedWeightNames.push(name);\n\n      if (dtype == null) {\n        dtype = 'float32';\n      }\n\n      if (this.fastWeightInitDuringBuild) {\n        initializer = getInitializer('zeros');\n      }\n\n      var initValue = initializer.apply(shape, dtype);\n      var weight = new LayerVariable(initValue, dtype, name, trainable, constraint);\n      initValue.dispose(); // Request backend not to dispose the weights of the model on scope() exit.\n\n      if (regularizer != null) {\n        this.addLoss(function () {\n          return regularizer.apply(weight.read());\n        });\n      }\n\n      if (trainable == null) {\n        trainable = true;\n      }\n\n      if (trainable) {\n        this._trainableWeights.push(weight);\n      } else {\n        this._nonTrainableWeights.push(weight);\n      }\n\n      return weight;\n    }\n    /**\n     * Set the fast-weight-initialization flag.\n     *\n     * In cases where the initialized weight values will be immediately\n     * overwritten by loaded weight values during model loading, setting\n     * the flag to `true` saves unnecessary calls to potentially expensive\n     * initializers and speeds up the loading process.\n     *\n     * @param value Target value of the flag.\n     */\n\n  }, {\n    key: \"setFastWeightInitDuringBuild\",\n    value: function setFastWeightInitDuringBuild(value) {\n      this.fastWeightInitDuringBuild = value;\n    }\n    /**\n     * Add losses to the layer.\n     *\n     * The loss may potentionally be conditional on some inputs tensors,\n     * for instance activity losses are conditional on the layer's inputs.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n\n  }, {\n    key: \"addLoss\",\n    value: function addLoss(losses) {\n      if (losses == null || Array.isArray(losses) && losses.length === 0) {\n        return;\n      } // Update this.losses\n\n\n      losses = generic_utils.toList(losses);\n\n      if (this._losses !== undefined && this._losses !== null) {\n        var _this$losses;\n\n        (_this$losses = this.losses).push.apply(_this$losses, _toConsumableArray(losses));\n      }\n    }\n    /**\n     * Computes the output shape of the layer.\n     *\n     * Assumes that the layer will be built to match that input shape provided.\n     *\n     * @param inputShape A shape (tuple of integers) or a list of shape tuples\n     *   (one per output tensor of the layer). Shape tuples can include null for\n     *   free dimensions, instead of an integer.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n    /**\n     * Computes an output mask tensor.\n     *\n     * @param inputs Tensor or list of tensors.\n     * @param mask Tensor or list of tensors.\n     *\n     * @return null or a tensor (or list of tensors, one per output tensor of the\n     * layer).\n     */\n\n  }, {\n    key: \"computeMask\",\n    value: function computeMask(inputs, mask) {\n      var _this4 = this;\n\n      if (!this.supportsMasking) {\n        if (mask != null) {\n          if (Array.isArray(mask)) {\n            mask.forEach(function (maskElement) {\n              if (maskElement != null) {\n                throw new TypeError(\"Layer \".concat(_this4.name, \" does not support masking, \") + 'but was passed an inputMask.');\n              }\n            });\n          } else {\n            throw new TypeError(\"Layer \".concat(this.name, \" does not support masking, \") + 'but was passed an inputMask.');\n          }\n        } // masking not explicitly supported: return null as mask\n\n\n        return null;\n      } // if masking is explictly supported, by default\n      // carry over the input mask\n\n\n      return mask;\n    }\n    /**\n     * Internal method to create an inbound node for the layer.\n     *\n     * @param inputTensors List of input tensors.\n     * @param outputTensors List of output tensors.\n     * @param inputMasks List of input masks (a mask can be a tensor, or null).\n     * @param outputMasks List of output masks (a mask can be a tensor, or null).\n     * @param inputShapes List of input shape tuples.\n     * @param outputShapes List of output shape tuples.\n     * @param kwargs Dictionary of keyword arguments that were passed to the\n     *   `call` method of the layer at the call that created the node.\n     */\n\n  }, {\n    key: \"addInboundNode\",\n    value: function addInboundNode(inputTensors, outputTensors, inputMasks, outputMasks, inputShapes, outputShapes) {\n      var kwargs = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : null;\n      var inputTensorList = generic_utils.toList(inputTensors);\n      outputTensors = generic_utils.toList(outputTensors);\n      inputMasks = generic_utils.toList(inputMasks);\n      outputMasks = generic_utils.toList(outputMasks);\n      inputShapes = types_utils.normalizeShapeList(inputShapes);\n      outputShapes = types_utils.normalizeShapeList(outputShapes); // Collect input tensor(s) coordinates.\n\n      var inboundLayers = [];\n      var nodeIndices = [];\n      var tensorIndices = [];\n\n      var _iterator7 = _createForOfIteratorHelper(inputTensorList),\n          _step7;\n\n      try {\n        for (_iterator7.s(); !(_step7 = _iterator7.n()).done;) {\n          var x = _step7.value;\n\n          /*\n           * TODO(michaelterry): Keras adds this value to tensors; it's not\n           * clear whether we'll use this or not.\n           */\n          inboundLayers.push(x.sourceLayer);\n          nodeIndices.push(x.nodeIndex);\n          tensorIndices.push(x.tensorIndex);\n        } // Create node, add it to inbound nodes.\n        // (This call has side effects.)\n        // tslint:disable-next-line:no-unused-expression\n\n      } catch (err) {\n        _iterator7.e(err);\n      } finally {\n        _iterator7.f();\n      }\n\n      new Node({\n        outboundLayer: this,\n        inboundLayers: inboundLayers,\n        nodeIndices: nodeIndices,\n        tensorIndices: tensorIndices,\n        inputTensors: inputTensorList,\n        outputTensors: outputTensors,\n        inputMasks: inputMasks,\n        outputMasks: outputMasks,\n        inputShapes: inputShapes,\n        outputShapes: outputShapes\n      }, kwargs); // Update tensor history\n\n      for (var i = 0; i < outputTensors.length; i++) {\n        // TODO(michaelterry: _uses_learning_phase not tracked.\n        outputTensors[i].sourceLayer = this;\n        outputTensors[i].nodeIndex = this.inboundNodes.length - 1;\n        outputTensors[i].tensorIndex = i;\n      }\n    }\n    /**\n     * Returns the config of the layer.\n     *\n     * A layer config is a TS dictionary (serializable)\n     * containing the configuration of a layer.\n     * The same layer can be reinstantiated later\n     * (without its trained weights) from this configuration.\n     *\n     * The config of a layer does not include connectivity\n     * information, nor the layer class name.  These are handled\n     * by 'Container' (one layer of abstraction above).\n     *\n     * Porting Note: The TS dictionary follows TS naming standrds for\n     * keys, and uses tfjs-layers type-safe Enums.  Serialization methods\n     * should use a helper function to convert to the pythonic storage\n     * standard. (see serialization_utils.convertTsToPythonic)\n     *\n     * @returns TS dictionary of configuration.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        name: this.name,\n        trainable: this.trainable\n      };\n\n      if (this.batchInputShape != null) {\n        config['batchInputShape'] = this.batchInputShape;\n      }\n\n      if (this.dtype != null) {\n        config['dtype'] = this.dtype;\n      }\n\n      return config;\n    }\n    /**\n     * Dispose the weight variables that this Layer instance holds.\n     *\n     * @returns {number} Number of disposed variables.\n     */\n\n  }, {\n    key: \"disposeWeights\",\n    value: function disposeWeights() {\n      this.weights.forEach(function (weight) {\n        return weight.dispose();\n      });\n      return this.weights.length;\n    }\n  }, {\n    key: \"assertNotDisposed\",\n    value: function assertNotDisposed() {\n      if (this._refCount === 0) {\n        throw new Error(\"Layer '\".concat(this.name, \"' is already disposed.\"));\n      }\n    }\n    /**\n     * Attempt to dispose layer's weights.\n     *\n     * This method decrease the reference count of the Layer object by 1.\n     *\n     * A Layer is reference-counted. Its reference count is incremented by 1\n     * the first item its `apply()` method is called and when it becomes a part\n     * of a new `Node` (through calling the `apply()`) method on a\n     * `tf.SymbolicTensor`).\n     *\n     * If the reference count of a Layer becomes 0, all the weights will be\n     * disposed and the underlying memory (e.g., the textures allocated in WebGL)\n     * will be freed.\n     *\n     * Note: If the reference count is greater than 0 after the decrement, the\n     * weights of the Layer will *not* be disposed.\n     *\n     * After a Layer is disposed, it cannot be used in calls such as `apply()`,\n     * `getWeights()` or `setWeights()` anymore.\n     *\n     * @returns A DisposeResult Object with the following fields:\n     *   - refCountAfterDispose: The reference count of the Container after this\n     *     `dispose()` call.\n     *   - numDisposedVariables: Number of `tf.Variable`s (i.e., weights) disposed\n     *     during this `dispose()` call.\n     * @throws {Error} If the layer is not built yet, or if the layer has already\n     *   been disposed.\n     *\n     * @doc {heading: 'Models', 'subheading': 'Classes'}\n     */\n\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      if (!this.built) {\n        throw new Error(\"Cannot dispose Layer \".concat(this.name, \" because it has not been \") + \"built yet.\");\n      }\n\n      if (this._refCount === null) {\n        throw new Error(\"Cannot dispose Layer \".concat(this.name, \" because it has not been used \") + \"yet.\");\n      }\n\n      this.assertNotDisposed();\n      var numDisposedVariables = 0;\n\n      if (--this._refCount === 0) {\n        numDisposedVariables = this.disposeWeights();\n      }\n\n      return {\n        refCountAfterDispose: this._refCount,\n        numDisposedVariables: numDisposedVariables\n      };\n    }\n  }, {\n    key: \"input\",\n    get: function get() {\n      if (this.inboundNodes.length > 1) {\n        throw new AttributeError(\"Layer \".concat(this.name) + ' has multiple inbound nodes, ' + 'hence the notion of \"layer input\" ' + 'is ill-defined. ' + 'Use `getInputAt(nodeIndex)` instead.');\n      } else if (this.inboundNodes.length === 0) {\n        throw new AttributeError(\"Layer \".concat(this.name) + ' is not connected, no input to return.');\n      }\n\n      return generic_utils.singletonOrArray(this.getNodeAtIndex(0, 'input').inputTensors);\n    }\n    /**\n     * Retrieves the output tensor(s) of a layer.\n     *\n     * Only applicable if the layer has exactly one inbound node,\n     * i.e. if it is connected to one incoming layer.\n     *\n     * @return Output tensor or list of output tensors.\n     *\n     * @exception AttributeError if the layer is connected to more than one\n     *   incoming layers.\n     */\n\n  }, {\n    key: \"output\",\n    get: function get() {\n      if (this.inboundNodes.length === 0) {\n        throw new AttributeError(\"Layer \".concat(this.name) + ' has no inbound nodes.');\n      }\n\n      if (this.inboundNodes.length > 1) {\n        throw new AttributeError(\"Layer \".concat(this.name) + ' has multiple inbound nodes, ' + 'hence the notion of \"layer output\" ' + 'is ill-defined. ' + 'Use `getOutputAt(nodeIndex)` instead.');\n      }\n\n      return generic_utils.singletonOrArray(this.getNodeAtIndex(0, 'output').outputTensors);\n    }\n  }, {\n    key: \"losses\",\n    get: function get() {\n      return this._losses;\n    }\n  }, {\n    key: \"updates\",\n    get: function get() {\n      return this._updates;\n    }\n  }, {\n    key: \"built\",\n    get: function get() {\n      return this._built;\n    },\n    set: function set(built) {\n      this._built = built;\n    }\n  }, {\n    key: \"trainable\",\n    get: function get() {\n      return this.trainable_;\n    },\n    set: function set(trainable) {\n      this._trainableWeights.forEach(function (w) {\n        return w.trainable = trainable;\n      });\n\n      this.trainable_ = trainable;\n    }\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      if (this.trainable_) {\n        return this._trainableWeights.filter(function (w) {\n          return w.trainable;\n        });\n      } else {\n        return [];\n      }\n    },\n    set: function set(weights) {\n      this._trainableWeights = weights;\n    }\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      if (this.trainable) {\n        return this._trainableWeights.filter(function (w) {\n          return !w.trainable;\n        }).concat(this._nonTrainableWeights);\n      } else {\n        return this._trainableWeights.concat(this._nonTrainableWeights);\n      }\n    },\n    set: function set(weights) {\n      this._nonTrainableWeights = weights;\n    }\n    /**\n     * The concatenation of the lists trainableWeights and nonTrainableWeights\n     * (in this order).\n     */\n\n  }, {\n    key: \"weights\",\n    get: function get() {\n      return this.trainableWeights.concat(this.nonTrainableWeights);\n    }\n  }, {\n    key: \"stateful\",\n    get: function get() {\n      return this._stateful;\n    }\n  }, {\n    key: \"outputShape\",\n    get: function get() {\n      if (this.inboundNodes == null || this.inboundNodes.length === 0) {\n        throw new AttributeError(\"The layer \".concat(this.name, \" has never been called and thus has no \") + \"defined output shape.\");\n      }\n\n      var allOutputShapes = [];\n\n      var _iterator8 = _createForOfIteratorHelper(this.inboundNodes),\n          _step8;\n\n      try {\n        for (_iterator8.s(); !(_step8 = _iterator8.n()).done;) {\n          var node = _step8.value;\n          var shapeString = JSON.stringify(node.outputShapes);\n\n          if (allOutputShapes.indexOf(shapeString) === -1) {\n            allOutputShapes.push(shapeString);\n          }\n        }\n      } catch (err) {\n        _iterator8.e(err);\n      } finally {\n        _iterator8.f();\n      }\n\n      if (allOutputShapes.length === 1) {\n        var outputShapes = this.inboundNodes[0].outputShapes;\n\n        if (Array.isArray(outputShapes) && Array.isArray(outputShapes[0]) && outputShapes.length === 1) {\n          return outputShapes[0];\n        } else {\n          return outputShapes;\n        }\n      } else {\n        throw new AttributeError(\"The layer \".concat(this.name, \" has multiple inbound nodes with different \") + \"output shapes. Hence the notion of \\\"output shape\\\" is ill-defined \" + \"for the layer.\"); // TODO(cais): Implement getOutputShapeAt().\n      }\n    }\n  }], [{\n    key: \"nodeKey\",\n    value: function nodeKey(layer, nodeIndex) {\n      return layer.name + '_ib-' + nodeIndex.toString();\n    }\n  }]);\n\n  return Layer;\n}(serialization.Serializable);\n/**\n * Collects the input shape(s) of a list of `tf.Tensor`s or\n * `tf.SymbolicTensor`s.\n *\n * TODO(michaelterry): Update PyKeras docs (backport).\n *\n * @param inputTensors List of input tensors (or single input tensor).\n *\n * @return List of shape tuples (or single tuple), one tuple per input.\n */\n\nfunction collectInputShape(inputTensors) {\n  inputTensors = generic_utils.toList(inputTensors);\n  var shapes = [];\n\n  var _iterator9 = _createForOfIteratorHelper(inputTensors),\n      _step9;\n\n  try {\n    for (_iterator9.s(); !(_step9 = _iterator9.n()).done;) {\n      var x = _step9.value;\n      shapes.push(x.shape);\n    }\n  } catch (err) {\n    _iterator9.e(err);\n  } finally {\n    _iterator9.f();\n  }\n\n  return generic_utils.singletonOrArray(shapes);\n}\n/**\n * Guesses output dtype based on inputs.\n *\n * At present, just returns 'float32' for any input.\n *\n * @param inputTensors List of input tensors (or single input tensor).\n *\n * @return The guessed DType. At present, always returns 'float32'.\n */\n\n\nfunction guessOutputDType(inputTensors) {\n  return 'float32';\n}\n/**\n * Returns the list of input tensors necessary to compute `tensor`.\n *\n * Output will always be a list of tensors (potentially with 1 element).\n *\n * @param tensor The tensor to start from.\n * @param layer Origin layer of the tensor.\n * @param nodeIndex Origin node index of the tensor.\n *\n * @return Array of input tensors.\n */\n\n\nexport function getSourceInputs(tensor, layer, nodeIndex) {\n  if (layer == null || nodeIndex != null && nodeIndex > 0) {\n    layer = tensor.sourceLayer;\n    nodeIndex = tensor.nodeIndex;\n  }\n\n  if (layer.inboundNodes.length === 0) {\n    return [tensor];\n  } else {\n    var node = layer.inboundNodes[nodeIndex];\n\n    if (node.inboundLayers.length === 0) {\n      return node.inputTensors;\n    } else {\n      var sourceTensors = [];\n\n      for (var i = 0; i < node.inboundLayers.length; i++) {\n        var x = node.inputTensors[i];\n        var _layer = node.inboundLayers[i];\n        var _nodeIndex = node.nodeIndices[i];\n        var previousSources = getSourceInputs(x, _layer, _nodeIndex); // Avoid input redundancy.\n\n        var _iterator10 = _createForOfIteratorHelper(previousSources),\n            _step10;\n\n        try {\n          for (_iterator10.s(); !(_step10 = _iterator10.n()).done;) {\n            var _x = _step10.value;\n\n            if (sourceTensors.indexOf(_x) === -1) {\n              sourceTensors.push(_x);\n            }\n          }\n        } catch (err) {\n          _iterator10.e(err);\n        } finally {\n          _iterator10.f();\n        }\n      }\n\n      return sourceTensors;\n    }\n  }\n}","map":null,"metadata":{},"sourceType":"module"}