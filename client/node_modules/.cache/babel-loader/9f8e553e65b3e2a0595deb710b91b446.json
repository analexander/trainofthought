{"ast":null,"code":"import _get from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/get\";\nimport _classCallCheck from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass\";\nimport _possibleConstructorReturn from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";\nimport _getPrototypeOf from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";\nimport _inherits from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/inherits\";\n\nfunction _createForOfIteratorHelper(o, allowArrayLike) { var it; if (typeof Symbol === \"undefined\" || o[Symbol.iterator] == null) { if (Array.isArray(o) || (it = _unsupportedIterableToArray(o)) || allowArrayLike && o && typeof o.length === \"number\") { if (it) o = it; var i = 0; var F = function F() {}; return { s: F, n: function n() { if (i >= o.length) return { done: true }; return { done: false, value: o[i++] }; }, e: function e(_e) { throw _e; }, f: F }; } throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\"); } var normalCompletion = true, didErr = false, err; return { s: function s() { it = o[Symbol.iterator](); }, n: function n() { var step = it.next(); normalCompletion = step.done; return step; }, e: function e(_e2) { didErr = true; err = _e2; }, f: function f() { try { if (!normalCompletion && it.return != null) it.return(); } finally { if (didErr) throw err; } } }; }\n\nfunction _unsupportedIterableToArray(o, minLen) { if (!o) return; if (typeof o === \"string\") return _arrayLikeToArray(o, minLen); var n = Object.prototype.toString.call(o).slice(8, -1); if (n === \"Object\" && o.constructor) n = o.constructor.name; if (n === \"Map\" || n === \"Set\") return Array.from(o); if (n === \"Arguments\" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return _arrayLikeToArray(o, minLen); }\n\nfunction _arrayLikeToArray(arr, len) { if (len == null || len > arr.length) len = arr.length; for (var i = 0, arr2 = new Array(len); i < len; i++) { arr2[i] = arr[i]; } return arr2; }\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Merge Layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { l2Normalize } from '../losses';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as mathUtils from '../utils/math_utils';\nimport { getExactlyOneShape } from '../utils/types_utils';\n/**\n * Generic Merge layer for element-wise merge functions.\n *\n * Used to implement `Sum`, `Average`, `Concatenate`, etc.\n */\n\nexport var Merge = /*#__PURE__*/function (_Layer) {\n  _inherits(Merge, _Layer);\n\n  function Merge(args) {\n    var _this;\n\n    _classCallCheck(this, Merge);\n\n    _this = _possibleConstructorReturn(this, _getPrototypeOf(Merge).call(this, args || {}));\n    _this.supportsMasking = true;\n    return _this;\n  }\n  /**\n   * Logic for merging multiple tensors, to be overridden by subclasses.\n   * @param inputs\n   */\n\n\n  _createClass(Merge, [{\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      throw new NotImplementedError();\n    }\n    /**\n     * Computes the shape of the result of an elementwise operation.\n     *\n     * @param shape1: Shape of the first tensor.\n     * @param shape2: Shape of the second tensor.\n     * @returns Expected output shape when an elementwise operation is carried\n     *   out on 2 tensors with shapes `shape1` and `shape2`.\n     * @throws ValueError: If `shape1` and `shape2` are not compatible for\n     *   element-wise operations.\n     */\n\n  }, {\n    key: \"computeElementwiseOpOutputShape\",\n    value: function computeElementwiseOpOutputShape(shape1, shape2) {\n      if (shape1 == null || shape2 == null) {\n        return null;\n      } else if (shape1.length < shape2.length) {\n        return this.computeElementwiseOpOutputShape(shape2, shape1);\n      } else if (shape2.length === 0) {\n        return shape1;\n      }\n\n      var outputShape = shape1.slice(0, shape1.length - shape2.length);\n\n      for (var k = 0; k < shape2.length; ++k) {\n        var i = shape1[shape1.length - shape2.length + k];\n        var j = shape2[k];\n\n        if (i == null || j == null || i < 0 || j < 0) {\n          outputShape.push(null);\n        } else if (i === 1) {\n          outputShape.push(j);\n        } else if (j === 1) {\n          outputShape.push(i);\n        } else {\n          if (i !== j) {\n            throw new ValueError('Operands could not be broadcast together with shapes ' + JSON.stringify(shape1) + ' ' + JSON.stringify(shape2));\n          }\n\n          outputShape.push(i);\n        }\n      }\n\n      return outputShape;\n    }\n  }, {\n    key: \"build\",\n    value: function build(inputShape) {\n      // Used purely for shape validation.\n      if (Array.isArray(inputShape) && !Array.isArray(inputShape[0])) {\n        // Make sure that inputShape is an Array of shape.\n        inputShape = [getExactlyOneShape(inputShape)];\n      }\n\n      inputShape = inputShape;\n\n      if (inputShape.length < 2) {\n        throw new ValueError('A merge layer should be called on an Array of at least 2 inputs.' + \" Got \".concat(inputShape.length, \" input(s).\"));\n      } // Make sure that there is at most one unique batch size among the input\n      // shapes.\n\n\n      var batchSizes = [];\n\n      var _iterator = _createForOfIteratorHelper(inputShape),\n          _step;\n\n      try {\n        for (_iterator.s(); !(_step = _iterator.n()).done;) {\n          var _shape = _step.value;\n\n          if (_shape != null && _shape[0] !== null) {\n            batchSizes.push(_shape[0]);\n          }\n        }\n      } catch (err) {\n        _iterator.e(err);\n      } finally {\n        _iterator.f();\n      }\n\n      batchSizes = generic_utils.unique(batchSizes);\n\n      if (batchSizes.length > 1) {\n        throw new ValueError(\"Can not merge tensors with different batch sizes. \" + \"Got tensors with shapes: \".concat(JSON.stringify(inputShape), \".\"));\n      }\n\n      var outputShape = inputShape[0] == null ? null : inputShape[0].slice(1);\n\n      for (var i = 1; i < inputShape.length; ++i) {\n        var shape = inputShape[i] == null ? null : inputShape[i].slice(1);\n        outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);\n      } // If the inputs have different ranks, we have to reshape them to make them\n      // broadcastable.\n\n\n      var allRanks = inputShape.map(function (shape) {\n        return shape.length;\n      });\n\n      if (inputShape.indexOf(null) === -1 && generic_utils.unique(allRanks).length === 1) {\n        this.reshapeRequired = false;\n      } else {\n        this.reshapeRequired = true;\n      }\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this2 = this;\n\n      return tidy(function () {\n        inputs = inputs;\n\n        if (_this2.reshapeRequired) {\n          var reshapedInputs = [];\n          var inputDims = inputs.map(function (input) {\n            return input.rank;\n          });\n\n          if (inputDims.indexOf(null) === -1) {\n            // If ranks of all inputs are available, we simply expand each of them\n            // at axis=1 until all of them have the same rank.\n            var maxNDim = mathUtils.max(inputDims);\n\n            var _iterator2 = _createForOfIteratorHelper(inputs),\n                _step2;\n\n            try {\n              for (_iterator2.s(); !(_step2 = _iterator2.n()).done;) {\n                var x = _step2.value;\n                var xNDim = x.rank;\n\n                for (var k = 0; k < maxNDim - xNDim; ++k) {\n                  x = K.expandDims(x, 1);\n                }\n\n                reshapedInputs.push(x);\n              }\n            } catch (err) {\n              _iterator2.e(err);\n            } finally {\n              _iterator2.f();\n            }\n\n            return _this2.mergeFunction(reshapedInputs);\n          } else {\n            // Transpose all inputs so that batch size is the last dimension.\n            // [batchSize, dim1, dim2, ...] -> [dim1, dim2, ..., batchSize]\n            var transposed = false;\n\n            var _iterator3 = _createForOfIteratorHelper(inputs),\n                _step3;\n\n            try {\n              for (_iterator3.s(); !(_step3 = _iterator3.n()).done;) {\n                var _x = _step3.value;\n                var _xNDim = _x.rank;\n\n                if (_xNDim == null) {\n                  var xShape = _x.shape;\n                  var _batchSize = xShape[0];\n\n                  var _newShape = xShape.slice(1).concat([_batchSize]);\n\n                  var xTransposed = _x.reshape([_batchSize].concat(mathUtils.arrayProd(xShape.slice(1))));\n\n                  xTransposed = tfc.transpose(xTransposed, [1, 0]);\n                  xTransposed = xTransposed.reshape(_newShape);\n                  reshapedInputs.push(xTransposed);\n                  transposed = true;\n                } else if (_xNDim > 1) {\n                  var _dims = mathUtils.range(1, _xNDim).concat([0]);\n\n                  reshapedInputs.push(tfc.transpose(_x, _dims));\n                  transposed = true;\n                } else {\n                  // We don't transpose inputs if they are 1D vectors or scalars.\n                  reshapedInputs.push(_x);\n                }\n              }\n            } catch (err) {\n              _iterator3.e(err);\n            } finally {\n              _iterator3.f();\n            }\n\n            var y = _this2.mergeFunction(reshapedInputs);\n\n            var yNDim = y.rank;\n\n            if (transposed) {\n              // If inputs have been transposed, we have to transpose the output\n              // too.\n              if (yNDim == null) {\n                var yShape = y.shape;\n                var _yNDim = yShape.length;\n                var batchSize = yShape[_yNDim - 1];\n                var newShape = [batchSize].concat(yShape.slice(0, yShape.length - 1));\n                y = tfc.transpose(y.reshape([-1, batchSize]), [1, 0]).reshape(newShape);\n              } else if (yNDim > 1) {\n                var dims = [yNDim - 1].concat(mathUtils.range(0, yNDim - 1));\n                y = tfc.transpose(y, dims);\n              }\n            }\n\n            return y;\n          }\n        } else {\n          return _this2.mergeFunction(inputs);\n        }\n      });\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      inputShape = inputShape;\n      var outputShape;\n\n      if (inputShape[0] == null) {\n        outputShape = null;\n      } else {\n        outputShape = inputShape[0].slice(1);\n      }\n\n      for (var i = 1; i < inputShape.length; ++i) {\n        var shape = inputShape[i] == null ? null : inputShape[i].slice(1);\n        outputShape = this.computeElementwiseOpOutputShape(outputShape, shape);\n      }\n\n      var batchSizes = [];\n\n      var _iterator4 = _createForOfIteratorHelper(inputShape),\n          _step4;\n\n      try {\n        for (_iterator4.s(); !(_step4 = _iterator4.n()).done;) {\n          var _shape2 = _step4.value;\n\n          if (_shape2 != null && _shape2[0] !== null) {\n            batchSizes.push(_shape2[0]);\n          }\n        }\n      } catch (err) {\n        _iterator4.e(err);\n      } finally {\n        _iterator4.f();\n      }\n\n      batchSizes = generic_utils.unique(batchSizes);\n\n      if (batchSizes.length === 1) {\n        outputShape = batchSizes.concat(outputShape);\n      } else {\n        outputShape = [null].concat(outputShape);\n      }\n\n      return outputShape;\n    }\n  }, {\n    key: \"computeMask\",\n    value: function computeMask(inputs, mask) {\n      return tfc.tidy(function () {\n        if (mask == null) {\n          return null;\n        }\n\n        if (!Array.isArray(mask)) {\n          throw new ValueError('`mask` should be an Array');\n        }\n\n        if (!Array.isArray(inputs)) {\n          throw new ValueError('`inputs` should be an Array');\n        }\n\n        if (mask.length !== inputs.length) {\n          throw new ValueError(\"The Array 'inputs' and 'mask' are expected to have the same \" + \"length, but have different lengths \" + \"(\".concat(inputs.length, \" vs \").concat(mask.length, \")\"));\n        }\n\n        if (mask.every(function (m) {\n          return m == null;\n        })) {\n          return null;\n        }\n\n        mask = mask.map(function (m) {\n          return m == null ? m : tfc.expandDims(m, 0);\n        });\n        var output = mask[0];\n\n        for (var i = 1; i < mask.length - 1; ++i) {\n          output = tfc.logicalAnd(output, mask[i]);\n        }\n\n        return output;\n      });\n    }\n  }]);\n\n  return Merge;\n}(Layer);\nexport var Add = /*#__PURE__*/function (_Merge) {\n  _inherits(Add, _Merge);\n\n  function Add(args) {\n    _classCallCheck(this, Add);\n\n    return _possibleConstructorReturn(this, _getPrototypeOf(Add).call(this, args));\n  }\n\n  _createClass(Add, [{\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      return tidy(function () {\n        var output = inputs[0].clone();\n\n        for (var i = 1; i < inputs.length; ++i) {\n          output = tfc.add(output, inputs[i]);\n        }\n\n        return output;\n      });\n    }\n  }]);\n\n  return Add;\n}(Merge);\n/** @nocollapse */\n\nAdd.className = 'Add';\nserialization.registerClass(Add);\n/**\n * Calculate the element-wise sum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Add` layer, by using no input argument\n *    or a single configuration argument. The resultant `Add` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const addLayer = tf.layers.add();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = addLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.add([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.add([input1, input2]).print();\n * // Gives [[11, 22], [33, 44]].\n *\n */\n\nexport function add(config) {\n  if (Array.isArray(config)) {\n    var layer = new Add({});\n    return layer.apply(config);\n  } else {\n    return new Add(config);\n  }\n}\nexport var Multiply = /*#__PURE__*/function (_Merge2) {\n  _inherits(Multiply, _Merge2);\n\n  function Multiply(args) {\n    _classCallCheck(this, Multiply);\n\n    return _possibleConstructorReturn(this, _getPrototypeOf(Multiply).call(this, args));\n  }\n\n  _createClass(Multiply, [{\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      return tidy(function () {\n        var output = inputs[0].clone();\n\n        for (var i = 1; i < inputs.length; ++i) {\n          output = tfc.mul(output, inputs[i]);\n        }\n\n        return output;\n      });\n    }\n  }]);\n\n  return Multiply;\n}(Merge);\n/** @nocollapse */\n\nMultiply.className = 'Multiply';\nserialization.registerClass(Multiply);\n/**\n * Calculate the element-wise product of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Multiply` layer, by using no input argument\n *    or a single configuration argument. The resultant `Multiply` layer can\n *    then be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const multiplyLayer = tf.layers.multiply();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = multiplyLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.multiply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.multiply([input1, input2]).print();\n * // Gives [[10, 40], [90, 160]].\n *\n */\n\nexport function multiply(config) {\n  if (Array.isArray(config)) {\n    var layer = new Multiply({});\n    return layer.apply(config);\n  } else {\n    return new Multiply(config);\n  }\n}\nexport var Average = /*#__PURE__*/function (_Merge3) {\n  _inherits(Average, _Merge3);\n\n  function Average(args) {\n    _classCallCheck(this, Average);\n\n    return _possibleConstructorReturn(this, _getPrototypeOf(Average).call(this, args));\n  }\n\n  _createClass(Average, [{\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      return tidy(function () {\n        var output = inputs[0].clone();\n\n        for (var i = 1; i < inputs.length; ++i) {\n          output = tfc.add(output, inputs[i]);\n        }\n\n        return tfc.mul(1 / inputs.length, output);\n      });\n    }\n  }]);\n\n  return Average;\n}(Merge);\n/** @nocollapse */\n\nAverage.className = 'Average';\nserialization.registerClass(Average);\n/**\n * Calculate the element-wise arithmetic mean of inputs, which all have the same\n * shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Average` layer, by using no input argument\n *    or a single configuration argument. The resultant `Average` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const averageLayer = tf.layers.average();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = averageLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.average([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const input2 = tf.tensor2d([10, 20, 30, 40], [2, 2]);\n * tf.layers.average([input1, input2]).print();\n * // Gives [[5.5, 11], [16.5, 22]].\n *\n */\n\nexport function average(config) {\n  if (Array.isArray(config)) {\n    var layer = new Average({});\n    return layer.apply(config);\n  } else {\n    return new Average(config);\n  }\n}\nexport var Maximum = /*#__PURE__*/function (_Merge4) {\n  _inherits(Maximum, _Merge4);\n\n  function Maximum(args) {\n    _classCallCheck(this, Maximum);\n\n    return _possibleConstructorReturn(this, _getPrototypeOf(Maximum).call(this, args));\n  }\n\n  _createClass(Maximum, [{\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      return tidy(function () {\n        var output = inputs[0];\n\n        for (var i = 1; i < inputs.length; ++i) {\n          output = tfc.maximum(output, inputs[i]);\n        }\n\n        return output;\n      });\n    }\n  }]);\n\n  return Maximum;\n}(Merge);\n/** @nocollapse */\n\nMaximum.className = 'Maximum';\nserialization.registerClass(Maximum);\n/**\n * Calculate the element-wise maximum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Maximum` layer, by using no input argument\n *    or a single configuration argument. The resultant `Maximum` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const maximumLayer = tf.layers.maximum();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = maximumLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.maximum([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 20, 3, 40], [2, 2]);\n * const input2 = tf.tensor2d([10, 2, 30, 4], [2, 2]);\n * tf.layers.maximum([input1, input2]).print();\n * // Gives [[10, 20], [30, 40]].\n *\n */\n\nexport function maximum(config) {\n  if (Array.isArray(config)) {\n    var layer = new Maximum({});\n    return layer.apply(config);\n  } else {\n    return new Maximum(config);\n  }\n}\nexport var Minimum = /*#__PURE__*/function (_Merge5) {\n  _inherits(Minimum, _Merge5);\n\n  function Minimum(args) {\n    _classCallCheck(this, Minimum);\n\n    return _possibleConstructorReturn(this, _getPrototypeOf(Minimum).call(this, args));\n  }\n\n  _createClass(Minimum, [{\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      return tidy(function () {\n        var output = inputs[0];\n\n        for (var i = 1; i < inputs.length; ++i) {\n          output = tfc.minimum(output, inputs[i]);\n        }\n\n        return output;\n      });\n    }\n  }]);\n\n  return Minimum;\n}(Merge);\n/** @nocollapse */\n\nMinimum.className = 'Minimum';\nserialization.registerClass(Minimum);\n/**\n * Calculate the element-wise minimum of inputs, which all have the same shape.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Minimum` layer, by using no input argument\n *    or a single configuration argument. The resultant `Minimum` layer can then\n *    be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const minimumLayer = tf.layers.minimum();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = minimumLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 2]});\n * const input2 = tf.input({shape: [2, 2]});\n * const output = tf.layers.minimum([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([1, 20, 3, 40], [2, 2]);\n * const input2 = tf.tensor2d([10, 2, 30, 4], [2, 2]);\n * tf.layers.minimum([input1, input2]).print();\n * // Gives [[1, 2], [3, 4]].\n *\n */\n\nexport function minimum(config) {\n  if (Array.isArray(config)) {\n    var layer = new Minimum({});\n    return layer.apply(config);\n  } else {\n    return new Minimum(config);\n  }\n}\nexport var Concatenate = /*#__PURE__*/function (_Merge6) {\n  _inherits(Concatenate, _Merge6);\n\n  function Concatenate(args) {\n    var _this3;\n\n    _classCallCheck(this, Concatenate);\n\n    _this3 = _possibleConstructorReturn(this, _getPrototypeOf(Concatenate).call(this, args));\n    _this3.DEFAULT_AXIS = -1;\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this3.axis = args.axis == null ? _this3.DEFAULT_AXIS : args.axis;\n    _this3.supportsMasking = true;\n    _this3.reshapeRequired = false;\n    return _this3;\n  }\n\n  _createClass(Concatenate, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      // Used purely for shape validation.]\n      if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0])) || inputShape.length === 1) {\n        throw new ValueError('A `Concatenate` layer should be called on a list of at least 2 ' + 'inputs');\n      }\n\n      inputShape = inputShape;\n      var allNoneShape = true;\n\n      var _iterator5 = _createForOfIteratorHelper(inputShape),\n          _step5;\n\n      try {\n        for (_iterator5.s(); !(_step5 = _iterator5.n()).done;) {\n          var _shape3 = _step5.value;\n\n          if (_shape3 != null) {\n            allNoneShape = false;\n            break;\n          }\n        }\n      } catch (err) {\n        _iterator5.e(err);\n      } finally {\n        _iterator5.f();\n      }\n\n      if (allNoneShape) {\n        return;\n      }\n\n      var shapeSet = [];\n\n      for (var i = 0; i < inputShape.length; ++i) {\n        var shapeWithoutConcatAxis = inputShape[i].slice();\n        shapeWithoutConcatAxis.splice(this.axis, 1);\n        var exists = false;\n\n        var _iterator6 = _createForOfIteratorHelper(shapeSet),\n            _step6;\n\n        try {\n          for (_iterator6.s(); !(_step6 = _iterator6.n()).done;) {\n            var shape = _step6.value;\n\n            if (util.arraysEqual(shape, shapeWithoutConcatAxis)) {\n              exists = true;\n              break;\n            }\n          }\n        } catch (err) {\n          _iterator6.e(err);\n        } finally {\n          _iterator6.f();\n        }\n\n        if (!exists) {\n          shapeSet.push(shapeWithoutConcatAxis);\n        }\n      }\n\n      if (shapeSet.length > 1) {\n        throw new ValueError('A `Concatenate` layer requires inputs with matching shapes ' + 'except for the concat axis. Got input shapes: ' + JSON.stringify(inputShape));\n      }\n    }\n  }, {\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      var _this4 = this;\n\n      return tidy(function () {\n        return K.concatenate(inputs, _this4.axis);\n      });\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      if (!(Array.isArray(inputShape) && Array.isArray(inputShape[0]))) {\n        throw new ValueError('A `Concatenate` layer should be called on a list of inputs.');\n      }\n\n      var inputShapes = inputShape;\n      var outputShape = inputShapes[0].slice();\n      var axis = this.axis < 0 ? outputShape.length + this.axis : this.axis; // Porting Note: the line above is because TypeScript doesn't support\n      //   negative indices.\n\n      var _iterator7 = _createForOfIteratorHelper(inputShapes.slice(1)),\n          _step7;\n\n      try {\n        for (_iterator7.s(); !(_step7 = _iterator7.n()).done;) {\n          var shape = _step7.value;\n\n          if (outputShape[axis] == null || shape[axis] == null) {\n            outputShape[axis] = null;\n            break;\n          }\n\n          outputShape[axis] += shape[axis];\n        }\n      } catch (err) {\n        _iterator7.e(err);\n      } finally {\n        _iterator7.f();\n      }\n\n      return outputShape;\n    }\n  }, {\n    key: \"computeMask\",\n    value: function computeMask(inputs, mask) {\n      var _this5 = this;\n\n      if (mask == null) {\n        return null;\n      }\n\n      if (!Array.isArray(mask)) {\n        throw new ValueError('`mask` should be an array for Concatenate');\n      }\n\n      if (!Array.isArray(inputs)) {\n        throw new ValueError('`inputs` should be an array for Concatenate');\n      }\n\n      if (mask.length !== inputs.length) {\n        throw new ValueError(\"Mismatch in the length of mask (\".concat(mask.length, \") \") + \"and the legnth of inputs (\".concat(inputs.length, \")\"));\n      }\n\n      return tfc.tidy(function () {\n        var allNullMasks = true;\n        mask.forEach(function (m) {\n          if (m != null) {\n            allNullMasks = false;\n            return;\n          }\n        });\n\n        if (allNullMasks) {\n          return null;\n        }\n\n        var outputMasks = [];\n\n        for (var i = 0; i < inputs.length; ++i) {\n          if (mask[i] == null) {\n            // Input is unmasked. Append all 1's to masks.\n            outputMasks.push(tfc.onesLike(inputs[i]).asType('bool'));\n          } else if (mask[i].rank < inputs[i].rank) {\n            // Mask is smaller than the input, expand it.\n            outputMasks.push(tfc.expandDims(mask[i], -1));\n          } else {\n            outputMasks.push(mask[i]);\n          }\n        }\n\n        var concatenatedMasks = tfc.concat(outputMasks, _this5.axis);\n        return tfc.all(concatenatedMasks, -1, false);\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        'axis': this.axis\n      };\n\n      var baseConfig = _get(_getPrototypeOf(Concatenate.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n\n  return Concatenate;\n}(Merge);\n/** @nocollapse */\n\nConcatenate.className = 'Concatenate';\nserialization.registerClass(Concatenate);\n/**\n * Concatenate an `Array` of inputs.\n *\n * This function can be invoked in three ways.\n *\n * 1. Construct an instance of `Concatenate` layer, by using no input argument\n *    or a single configuration argument. The resultant `Concatenate` layer can\n *    then be used on `tf.SymbolicTensor`s or `tf.Tensor`s. For example:\n *\n * ```js\n * const concatLayer = tf.layers.concatenate();\n *\n * // The layer can be applied to inputs.\n * const input1 = tf.input({shape: [2, 3]});\n * const input2 = tf.input({shape: [2, 4]});\n * const output = concatLayer.apply([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 7], with the first dimension as the undetermined batch\n * // dimension and the last dimension as the result of concatenating the\n * // last dimensions of the two inputs.\n * ```\n *\n * 2. Invoke directly on an `Array` of `tf.SymbolicTensor`s. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.SymbolicTensor`. For example:\n *\n * ```js\n * const input1 = tf.input({shape: [2, 3]});\n * const input2 = tf.input({shape: [2, 4]});\n * const output = tf.layers.concatenate([input1, input2]);\n * console.log(output.shape);\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\n * // dimension and the last dimension as the result of concatenating the\n * // last dimensions of the two inputs.\n * ```\n *\n * 3. Invoke directly on `tf.Tensor`s, i.e., concrete values. This constructs\n *    an `Layer` object internally and calls its `apply` method on the inputs,\n *    generating a new `tf.Tensor` as the result of the computation. For\n * example:\n *\n * ```js\n * const input1 = tf.tensor2d([[1, 2], [3, 4]], [2, 2]);\n * const input2 = tf.tensor2d([[10, 20], [30, 40]], [2, 2]);\n * tf.layers.concatenate([input1, input2]).print();\n * // Gives [[1, 2, 10, 20], [3, 4, 30, 40]].\n *\n */\n\nexport function concatenate(config) {\n  if (Array.isArray(config)) {\n    var layer = new Concatenate({});\n    return layer.apply(config);\n  } else {\n    return new Concatenate(config);\n  }\n}\n/**\n * Interpretable potentially negative axis index.\n *\n * For example, given axis = -1, and dim = 3, this function will return 2.\n *\n * @param axis The axis index, may be a positive, zero or negative integer.\n * @param dim Total number of dimensions, a positive integer.\n * @returns A non-negative axis index equivalent to the input `axis`.\n */\n\nfunction interpretAxis(axis, dim) {\n  while (axis < 0) {\n    axis += dim;\n  }\n\n  return axis;\n}\n\nfunction batchDot(x, y, axes) {\n  if (x.shape.length > 3 || y.shape.length > 3) {\n    throw new NotImplementedError('batchDot is not implemented for tensors of 4D or higher rank yet');\n  }\n\n  tfc.util.assert(x.shape.length >= 2, function () {\n    return \"batchDot requires the rank of x to be >= 2, \" + \"but got \".concat(x.shape.length);\n  });\n  tfc.util.assert(x.shape.length >= 2, function () {\n    return \"batchDot requires the rank of y to be >= 2, \" + \"but got \".concat(y.shape.length);\n  });\n\n  if (typeof axes === 'number') {\n    axes = [axes, axes];\n  }\n\n  if (x.dtype === 'complex64' || y.dtype === 'complex64') {\n    throw new NotImplementedError('batchDot is not implemented for complex64-type Tensors yet.');\n  }\n\n  var xNDim = x.shape.length;\n  var yNDim = y.shape.length;\n\n  if (axes == null) {\n    // Behave like batchMatmul by default.\n    axes = [xNDim - 1, yNDim - 2];\n  }\n\n  var axesArray = axes;\n  return tfc.tidy(function () {\n    var diff;\n\n    if (xNDim > yNDim) {\n      diff = xNDim - yNDim;\n      var diffShape = [];\n\n      for (var i = 0; i < diff; ++i) {\n        diffShape.push(1);\n      }\n\n      y = y.reshape(y.shape.concat(diffShape));\n    } else if (yNDim > xNDim) {\n      diff = yNDim - xNDim;\n      var _diffShape = [];\n\n      for (var _i = 0; _i < diff; ++_i) {\n        _diffShape.push(1);\n      }\n\n      x = x.reshape(x.shape.concat(_diffShape));\n    } else {\n      diff = 0;\n    }\n\n    var out;\n\n    if (x.shape.length === 2 && y.shape.length === 2) {\n      if (axesArray[0] === axesArray[1]) {\n        out = x.mul(y).sum(axesArray[0]);\n      } else {\n        out = x.transpose([1, 0]).mul(y).sum(axesArray[1]);\n      }\n    } else {\n      var adjX = axesArray[0] !== x.shape.length - 1;\n      var adjY = axesArray[1] === y.shape.length - 1;\n      out = x.matMul(y, adjX, adjY);\n    }\n\n    if (diff > 0) {\n      var idx;\n\n      if (xNDim > yNDim) {\n        idx = xNDim + yNDim - 3;\n      } else {\n        idx = xNDim - 1;\n      }\n\n      var squeezeAxes = [];\n\n      for (var _i2 = idx; _i2 < idx + diff; ++_i2) {\n        squeezeAxes.push(_i2);\n      }\n\n      out = out.squeeze(squeezeAxes);\n    }\n\n    if (out.shape.length === 1) {\n      out = out.expandDims(1);\n    }\n\n    return out;\n  });\n}\n\nexport var Dot = /*#__PURE__*/function (_Merge7) {\n  _inherits(Dot, _Merge7);\n\n  function Dot(args) {\n    var _this6;\n\n    _classCallCheck(this, Dot);\n\n    _this6 = _possibleConstructorReturn(this, _getPrototypeOf(Dot).call(this, args));\n    _this6.axes = args.axes;\n    _this6.normalize = args.normalize == null ? false : args.normalize;\n    _this6.supportsMasking = true;\n    _this6.reshapeRequired = false;\n    return _this6;\n  }\n\n  _createClass(Dot, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      tfc.util.assert(Array.isArray(inputShape) && inputShape.length === 2 && Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]), function () {\n        return 'A `Dot` layer should be called on a list of exactly 2 inputs.';\n      });\n      var shape1 = inputShape[0];\n      var shape2 = inputShape[1];\n\n      if (shape1.length > 3 || shape2.length > 3) {\n        throw new NotImplementedError('Dot layer does not support tensors of 4D or higher rank yet.');\n      }\n\n      var axes = this.interpretAxes(shape1, shape2);\n\n      if (shape1[axes[0]] !== shape2[axes[1]]) {\n        throw new ValueError(\"Dimension incompatibility: \" + \"\".concat(shape1[axes[0]], \" !== \").concat(shape2[axes[1]]));\n      }\n    }\n  }, {\n    key: \"mergeFunction\",\n    value: function mergeFunction(inputs) {\n      if (inputs.length !== 2) {\n        throw new ValueError('A `Dot` layer must be called on exactly 2 inputs, ' + \"but received \".concat(inputs.length, \" input(s).\"));\n      }\n\n      var x1 = inputs[0];\n      var x2 = inputs[1];\n      var axes;\n\n      if (!Array.isArray(this.axes)) {\n        axes = [interpretAxis(this.axes, x1.shape.length), interpretAxis(this.axes, x2.shape.length)];\n      } else {\n        axes = this.axes.map(function (axis, i) {\n          return interpretAxis(axis, inputs[i].shape.length);\n        });\n      }\n\n      if (this.normalize) {\n        x1 = l2Normalize(x1, axes[0]);\n        x2 = l2Normalize(x2, axes[1]);\n      }\n\n      return batchDot(x1, x2, axes);\n    }\n  }, {\n    key: \"interpretAxes\",\n    value: function interpretAxes(shape1, shape2) {\n      var axes;\n\n      if (!Array.isArray(this.axes)) {\n        // `this.axes` is a single integer.\n        axes = [interpretAxis(this.axes, shape1.length), interpretAxis(this.axes, shape2.length)];\n      } else {\n        // `this.axes` is an Array of integers.\n        axes = this.axes;\n      }\n\n      return axes;\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      tfc.util.assert(Array.isArray(inputShape) && inputShape.length === 2 && Array.isArray(inputShape[0]) && Array.isArray(inputShape[1]), function () {\n        return 'A `Dot` layer should be called on a list of exactly 2 inputs.';\n      });\n      var shape1 = inputShape[0].slice();\n      var shape2 = inputShape[1].slice();\n\n      if (shape1.length > 3 || shape2.length > 3) {\n        throw new NotImplementedError('Dot layer does not support tensors of 4D or higher rank yet.');\n      }\n\n      var axes = this.interpretAxes(shape1, shape2);\n      shape1.splice(axes[0], 1);\n      shape2.splice(axes[1], 1);\n      shape2.splice(0, 1);\n      var outputShape = shape1.concat(shape2);\n\n      if (outputShape.length === 1) {\n        outputShape.push(1);\n      }\n\n      return outputShape;\n    }\n  }, {\n    key: \"computeMask\",\n    value: function computeMask(inputs, mask) {\n      return null;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        'axes': this.axes,\n        'normalize': this.normalize\n      };\n\n      var baseConfig = _get(_getPrototypeOf(Dot.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n\n  return Dot;\n}(Merge);\n/** @nocollapse */\n\nDot.className = 'Dot';\nserialization.registerClass(Dot); // TODO(cais): Add functional interfaces for the merge layers.","map":null,"metadata":{},"sourceType":"module"}