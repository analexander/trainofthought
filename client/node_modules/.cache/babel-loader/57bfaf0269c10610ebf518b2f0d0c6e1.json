{"ast":null,"code":"import _regeneratorRuntime from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/regenerator\";\nimport _asyncToGenerator from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\n\nfunction _createForOfIteratorHelper(o, allowArrayLike) { var it; if (typeof Symbol === \"undefined\" || o[Symbol.iterator] == null) { if (Array.isArray(o) || (it = _unsupportedIterableToArray(o)) || allowArrayLike && o && typeof o.length === \"number\") { if (it) o = it; var i = 0; var F = function F() {}; return { s: F, n: function n() { if (i >= o.length) return { done: true }; return { done: false, value: o[i++] }; }, e: function e(_e) { throw _e; }, f: F }; } throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\"); } var normalCompletion = true, didErr = false, err; return { s: function s() { it = o[Symbol.iterator](); }, n: function n() { var step = it.next(); normalCompletion = step.done; return step; }, e: function e(_e2) { didErr = true; err = _e2; }, f: function f() { try { if (!normalCompletion && it.return != null) it.return(); } finally { if (didErr) throw err; } } }; }\n\nfunction _unsupportedIterableToArray(o, minLen) { if (!o) return; if (typeof o === \"string\") return _arrayLikeToArray(o, minLen); var n = Object.prototype.toString.call(o).slice(8, -1); if (n === \"Object\" && o.constructor) n = o.constructor.name; if (n === \"Map\" || n === \"Set\") return Array.from(o); if (n === \"Arguments\" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return _arrayLikeToArray(o, minLen); }\n\nfunction _arrayLikeToArray(arr, len) { if (len == null || len > arr.length) len = arr.length; for (var i = 0, arr2 = new Array(len); i < len; i++) { arr2[i] = arr[i]; } return arr2; }\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { scalar } from '@tensorflow/tfjs-core';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { disposeTensorsInLogs } from '../logs';\nimport { singletonOrArray, toList } from '../utils/generic_utils';\nimport { standardizeClassWeights, standardizeWeights } from './training_utils'; // Default batch size used during tensor-based validation.\n\nvar DEFAULT_VALIDATION_BATCH_SIZE = 32;\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\n\nfunction standardizeDataIteratorOutput( // Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, iteratorOut) {\n  var xs;\n  var ys;\n  var iteratorOutObj = iteratorOut;\n  xs = iteratorOutObj['xs'];\n  ys = iteratorOutObj['ys'];\n  tfc.util.assert(xs != null && ys != null, function () {\n    return 'A Dataset iterator for fitDataset() is expected to generate ' + 'objects of the form `{xs: xVal, ys: yVal}`, where the two ' + 'values may be `tf.Tensor`, an array of Tensors, or a map of ' + 'string to Tensor.  The provided Dataset instead generates ' + \"\".concat(iteratorOut);\n  });\n  var flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n  var flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n  var batchSize = flattenedXs[0].shape[0];\n  tfc.util.assert(flattenedXs.length === model.inputs.length, function () {\n    return \"LayersModel has \".concat(model.inputs.length, \" inputs, but the dataset \") + \"provides \".concat(flattenedXs.length, \" inputs.  (Expected input keys: \") + \"\".concat(JSON.stringify(model.inputNames), \")\");\n  });\n  tfc.util.assert(flattenedYs.length === model.outputs.length, function () {\n    return \"LayersModel has \".concat(model.outputs.length, \" outputs, but the dataset \") + \"provides \".concat(flattenedYs.length, \" outputs.  (Expected output keys: \") + \"\".concat(JSON.stringify(model.outputNames), \")\");\n  });\n\n  var _loop = function _loop(xIndex) {\n    tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, function () {\n      return \"Batch size mismatch: input \" + \"\".concat(model.inputNames[xIndex], \" has \").concat(flattenedXs[xIndex].shape[0], \"; \") + \"expected  \".concat(batchSize, \" based on input \").concat(model.inputNames[0], \".\");\n    });\n  };\n\n  for (var xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n    _loop(xIndex);\n  }\n\n  var _loop2 = function _loop2(yIndex) {\n    tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, function () {\n      return \"Batch size mismatch: output \" + \"\".concat(model.outputNames[yIndex], \" has \").concat(flattenedYs[yIndex].shape[0], \"; \") + \"expected  \".concat(batchSize, \" based on input \").concat(model.inputNames[0], \".\");\n    });\n  };\n\n  for (var yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n    _loop2(yIndex);\n  }\n\n  return {\n    xs: flattenedXs,\n    ys: flattenedYs\n  };\n}\n\nfunction flattenTensorOrArrayOrMap(inputOrOutput, names, values) {\n  if (values instanceof tfc.Tensor) {\n    return [values];\n  } else if (Array.isArray(values)) {\n    tfc.util.assert(values.length === names.length, function () {\n      return \"Received an array of \".concat(values.length, \" Tensors, but expected \").concat(names.length, \" to match the \").concat(inputOrOutput, \" keys \").concat(names, \".\");\n    });\n    return values;\n  } else {\n    var result = []; // Check that all the required keys are available.\n\n    var _iterator = _createForOfIteratorHelper(names),\n        _step;\n\n    try {\n      for (_iterator.s(); !(_step = _iterator.n()).done;) {\n        var name = _step.value;\n\n        if (values[name] == null) {\n          throw new ValueError(\"The feature data generated by the dataset lacks the required \" + \"\".concat(inputOrOutput, \" key '\").concat(name, \"'.\"));\n        }\n\n        result.push(values[name]);\n      }\n    } catch (err) {\n      _iterator.e(err);\n    } finally {\n      _iterator.f();\n    }\n\n    return result;\n  }\n}\n\nfunction standardizeTensorValidationData(data) {\n  if (data.length === 3) {\n    throw new NotImplementedError('Validation with sample weights is not implemented yet.');\n  }\n\n  return {\n    xs: data[0],\n    ys: data[1]\n  };\n}\n\nexport function fitDataset(_x, _x2, _x3) {\n  return _fitDataset.apply(this, arguments);\n}\n/** Helper function that determines number of steps (batches) per epoch. */\n\nfunction _fitDataset() {\n  _fitDataset = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee( // Type `model` as `any` here to avoid circular dependency w/\n  // training.ts.\n  // tslint:disable-next-line:no-any\n  model, dataset, args) {\n    var hasBatchesPerEpoch, doValidation, valXs, valYs, validationData, trainFunction, outLabels, callbackMetrics, callbacks, verbose, _configureCallbacks, callbackList, history, epoch, dataIterator, epochLogs, stepsDone, batchIndex, iteratorOut, _standardizeDataItera, xs, ys, batchLogs, sampleWeights, standardClassWeights, i, ins, outs, _i, label, out, valOuts, _i2;\n\n    return _regeneratorRuntime.wrap(function _callee$(_context) {\n      while (1) {\n        switch (_context.prev = _context.next) {\n          case 0:\n            hasBatchesPerEpoch = args.batchesPerEpoch != null;\n            tfc.util.assert(model.optimizer != null, function () {\n              return 'You must compile a model before training/testing. Use ' + 'LayersModel.compile(modelCompileConfig).';\n            });\n            tfc.util.assert(args != null, function () {\n              return \"For fitDataset(), the 2nd argument (config) is required, \" + \"but it is not provided in this call.\";\n            });\n            tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), function () {\n              return \"For fitDataset(), config.epochs is expected to be a positive \" + \"integer, but got \".concat(args.epochs);\n            });\n            tfc.util.assert(!hasBatchesPerEpoch || args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch), function () {\n              return \"For fitDataset(), config.batchesPerEpoch is expected to be a \" + \"positive integer if specified, but got \".concat(args.batchesPerEpoch);\n            });\n            tfc.util.assert( // tslint:disable-next-line:no-any\n            args['validationSplit'] == null, function () {\n              return '`validationSplit` is not supported by `fitDataset()`. ' + 'Use validationData instead.';\n            });\n\n            if (!model.isTraining) {\n              _context.next = 8;\n              break;\n            }\n\n            throw new Error('Cannot start training because another fit() call is ongoing.');\n\n          case 8:\n            model.isTraining = true;\n            _context.prev = 9;\n            doValidation = args.validationData != null;\n\n            if (doValidation) {\n              if (isDatasetObject(args.validationData)) {\n                tfc.util.assert(args.validationBatches == null || args.validationBatches > 0 && Number.isInteger(args.validationBatches), function () {\n                  return \"For fitDataset() with dataset-based validation, \" + \"config.validationBatches is expected not to be provided, \" + \"or to be a positive integer, \" + \"but got \".concat(args.validationBatches);\n                });\n              } else {\n                validationData = standardizeTensorValidationData(args.validationData);\n                valXs = validationData.xs;\n                valYs = validationData.ys;\n              }\n            }\n\n            trainFunction = model.makeTrainFunction();\n            outLabels = model.getDedupedMetricsNames();\n\n            if (doValidation) {\n              callbackMetrics = outLabels.slice().concat(outLabels.map(function (n) {\n                return 'val_' + n;\n              }));\n            } else {\n              callbackMetrics = outLabels.slice();\n            }\n\n            callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n            verbose = args.verbose == null ? 1 : args.verbose;\n            _configureCallbacks = configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null, // Batch size determined by the dataset itself.\n            doValidation, callbackMetrics), callbackList = _configureCallbacks.callbackList, history = _configureCallbacks.history;\n            callbackList.setModel(model);\n            model.history = history;\n            _context.next = 22;\n            return callbackList.onTrainBegin();\n\n          case 22:\n            model.stopTraining_ = false;\n            epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n            _context.next = 26;\n            return dataset.iterator();\n\n          case 26:\n            dataIterator = _context.sent;\n\n          case 27:\n            if (!(epoch < args.epochs)) {\n              _context.next = 98;\n              break;\n            }\n\n            epochLogs = {};\n            _context.next = 31;\n            return callbackList.onEpochBegin(epoch);\n\n          case 31:\n            stepsDone = 0;\n            batchIndex = 0;\n\n            if (hasBatchesPerEpoch) {\n              _context.next = 37;\n              break;\n            }\n\n            _context.next = 36;\n            return dataset.iterator();\n\n          case 36:\n            dataIterator = _context.sent;\n\n          case 37:\n            if (!(hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true)) {\n              _context.next = 91;\n              break;\n            }\n\n            _context.next = 40;\n            return dataIterator.next();\n\n          case 40:\n            iteratorOut = _context.sent;\n\n            if (!(hasBatchesPerEpoch && iteratorOut.done)) {\n              _context.next = 44;\n              break;\n            }\n\n            console.warn('You provided `batchesPerEpoch` as ' + \"\".concat(args.batchesPerEpoch, \", \") + 'but your dataset iterator ran out of data after ' + \"\".concat(stepsDone, \" batches; \") + 'interrupting training. Make sure that your ' + 'dataset can generate at least `batchesPerEpoch * epochs` ' + 'batches (in this case, ' + \"\".concat(args.batchesPerEpoch * args.epochs, \" batches). \") + 'You may need to use the repeat() function when building ' + 'your dataset.');\n            return _context.abrupt(\"break\", 91);\n\n          case 44:\n            if (!(iteratorOut.value != null)) {\n              _context.next = 73;\n              break;\n            }\n\n            _standardizeDataItera = standardizeDataIteratorOutput(model, iteratorOut.value), xs = _standardizeDataItera.xs, ys = _standardizeDataItera.ys;\n            batchLogs = {};\n            batchLogs['batch'] = batchIndex;\n            batchLogs['size'] = xs[0].shape[0];\n            _context.next = 51;\n            return callbackList.onBatchBegin(batchIndex, batchLogs);\n\n          case 51:\n            sampleWeights = [];\n\n            if (!(args.classWeight != null)) {\n              _context.next = 64;\n              break;\n            }\n\n            standardClassWeights = standardizeClassWeights(args.classWeight, model.outputNames);\n            i = 0;\n\n          case 55:\n            if (!(i < standardClassWeights.length)) {\n              _context.next = 64;\n              break;\n            }\n\n            _context.t0 = sampleWeights;\n            _context.next = 59;\n            return standardizeWeights(ys[i], null, standardClassWeights[i]);\n\n          case 59:\n            _context.t1 = _context.sent;\n\n            _context.t0.push.call(_context.t0, _context.t1);\n\n          case 61:\n            ++i;\n            _context.next = 55;\n            break;\n\n          case 64:\n            // Train on batch.\n            ins = xs.concat(ys).concat(sampleWeights);\n            outs = trainFunction(ins);\n            tfc.dispose(ins);\n\n            for (_i = 0; _i < outLabels.length; ++_i) {\n              label = outLabels[_i];\n              out = outs[_i];\n              batchLogs[label] = out;\n              tfc.keep(out);\n            }\n\n            _context.next = 70;\n            return callbackList.onBatchEnd(batchIndex, batchLogs);\n\n          case 70:\n            disposeTensorsInLogs(batchLogs);\n            batchIndex++;\n            stepsDone++;\n\n          case 73:\n            if (!(hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch : iteratorOut.done)) {\n              _context.next = 87;\n              break;\n            }\n\n            if (!doValidation) {\n              _context.next = 86;\n              break;\n            }\n\n            valOuts = void 0;\n\n            if (!isDatasetObject(args.validationData)) {\n              _context.next = 84;\n              break;\n            }\n\n            _context.t2 = toList;\n            _context.next = 80;\n            return model.evaluateDataset(args.validationData, {\n              batches: args.validationBatches\n            });\n\n          case 80:\n            _context.t3 = _context.sent;\n            valOuts = (0, _context.t2)(_context.t3);\n            _context.next = 85;\n            break;\n\n          case 84:\n            valOuts = toList(model.evaluate(valXs, valYs, {\n              batchSize: args.validationBatchSize == null ? DEFAULT_VALIDATION_BATCH_SIZE : args.validationBatchSize,\n              verbose: 0\n            }));\n\n          case 85:\n            for (_i2 = 0; _i2 < model.metricsNames.length; ++_i2) {\n              epochLogs[\"val_\".concat(model.metricsNames[_i2])] = valOuts[_i2];\n            }\n\n          case 86:\n            return _context.abrupt(\"break\", 91);\n\n          case 87:\n            if (!model.stopTraining_) {\n              _context.next = 89;\n              break;\n            }\n\n            return _context.abrupt(\"break\", 91);\n\n          case 89:\n            _context.next = 37;\n            break;\n\n          case 91:\n            _context.next = 93;\n            return callbackList.onEpochEnd(epoch, epochLogs);\n\n          case 93:\n            epoch++;\n\n            if (!model.stopTraining_) {\n              _context.next = 96;\n              break;\n            }\n\n            return _context.abrupt(\"break\", 98);\n\n          case 96:\n            _context.next = 27;\n            break;\n\n          case 98:\n            _context.next = 100;\n            return callbackList.onTrainEnd();\n\n          case 100:\n            _context.next = 102;\n            return model.history.syncData();\n\n          case 102:\n            return _context.abrupt(\"return\", model.history);\n\n          case 103:\n            _context.prev = 103;\n            model.isTraining = false;\n            return _context.finish(103);\n\n          case 106:\n          case \"end\":\n            return _context.stop();\n        }\n      }\n    }, _callee, null, [[9,, 103, 106]]);\n  }));\n  return _fitDataset.apply(this, arguments);\n}\n\nfunction getStepsPerEpoch(dataset, args) {\n  // Attempt to determine # of batches in an epoch.\n  var stepsPerEpoch = null;\n\n  if (args.batchesPerEpoch != null) {\n    stepsPerEpoch = args.batchesPerEpoch;\n  } else if (Number.isFinite(dataset.size)) {\n    stepsPerEpoch = dataset.size;\n  }\n\n  return stepsPerEpoch;\n} // Check if provided object is a Dataset object by checking its .iterator\n// element.\n\n\nfunction isDatasetObject(dataset) {\n  return typeof dataset.iterator === 'function';\n} // Check if provided object is a LazyIterator object by checking it's .next\n// element.\n\n\nfunction isLazyIteratorObject(iterator) {\n  return typeof iterator.next === 'function';\n}\n\nexport function evaluateDataset(_x4, _x5, _x6) {\n  return _evaluateDataset.apply(this, arguments);\n}\n\nfunction _evaluateDataset() {\n  _evaluateDataset = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2( // Type `model` as `any` here to avoid circular dependency w/\n  // training.ts.\n  // tslint:disable-next-line:no-any\n  model, dataset, args) {\n    var hasBatches, f, outs, dataIterator, numExamples, batch, _loop3, _ret, i, oldScalar;\n\n    return _regeneratorRuntime.wrap(function _callee2$(_context3) {\n      while (1) {\n        switch (_context3.prev = _context3.next) {\n          case 0:\n            args = args || {};\n            hasBatches = args.batches != null;\n            f = model.testFunction;\n            outs = [];\n\n            if (!(args.verbose > 0)) {\n              _context3.next = 6;\n              break;\n            }\n\n            throw new NotImplementedError('Verbose mode is not implemented yet.');\n\n          case 6:\n            tfc.util.assert(!hasBatches || args.batches > 0 && Number.isInteger(args.batches), function () {\n              return 'Test loop expects `batches` to be a positive integer, but ' + \"received \".concat(JSON.stringify(args.batches));\n            });\n\n            if (!isLazyIteratorObject(dataset)) {\n              _context3.next = 11;\n              break;\n            }\n\n            _context3.t0 = dataset;\n            _context3.next = 14;\n            break;\n\n          case 11:\n            _context3.next = 13;\n            return dataset.iterator();\n\n          case 13:\n            _context3.t0 = _context3.sent;\n\n          case 14:\n            dataIterator = _context3.t0;\n            // Keeps track of number of examples used in this evaluation.\n            numExamples = 0;\n            batch = 0;\n            _loop3 = /*#__PURE__*/_regeneratorRuntime.mark(function _loop3() {\n              var iteratorOut;\n              return _regeneratorRuntime.wrap(function _loop3$(_context2) {\n                while (1) {\n                  switch (_context2.prev = _context2.next) {\n                    case 0:\n                      _context2.next = 2;\n                      return dataIterator.next();\n\n                    case 2:\n                      iteratorOut = _context2.sent;\n                      outs = tfc.tidy(function () {\n                        if (iteratorOut.value) {\n                          (function () {\n                            // TODO(cais): Once real dataset is available, use\n                            //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n                            var _standardizeDataItera2 = standardizeDataIteratorOutput(model, iteratorOut.value),\n                                xs = _standardizeDataItera2.xs,\n                                ys = _standardizeDataItera2.ys;\n\n                            var xsAndYs = xs.concat(ys);\n                            var batchOuts = tfc.tidy(function () {\n                              return f(xsAndYs);\n                            });\n                            tfc.dispose(xsAndYs);\n\n                            if (batch === 0) {\n                              for (var _i3 = 0; _i3 < batchOuts.length; ++_i3) {\n                                outs.push(scalar(0));\n                              }\n                            }\n\n                            var batchSize = xsAndYs[0].shape[0];\n\n                            var _loop4 = function _loop4(_i4) {\n                              var batchOut = batchOuts[_i4];\n                              var oldScalar = outs[_i4];\n                              outs[_i4] = tfc.tidy(function () {\n                                return tfc.add(outs[_i4], tfc.mul(batchSize, batchOut));\n                              });\n\n                              if (batch > 0) {\n                                tfc.dispose(oldScalar);\n                              }\n                            };\n\n                            for (var _i4 = 0; _i4 < batchOuts.length; ++_i4) {\n                              _loop4(_i4);\n                            }\n\n                            tfc.dispose(batchOuts);\n                            numExamples += batchSize;\n                            ++batch;\n                          })();\n                        }\n\n                        return outs;\n                      });\n\n                      if (!iteratorOut.done) {\n                        _context2.next = 7;\n                        break;\n                      }\n\n                      if (hasBatches) {\n                        console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' + 'Interrupting evalution. Make sure that your ' + 'dataset can generate at least `batches` ' + \"batches (in this case, \".concat(args.batches, \" batches). \") + 'You may need to use the repeat() function when building ' + 'your dataset.');\n                      }\n\n                      return _context2.abrupt(\"return\", \"break\");\n\n                    case 7:\n                    case \"end\":\n                      return _context2.stop();\n                  }\n                }\n              }, _loop3);\n            });\n\n          case 18:\n            if (!(hasBatches ? batch < args.batches : true)) {\n              _context3.next = 25;\n              break;\n            }\n\n            return _context3.delegateYield(_loop3(), \"t1\", 20);\n\n          case 20:\n            _ret = _context3.t1;\n\n            if (!(_ret === \"break\")) {\n              _context3.next = 23;\n              break;\n            }\n\n            return _context3.abrupt(\"break\", 25);\n\n          case 23:\n            _context3.next = 18;\n            break;\n\n          case 25:\n            for (i = 0; i < outs.length; ++i) {\n              oldScalar = outs[i];\n              outs[i] = tfc.div(outs[i], numExamples);\n              tfc.dispose(oldScalar);\n            }\n\n            return _context3.abrupt(\"return\", singletonOrArray(outs));\n\n          case 27:\n          case \"end\":\n            return _context3.stop();\n        }\n      }\n    }, _callee2);\n  }));\n  return _evaluateDataset.apply(this, arguments);\n}","map":null,"metadata":{},"sourceType":"module"}