{"ast":null,"code":"import _slicedToArray from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/slicedToArray\";\nimport _defineProperty from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/defineProperty\";\nimport _classCallCheck from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass\";\nimport _possibleConstructorReturn from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";\nimport _get from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/get\";\nimport _getPrototypeOf from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";\nimport _inherits from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/inherits\";\n\nfunction _createForOfIteratorHelper(o, allowArrayLike) { var it; if (typeof Symbol === \"undefined\" || o[Symbol.iterator] == null) { if (Array.isArray(o) || (it = _unsupportedIterableToArray(o)) || allowArrayLike && o && typeof o.length === \"number\") { if (it) o = it; var i = 0; var F = function F() {}; return { s: F, n: function n() { if (i >= o.length) return { done: true }; return { done: false, value: o[i++] }; }, e: function e(_e) { throw _e; }, f: F }; } throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\"); } var normalCompletion = true, didErr = false, err; return { s: function s() { it = o[Symbol.iterator](); }, n: function n() { var step = it.next(); normalCompletion = step.done; return step; }, e: function e(_e2) { didErr = true; err = _e2; }, f: function f() { try { if (!normalCompletion && it.return != null) it.return(); } finally { if (didErr) throw err; } } }; }\n\nfunction _unsupportedIterableToArray(o, minLen) { if (!o) return; if (typeof o === \"string\") return _arrayLikeToArray(o, minLen); var n = Object.prototype.toString.call(o).slice(8, -1); if (n === \"Object\" && o.constructor) n = o.constructor.name; if (n === \"Map\" || n === \"Set\") return Array.from(o); if (n === \"Arguments\" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return _arrayLikeToArray(o, minLen); }\n\nfunction _arrayLikeToArray(arr, len) { if (len == null || len > arr.length) len = arr.length; for (var i = 0, arr2 = new Array(len); i < len; i++) { arr2[i] = arr[i]; } return arr2; }\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Normalization layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { moments, serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\n\nexport function batchNormalization(x, mean, variance, beta, gamma) {\n  var epsilon = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 1e-3;\n  var out;\n\n  if (x.rank === 2) {\n    out = tfc.batchNorm2d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 3) {\n    // TODO(cais): Check rank; give proper error message.\n    out = tfc.batchNorm3d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 4) {\n    out = tfc.batchNorm4d(x, mean, variance, beta, gamma, epsilon);\n  } else {\n    throw new NotImplementedError(\"batchNormalization is not implemented for array of rank \".concat(x.rank, \" \") + \"yet\");\n  }\n\n  return out;\n}\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\n\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  var epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  return tidy(function () {\n    var meanAndVariance = tfc.moments(x, reductionAxes);\n    var mean = meanAndVariance.mean;\n    var variance = meanAndVariance.variance;\n    var normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\n\n\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  var epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  return tidy(function () {\n    var meanAndVariance = tfc.moments(x, reductionAxes);\n    var mean = meanAndVariance.mean;\n    var variance = meanAndVariance.variance;\n    var targetShape = [];\n\n    var _iterator = _createForOfIteratorHelper(math_utils.range(0, x.rank)),\n        _step;\n\n    try {\n      for (_iterator.s(); !(_step = _iterator.n()).done;) {\n        var axis = _step.value;\n\n        if (reductionAxes.indexOf(axis) !== -1) {\n          targetShape.push(1);\n        } else {\n          targetShape.push(x.shape[axis]);\n        }\n      }\n    } catch (err) {\n      _iterator.e(err);\n    } finally {\n      _iterator.f();\n    }\n\n    var broadcastMean = mean.reshape(targetShape);\n    var broadcastVariance = variance.reshape(targetShape);\n    var broadcastGamma = gamma == null ? null : gamma.reshape(targetShape);\n    var broadcastBeta = beta == null ? null : beta.reshape(targetShape);\n    var normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\n\n\nexport function normalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  var epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n\n  if (util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n    return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  } else {\n    return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  }\n}\nexport var BatchNormalization = /*#__PURE__*/function (_Layer) {\n  _inherits(BatchNormalization, _Layer);\n\n  function BatchNormalization(args) {\n    var _this;\n\n    _classCallCheck(this, BatchNormalization);\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this = _possibleConstructorReturn(this, _getPrototypeOf(BatchNormalization).call(this, args));\n    _this.supportsMasking = true;\n    _this.axis = args.axis == null ? -1 : args.axis;\n    _this.momentum = args.momentum == null ? 0.99 : args.momentum;\n    _this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    _this.center = args.center == null ? true : args.center;\n    _this.scale = args.scale == null ? true : args.scale;\n    _this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    _this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    _this.movingMeanInitializer = getInitializer(args.movingMeanInitializer || 'zeros');\n    _this.movingVarianceInitializer = getInitializer(args.movingVarianceInitializer || 'ones');\n    _this.betaConstraint = getConstraint(args.betaConstraint);\n    _this.gammaConstraint = getConstraint(args.gammaConstraint);\n    _this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    _this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    return _this;\n  }\n\n  _createClass(BatchNormalization, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var axis = this.axis >= 0 ? this.axis : this.axis + inputShape.length;\n      var dim = inputShape[axis];\n\n      if (dim == null) {\n        throw new ValueError(\"Axis \".concat(axis, \" of input tensor should have a defined dimension but \") + \"the layer received an input with shape \" + \"\".concat(JSON.stringify(inputShape), \".\"));\n      }\n\n      this.inputSpec = [new InputSpec({\n        ndim: inputShape.length,\n        axes: _defineProperty({}, axis, dim)\n      })];\n      var shape = [dim];\n\n      if (this.scale) {\n        this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n      }\n\n      if (this.center) {\n        this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n      }\n\n      this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n      this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this2 = this;\n\n      return tidy(function () {\n        var training = kwargs['training'] == null ? false : kwargs['training'];\n        var input = getExactlyOneTensor(inputs);\n        var inputShape = input.shape;\n        var ndim = inputShape.length;\n        var reductionAxes = math_utils.range(0, ndim);\n        var axis = _this2.axis >= 0 ? _this2.axis : _this2.axis + ndim;\n        reductionAxes.splice(axis, 1);\n        var broadcastShape = generic_utils.pyListRepeat(1, ndim);\n        broadcastShape[axis] = inputShape[axis];\n        var sortedReductionAxes = reductionAxes.slice();\n        sortedReductionAxes.sort();\n        var needsBroadcasting = !util.arraysEqual(sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n\n        var normalizeInference = function normalizeInference() {\n          if (needsBroadcasting) {\n            var broadcastMovingMean = _this2.movingMean.read().reshape(broadcastShape);\n\n            var broadcastMovingVariance = _this2.movingVariance.read().reshape(broadcastShape);\n\n            var broadcastBeta = _this2.center ? _this2.beta.read().reshape(broadcastShape) : null;\n            var broadcastGamma = _this2.scale ? _this2.gamma.read().reshape(broadcastShape) : null;\n            return batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, _this2.epsilon);\n          } else {\n            return batchNormalization(input, _this2.movingMean.read(), _this2.movingVariance.read(), _this2.beta == null ? null : _this2.beta.read(), _this2.gamma == null ? null : _this2.gamma.read(), _this2.epsilon);\n          }\n        };\n\n        if (!training) {\n          return normalizeInference();\n        }\n\n        var _normalizeBatchInTrai = normalizeBatchInTraining(input, _this2.gamma.read(), _this2.beta.read(), reductionAxes, _this2.epsilon),\n            _normalizeBatchInTrai2 = _slicedToArray(_normalizeBatchInTrai, 3),\n            normedTraining = _normalizeBatchInTrai2[0],\n            mean = _normalizeBatchInTrai2[1],\n            variance = _normalizeBatchInTrai2[2];\n\n        var doMovingAverage = function doMovingAverage(variable, value, momentum) {\n          tfc.tidy(function () {\n            var decay = 1 - momentum;\n            var origValue = variable.read();\n            var updateDelta = origValue.sub(value).mul(decay);\n            variable.write(origValue.sub(updateDelta));\n          });\n        }; // Perform updates to moving mean and moving variance for training.\n        // Porting Note: In PyKeras, these updates to `movingMean` and\n        //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n        //   `update`s using the `add_update()` method. Here we do it imperatively\n        //   and encapsulate the updates in a function that is invoked\n        //   immediately.\n\n\n        var updateMovingMeanAndVariance = function updateMovingMeanAndVariance() {\n          doMovingAverage(_this2.movingMean, mean, _this2.momentum);\n          doMovingAverage(_this2.movingVariance, variance, _this2.momentum);\n        };\n\n        updateMovingMeanAndVariance();\n        return normedTraining;\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        axis: this.axis,\n        momentum: this.momentum,\n        epsilon: this.epsilon,\n        center: this.center,\n        scale: this.scale,\n        betaInitializer: serializeInitializer(this.betaInitializer),\n        gammaInitializer: serializeInitializer(this.gammaInitializer),\n        movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n        movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),\n        betaRegularizer: serializeRegularizer(this.betaRegularizer),\n        gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n        betaConstraint: serializeConstraint(this.betaConstraint),\n        gammaConstraint: serializeConstraint(this.gammaConstraint)\n      };\n\n      var baseConfig = _get(_getPrototypeOf(BatchNormalization.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n\n  return BatchNormalization;\n}(Layer);\n/** @nocollapse */\n\nBatchNormalization.className = 'BatchNormalization';\nserialization.registerClass(BatchNormalization);\nexport var LayerNormalization = /*#__PURE__*/function (_Layer2) {\n  _inherits(LayerNormalization, _Layer2);\n\n  function LayerNormalization(args) {\n    var _this3;\n\n    _classCallCheck(this, LayerNormalization);\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this3 = _possibleConstructorReturn(this, _getPrototypeOf(LayerNormalization).call(this, args));\n    _this3.axis = args.axis == null ? -1 : args.axis;\n\n    if (typeof _this3.axis === 'number') {\n      if (!Number.isInteger(_this3.axis)) {\n        throw new Error(\"Expected axis to be an integer, but received \".concat(_this3.axis));\n      }\n    } else if (Array.isArray(_this3.axis)) {\n      var _iterator2 = _createForOfIteratorHelper(_this3.axis),\n          _step2;\n\n      try {\n        for (_iterator2.s(); !(_step2 = _iterator2.n()).done;) {\n          var axis = _step2.value;\n\n          if (!Number.isInteger(axis)) {\n            throw new Error(\"Expected axis to be an array of integers, \" + \"but received \".concat(JSON.stringify(_this3.axis)));\n          }\n        }\n      } catch (err) {\n        _iterator2.e(err);\n      } finally {\n        _iterator2.f();\n      }\n    } else {\n      throw new Error(\"Expected axis to be an integer or an array of integers, \" + \"but received \".concat(JSON.stringify(_this3.axis)));\n    }\n\n    _this3.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    _this3.center = args.center == null ? true : args.center;\n    _this3.scale = args.scale == null ? true : args.scale;\n    _this3.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    _this3.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    _this3.betaRegularizer = getRegularizer(args.betaRegularizer);\n    _this3.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    _this3.supportsMasking = true;\n    return _this3;\n  }\n\n  _createClass(LayerNormalization, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var nDims = inputShape.length; // Convert axis to array and resolve negatives.\n\n      if (typeof this.axis === 'number') {\n        this.axis = [this.axis];\n      }\n\n      for (var i = 0; i < this.axis.length; ++i) {\n        if (this.axis[i] < 0) {\n          this.axis[i] += nDims;\n        }\n      } // Further validate axes.\n\n\n      var _iterator3 = _createForOfIteratorHelper(this.axis),\n          _step3;\n\n      try {\n        for (_iterator3.s(); !(_step3 = _iterator3.n()).done;) {\n          var axis = _step3.value;\n\n          if (axis < 0 || axis >= nDims) {\n            throw new Error(\"Invalid axis: \".concat(axis));\n          }\n        }\n      } catch (err) {\n        _iterator3.e(err);\n      } finally {\n        _iterator3.f();\n      }\n\n      if (this.axis.length !== generic_utils.unique(this.axis).length) {\n        throw new Error(\"Found duplicate axes in: \".concat(this.axis));\n      }\n\n      var paramShape = this.axis.map(function (axis) {\n        return inputShape[axis];\n      });\n      var trainable = true;\n\n      if (this.scale) {\n        this.gamma = this.addWeight('gamma', paramShape, 'float32', this.gammaInitializer, this.gammaRegularizer, trainable);\n      } else {\n        this.gamma = null;\n      }\n\n      if (this.center) {\n        this.beta = this.addWeight('beta', paramShape, 'float32', this.betaInitializer, this.betaRegularizer, trainable);\n      } else {\n        this.beta = null;\n      }\n\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this4 = this;\n\n      var input = getExactlyOneTensor(inputs);\n      var inputShape = input.shape;\n      var nDims = inputShape.length;\n      return tidy(function () {\n        var keepDims = true;\n\n        var _moments = moments(input, _this4.axis, keepDims),\n            mean = _moments.mean,\n            variance = _moments.variance;\n\n        var broadcastShape = generic_utils.pyListRepeat(1, nDims);\n\n        var _iterator4 = _createForOfIteratorHelper(_this4.axis),\n            _step4;\n\n        try {\n          for (_iterator4.s(); !(_step4 = _iterator4.n()).done;) {\n            var dim = _step4.value;\n            broadcastShape[dim] = inputShape[dim];\n          }\n        } catch (err) {\n          _iterator4.e(err);\n        } finally {\n          _iterator4.f();\n        }\n\n        var broadcast = function broadcast(v) {\n          if (v != null && v.shape.length !== nDims && _this4.axis !== [nDims - 1]) {\n            return v.reshape(broadcastShape);\n          } else {\n            return v;\n          }\n        };\n\n        var scale = broadcast(_this4.gamma.read());\n        var offset = broadcast(_this4.beta.read()); // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n        // is a workaround for the limitation of core's batchNormalization?d don't\n        // support broadcasting in their gradients. In addition, the tiling is\n        // necessary to ensure correctness on the browser CPU backend regardless\n        // of forward or backward computation. Remove this workaround once the\n        // limitation is addressed. See .\n\n        var momentsTiling = [];\n        var scaleOffsetTiling = [];\n\n        for (var i = 0; i < nDims; ++i) {\n          if (_this4.axis.indexOf(i) !== -1) {\n            momentsTiling.push(inputShape[i]);\n            scaleOffsetTiling.push(1);\n          } else {\n            momentsTiling.push(1);\n            scaleOffsetTiling.push(inputShape[i]);\n          }\n        }\n\n        mean = mean.tile(momentsTiling);\n        variance = variance.tile(momentsTiling);\n        scale = scale.tile(scaleOffsetTiling);\n        offset = offset.tile(scaleOffsetTiling);\n        return batchNormalization(input, mean, variance, offset, scale, _this4.epsilon);\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        axis: this.axis,\n        epsilon: this.epsilon,\n        center: this.center,\n        scale: this.scale,\n        betaInitializer: serializeInitializer(this.betaInitializer),\n        gammaInitializer: serializeInitializer(this.gammaInitializer),\n        betaRegularizer: serializeRegularizer(this.betaRegularizer),\n        gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n      };\n\n      var baseConfig = _get(_getPrototypeOf(LayerNormalization.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n\n  return LayerNormalization;\n}(Layer);\n/** @nocollapse */\n\nLayerNormalization.className = 'LayerNormalization';\nserialization.registerClass(LayerNormalization);","map":null,"metadata":{},"sourceType":"module"}