{"ast":null,"code":"import _slicedToArray from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/slicedToArray\";\nimport _toConsumableArray from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/toConsumableArray\";\nimport _classCallCheck from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass\";\nimport _possibleConstructorReturn from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";\nimport _get from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/get\";\nimport _getPrototypeOf from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";\nimport _inherits from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/inherits\";\n\nfunction _createForOfIteratorHelper(o, allowArrayLike) { var it; if (typeof Symbol === \"undefined\" || o[Symbol.iterator] == null) { if (Array.isArray(o) || (it = _unsupportedIterableToArray(o)) || allowArrayLike && o && typeof o.length === \"number\") { if (it) o = it; var i = 0; var F = function F() {}; return { s: F, n: function n() { if (i >= o.length) return { done: true }; return { done: false, value: o[i++] }; }, e: function e(_e) { throw _e; }, f: F }; } throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\"); } var normalCompletion = true, didErr = false, err; return { s: function s() { it = o[Symbol.iterator](); }, n: function n() { var step = it.next(); normalCompletion = step.done; return step; }, e: function e(_e2) { didErr = true; err = _e2; }, f: function f() { try { if (!normalCompletion && it.return != null) it.return(); } finally { if (didErr) throw err; } } }; }\n\nfunction _unsupportedIterableToArray(o, minLen) { if (!o) return; if (typeof o === \"string\") return _arrayLikeToArray(o, minLen); var n = Object.prototype.toString.call(o).slice(8, -1); if (n === \"Object\" && o.constructor) n = o.constructor.name; if (n === \"Map\" || n === \"Set\") return Array.from(o); if (n === \"Arguments\" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return _arrayLikeToArray(o, minLen); }\n\nfunction _arrayLikeToArray(arr, len) { if (len == null || len > arr.length) len = arr.length; for (var i = 0, arr2 = new Array(len); i < len; i++) { arr2[i] = arr[i]; } return arr2; }\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Recurrent Neural Network Layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getActivation, serializeActivation } from '../activations';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, SymbolicTensor } from '../engine/topology';\nimport { Layer } from '../engine/topology';\nimport { AttributeError, NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, Initializer, Ones, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { assertPositiveInteger } from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor, isArrayOfShapes } from '../utils/types_utils';\nimport { batchGetValue, batchSetValue } from '../variables';\nimport { deserialize } from './serialization';\n/**\n * Standardize `apply()` args to a single list of tensor inputs.\n *\n * When running a model loaded from file, the input tensors `initialState` and\n * `constants` are passed to `RNN.apply()` as part of `inputs` instead of the\n * dedicated kwargs fields. `inputs` consists of\n * `[inputs, initialState0, initialState1, ..., constant0, constant1]` in this\n * case.\n * This method makes sure that arguments are\n * separated and that `initialState` and `constants` are `Array`s of tensors\n * (or None).\n *\n * @param inputs Tensor or `Array` of  tensors.\n * @param initialState Tensor or `Array` of tensors or `null`/`undefined`.\n * @param constants Tensor or `Array` of tensors or `null`/`undefined`.\n * @returns An object consisting of\n *   inputs: A tensor.\n *   initialState: `Array` of tensors or `null`.\n *   constants: `Array` of tensors or `null`.\n * @throws ValueError, if `inputs` is an `Array` but either `initialState` or\n *   `constants` is provided.\n */\n\nexport function standardizeArgs(inputs, initialState, constants, numConstants) {\n  if (Array.isArray(inputs)) {\n    if (initialState != null || constants != null) {\n      throw new ValueError('When inputs is an array, neither initialState or constants ' + 'should be provided');\n    }\n\n    if (numConstants != null) {\n      constants = inputs.slice(inputs.length - numConstants, inputs.length);\n      inputs = inputs.slice(0, inputs.length - numConstants);\n    }\n\n    if (inputs.length > 1) {\n      initialState = inputs.slice(1, inputs.length);\n    }\n\n    inputs = inputs[0];\n  }\n\n  function toListOrNull(x) {\n    if (x == null || Array.isArray(x)) {\n      return x;\n    } else {\n      return [x];\n    }\n  }\n\n  initialState = toListOrNull(initialState);\n  constants = toListOrNull(constants);\n  return {\n    inputs: inputs,\n    initialState: initialState,\n    constants: constants\n  };\n}\n/**\n * Iterates over the time dimension of a tensor.\n *\n * @param stepFunction RNN step function.\n *   Parameters:\n *     inputs: tensor with shape `[samples, ...]` (no time dimension),\n *       representing input for the batch of samples at a certain time step.\n *     states: an Array of tensors.\n *   Returns:\n *     outputs: tensor with shape `[samples, outputDim]` (no time dimension).\n *     newStates: list of tensors, same length and shapes as `states`. The first\n *       state in the list must be the output tensor at the previous timestep.\n * @param inputs Tensor of temporal data of shape `[samples, time, ...]` (at\n *   least 3D).\n * @param initialStates Tensor with shape `[samples, outputDim]` (no time\n *   dimension), containing the initial values of the states used in the step\n *   function.\n * @param goBackwards If `true`, do the iteration over the time dimension in\n *   reverse order and return the reversed sequence.\n * @param mask Binary tensor with shape `[sample, time, 1]`, with a zero for\n *   every element that is masked.\n * @param constants An Array of constant values passed at each step.\n * @param unroll Whether to unroll the RNN or to use a symbolic loop. *Not*\n *   applicable to this imperative deeplearn.js backend. Its value is ignored.\n * @param needPerStepOutputs Whether the per-step outputs are to be\n *   concatenated into a single tensor and returned (as the second return\n *   value). Default: `false`. This arg is included so that the relatively\n *   expensive concatenation of the stepwise outputs can be omitted unless\n *   the stepwise outputs need to be kept (e.g., for an LSTM layer of which\n *   `returnSequence` is `true`.)\n * @returns An Array: `[lastOutput, outputs, newStates]`.\n *   lastOutput: the lastest output of the RNN, of shape `[samples, ...]`.\n *   outputs: tensor with shape `[samples, time, ...]` where each entry\n *     `output[s, t]` is the output of the step function at time `t` for sample\n *     `s`. This return value is provided if and only if the\n *     `needPerStepOutputs` is set as `true`. If it is set as `false`, this\n *     return value will be `undefined`.\n *   newStates: Array of tensors, latest states returned by the step function,\n *      of shape `(samples, ...)`.\n * @throws ValueError If input dimension is less than 3.\n *\n * TODO(nielsene): This needs to be tidy-ed.\n */\n\nexport function rnn(stepFunction, inputs, initialStates) {\n  var goBackwards = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : false;\n  var mask = arguments.length > 4 ? arguments[4] : undefined;\n  var constants = arguments.length > 5 ? arguments[5] : undefined;\n  var unroll = arguments.length > 6 && arguments[6] !== undefined ? arguments[6] : false;\n  var needPerStepOutputs = arguments.length > 7 && arguments[7] !== undefined ? arguments[7] : false;\n  return tfc.tidy(function () {\n    var ndim = inputs.shape.length;\n\n    if (ndim < 3) {\n      throw new ValueError(\"Input should be at least 3D, but is \".concat(ndim, \"D.\"));\n    } // Transpose to time-major, i.e., from [batch, time, ...] to [time, batch,\n    // ...].\n\n\n    var axes = [1, 0].concat(math_utils.range(2, ndim));\n    inputs = tfc.transpose(inputs, axes);\n\n    if (constants != null) {\n      throw new NotImplementedError('The rnn() functoin of the deeplearn.js backend does not support ' + 'constants yet.');\n    } // Porting Note: the unroll option is ignored by the imperative backend.\n\n\n    if (unroll) {\n      console.warn('Backend rnn(): the unroll = true option is not applicable to the ' + 'imperative deeplearn.js backend.');\n    }\n\n    if (mask != null) {\n      mask = mask.asType('bool').asType('float32');\n\n      if (mask.rank === ndim - 1) {\n        mask = tfc.expandDims(mask, -1);\n      }\n\n      mask = tfc.transpose(mask, axes);\n    }\n\n    if (goBackwards) {\n      inputs = tfc.reverse(inputs, 0);\n\n      if (mask != null) {\n        mask = tfc.reverse(mask, 0);\n      }\n    } // Porting Note: PyKeras with TensorFlow backend uses a symbolic loop\n    //   (tf.while_loop). But for the imperative deeplearn.js backend, we just\n    //   use the usual TypeScript control flow to iterate over the time steps in\n    //   the inputs.\n    // Porting Note: PyKeras patches a \"_use_learning_phase\" attribute to\n    // outputs.\n    //   This is not idiomatic in TypeScript. The info regarding whether we are\n    //   in a learning (i.e., training) phase for RNN is passed in a different\n    //   way.\n\n\n    var perStepOutputs = [];\n    var lastOutput;\n    var states = initialStates;\n    var timeSteps = inputs.shape[0];\n    var perStepInputs = tfc.unstack(inputs);\n    var perStepMasks;\n\n    if (mask != null) {\n      perStepMasks = tfc.unstack(mask);\n    }\n\n    var _loop = function _loop(t) {\n      var currentInput = perStepInputs[t];\n      var stepOutputs = tfc.tidy(function () {\n        return stepFunction(currentInput, states);\n      });\n\n      if (mask == null) {\n        lastOutput = stepOutputs[0];\n        states = stepOutputs[1];\n      } else {\n        var maskedOutputs = tfc.tidy(function () {\n          var stepMask = perStepMasks[t];\n          var negStepMask = tfc.onesLike(stepMask).sub(stepMask); // TODO(cais): Would tfc.where() be better for performance?\n\n          var output = stepOutputs[0].mul(stepMask).add(states[0].mul(negStepMask));\n          var newStates = states.map(function (state, i) {\n            return stepOutputs[1][i].mul(stepMask).add(state.mul(negStepMask));\n          });\n          return {\n            output: output,\n            newStates: newStates\n          };\n        });\n        lastOutput = maskedOutputs.output;\n        states = maskedOutputs.newStates;\n      }\n\n      if (needPerStepOutputs) {\n        perStepOutputs.push(lastOutput);\n      }\n    };\n\n    for (var t = 0; t < timeSteps; ++t) {\n      _loop(t);\n    }\n\n    var outputs;\n\n    if (needPerStepOutputs) {\n      var axis = 1;\n      outputs = tfc.stack(perStepOutputs, axis);\n    }\n\n    return [lastOutput, outputs, states];\n  });\n}\nexport var RNN = /*#__PURE__*/function (_Layer) {\n  _inherits(RNN, _Layer);\n\n  function RNN(args) {\n    var _this;\n\n    _classCallCheck(this, RNN);\n\n    _this = _possibleConstructorReturn(this, _getPrototypeOf(RNN).call(this, args));\n    var cell;\n\n    if (args.cell == null) {\n      throw new ValueError('cell property is missing for the constructor of RNN.');\n    } else if (Array.isArray(args.cell)) {\n      cell = new StackedRNNCells({\n        cells: args.cell\n      });\n    } else {\n      cell = args.cell;\n    }\n\n    if (cell.stateSize == null) {\n      throw new ValueError('The RNN cell should have an attribute `stateSize` (tuple of ' + 'integers, one integer per RNN state).');\n    }\n\n    _this.cell = cell;\n    _this.returnSequences = args.returnSequences == null ? false : args.returnSequences;\n    _this.returnState = args.returnState == null ? false : args.returnState;\n    _this.goBackwards = args.goBackwards == null ? false : args.goBackwards;\n    _this._stateful = args.stateful == null ? false : args.stateful;\n    _this.unroll = args.unroll == null ? false : args.unroll;\n    _this.supportsMasking = true;\n    _this.inputSpec = [new InputSpec({\n      ndim: 3\n    })];\n    _this.stateSpec = null;\n    _this.states_ = null; // TODO(cais): Add constantsSpec and numConstants.\n\n    _this.numConstants = null; // TODO(cais): Look into the use of initial_state in the kwargs of the\n    //   constructor.\n\n    _this.keptStates = [];\n    return _this;\n  } // Porting Note: This is the equivalent of `RNN.states` property getter in\n  //   PyKeras.\n\n\n  _createClass(RNN, [{\n    key: \"getStates\",\n    value: function getStates() {\n      if (this.states_ == null) {\n        var numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n        return math_utils.range(0, numStates).map(function (x) {\n          return null;\n        });\n      } else {\n        return this.states_;\n      }\n    } // Porting Note: This is the equivalent of the `RNN.states` property setter in\n    //   PyKeras.\n\n  }, {\n    key: \"setStates\",\n    value: function setStates(states) {\n      this.states_ = states;\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      if (isArrayOfShapes(inputShape)) {\n        inputShape = inputShape[0];\n      }\n\n      inputShape = inputShape; // TODO(cais): Remove the casting once stacked RNN cells become supported.\n\n      var stateSize = this.cell.stateSize;\n\n      if (!Array.isArray(stateSize)) {\n        stateSize = [stateSize];\n      }\n\n      var outputDim = stateSize[0];\n      var outputShape;\n\n      if (this.returnSequences) {\n        outputShape = [inputShape[0], inputShape[1], outputDim];\n      } else {\n        outputShape = [inputShape[0], outputDim];\n      }\n\n      if (this.returnState) {\n        var stateShape = [];\n\n        var _iterator = _createForOfIteratorHelper(stateSize),\n            _step;\n\n        try {\n          for (_iterator.s(); !(_step = _iterator.n()).done;) {\n            var dim = _step.value;\n            stateShape.push([inputShape[0], dim]);\n          }\n        } catch (err) {\n          _iterator.e(err);\n        } finally {\n          _iterator.f();\n        }\n\n        return [outputShape].concat(stateShape);\n      } else {\n        return outputShape;\n      }\n    }\n  }, {\n    key: \"computeMask\",\n    value: function computeMask(inputs, mask) {\n      var _this2 = this;\n\n      return tfc.tidy(function () {\n        if (Array.isArray(mask)) {\n          mask = mask[0];\n        }\n\n        var outputMask = _this2.returnSequences ? mask : null;\n\n        if (_this2.returnState) {\n          var stateMask = _this2.states.map(function (s) {\n            return null;\n          });\n\n          return [outputMask].concat(stateMask);\n        } else {\n          return outputMask;\n        }\n      });\n    }\n    /**\n     * Get the current state tensors of the RNN.\n     *\n     * If the state hasn't been set, return an array of `null`s of the correct\n     * length.\n     */\n\n  }, {\n    key: \"build\",\n    value: function build(inputShape) {\n      // Note inputShape will be an Array of Shapes of initial states and\n      // constants if these are passed in apply().\n      var constantShape = null;\n\n      if (this.numConstants != null) {\n        throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n      }\n\n      if (isArrayOfShapes(inputShape)) {\n        inputShape = inputShape[0];\n      }\n\n      inputShape = inputShape;\n      var batchSize = this.stateful ? inputShape[0] : null;\n      var inputDim = inputShape.slice(2);\n      this.inputSpec[0] = new InputSpec({\n        shape: [batchSize, null].concat(_toConsumableArray(inputDim))\n      }); // Allow cell (if RNNCell Layer) to build before we set or validate\n      // stateSpec.\n\n      var stepInputShape = [inputShape[0]].concat(inputShape.slice(2));\n\n      if (constantShape != null) {\n        throw new NotImplementedError('Constants support is not implemented in RNN yet.');\n      } else {\n        this.cell.build(stepInputShape);\n      } // Set or validate stateSpec.\n\n\n      var stateSize;\n\n      if (Array.isArray(this.cell.stateSize)) {\n        stateSize = this.cell.stateSize;\n      } else {\n        stateSize = [this.cell.stateSize];\n      }\n\n      if (this.stateSpec != null) {\n        if (!util.arraysEqual(this.stateSpec.map(function (spec) {\n          return spec.shape[spec.shape.length - 1];\n        }), stateSize)) {\n          throw new ValueError(\"An initialState was passed that is not compatible with \" + \"cell.stateSize. Received stateSpec=\".concat(this.stateSpec, \"; \") + \"However cell.stateSize is \".concat(this.cell.stateSize));\n        }\n      } else {\n        this.stateSpec = stateSize.map(function (dim) {\n          return new InputSpec({\n            shape: [null, dim]\n          });\n        });\n      }\n\n      if (this.stateful) {\n        this.resetStates();\n      }\n    }\n    /**\n     * Reset the state tensors of the RNN.\n     *\n     * If the `states` argument is `undefined` or `null`, will set the\n     * state tensor(s) of the RNN to all-zero tensors of the appropriate\n     * shape(s).\n     *\n     * If `states` is provided, will set the state tensors of the RNN to its\n     * value.\n     *\n     * @param states Optional externally-provided initial states.\n     * @param training Whether this call is done during training. For stateful\n     *   RNNs, this affects whether the old states are kept or discarded. In\n     *   particular, if `training` is `true`, the old states will be kept so\n     *   that subsequent backpropgataion through time (BPTT) may work properly.\n     *   Else, the old states will be discarded.\n     */\n\n  }, {\n    key: \"resetStates\",\n    value: function resetStates(states) {\n      var _this3 = this;\n\n      var training = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : false;\n      tidy(function () {\n        if (!_this3.stateful) {\n          throw new AttributeError('Cannot call resetStates() on an RNN Layer that is not stateful.');\n        }\n\n        var batchSize = _this3.inputSpec[0].shape[0];\n\n        if (batchSize == null) {\n          throw new ValueError('If an RNN is stateful, it needs to know its batch size. Specify ' + 'the batch size of your input tensors: \\n' + '- If using a Sequential model, specify the batch size by ' + 'passing a `batchInputShape` option to your first layer.\\n' + '- If using the functional API, specify the batch size by ' + 'passing a `batchShape` option to your Input layer.');\n        } // Initialize state if null.\n\n\n        if (_this3.states_ == null) {\n          if (Array.isArray(_this3.cell.stateSize)) {\n            _this3.states_ = _this3.cell.stateSize.map(function (dim) {\n              return tfc.zeros([batchSize, dim]);\n            });\n          } else {\n            _this3.states_ = [tfc.zeros([batchSize, _this3.cell.stateSize])];\n          }\n        } else if (states == null) {\n          // Dispose old state tensors.\n          tfc.dispose(_this3.states_); // For stateful RNNs, fully dispose kept old states.\n\n          if (_this3.keptStates != null) {\n            tfc.dispose(_this3.keptStates);\n            _this3.keptStates = [];\n          }\n\n          if (Array.isArray(_this3.cell.stateSize)) {\n            _this3.states_ = _this3.cell.stateSize.map(function (dim) {\n              return tfc.zeros([batchSize, dim]);\n            });\n          } else {\n            _this3.states_[0] = tfc.zeros([batchSize, _this3.cell.stateSize]);\n          }\n        } else {\n          if (!Array.isArray(states)) {\n            states = [states];\n          }\n\n          if (states.length !== _this3.states_.length) {\n            throw new ValueError(\"Layer \".concat(_this3.name, \" expects \").concat(_this3.states_.length, \" state(s), \") + \"but it received \".concat(states.length, \" state value(s). Input \") + \"received: \".concat(states));\n          }\n\n          if (training === true) {\n            // Store old state tensors for complete disposal later, i.e., during\n            // the next no-arg call to this method. We do not dispose the old\n            // states immediately because that BPTT (among other things) require\n            // them.\n            _this3.keptStates.push(_this3.states_.slice());\n          } else {\n            tfc.dispose(_this3.states_);\n          }\n\n          for (var index = 0; index < _this3.states_.length; ++index) {\n            var value = states[index];\n            var dim = Array.isArray(_this3.cell.stateSize) ? _this3.cell.stateSize[index] : _this3.cell.stateSize;\n            var expectedShape = [batchSize, dim];\n\n            if (!util.arraysEqual(value.shape, expectedShape)) {\n              throw new ValueError(\"State \".concat(index, \" is incompatible with layer \").concat(_this3.name, \": \") + \"expected shape=\".concat(expectedShape, \", received shape=\").concat(value.shape));\n            }\n\n            _this3.states_[index] = value;\n          }\n        }\n\n        _this3.states_ = _this3.states_.map(function (state) {\n          return tfc.keep(state.clone());\n        });\n      });\n    }\n  }, {\n    key: \"apply\",\n    value: function apply(inputs, kwargs) {\n      // TODO(cais): Figure out whether initialState is in kwargs or inputs.\n      var initialState = kwargs == null ? null : kwargs['initialState'];\n      var constants = kwargs == null ? null : kwargs['constants'];\n\n      if (kwargs == null) {\n        kwargs = {};\n      }\n\n      var standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n      inputs = standardized.inputs;\n      initialState = standardized.initialState;\n      constants = standardized.constants; // If any of `initial_state` or `constants` are specified and are\n      // `tf.SymbolicTensor`s, then add them to the inputs and temporarily modify\n      // the input_spec to include them.\n\n      var additionalInputs = [];\n      var additionalSpecs = [];\n\n      if (initialState != null) {\n        kwargs['initialState'] = initialState;\n        additionalInputs = additionalInputs.concat(initialState);\n        this.stateSpec = [];\n\n        var _iterator2 = _createForOfIteratorHelper(initialState),\n            _step2;\n\n        try {\n          for (_iterator2.s(); !(_step2 = _iterator2.n()).done;) {\n            var state = _step2.value;\n            this.stateSpec.push(new InputSpec({\n              shape: state.shape\n            }));\n          } // TODO(cais): Use the following instead.\n          // this.stateSpec = initialState.map(state => new InputSpec({shape:\n          // state.shape}));\n\n        } catch (err) {\n          _iterator2.e(err);\n        } finally {\n          _iterator2.f();\n        }\n\n        additionalSpecs = additionalSpecs.concat(this.stateSpec);\n      }\n\n      if (constants != null) {\n        kwargs['constants'] = constants;\n        additionalInputs = additionalInputs.concat(constants); // TODO(cais): Add this.constantsSpec.\n\n        this.numConstants = constants.length;\n      }\n\n      var isTensor = additionalInputs[0] instanceof SymbolicTensor;\n\n      if (isTensor) {\n        // Compute full input spec, including state and constants.\n        var fullInput = [inputs].concat(additionalInputs);\n        var fullInputSpec = this.inputSpec.concat(additionalSpecs); // Perform the call with temporarily replaced inputSpec.\n\n        var originalInputSpec = this.inputSpec;\n        this.inputSpec = fullInputSpec;\n\n        var output = _get(_getPrototypeOf(RNN.prototype), \"apply\", this).call(this, fullInput, kwargs);\n\n        this.inputSpec = originalInputSpec;\n        return output;\n      } else {\n        return _get(_getPrototypeOf(RNN.prototype), \"apply\", this).call(this, inputs, kwargs);\n      }\n    } // tslint:disable-next-line:no-any\n\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this4 = this;\n\n      // Input shape: `[samples, time (padded with zeros), input_dim]`.\n      // Note that the .build() method of subclasses **must** define\n      // this.inputSpec and this.stateSpec owith complete input shapes.\n      return tidy(function () {\n        var mask = kwargs == null ? null : kwargs['mask'];\n        var training = kwargs == null ? null : kwargs['training'];\n        var initialState = kwargs == null ? null : kwargs['initialState'];\n        inputs = getExactlyOneTensor(inputs);\n\n        if (initialState == null) {\n          if (_this4.stateful) {\n            initialState = _this4.states_;\n          } else {\n            initialState = _this4.getInitialState(inputs);\n          }\n        }\n\n        var numStates = Array.isArray(_this4.cell.stateSize) ? _this4.cell.stateSize.length : 1;\n\n        if (initialState.length !== numStates) {\n          throw new ValueError(\"RNN Layer has \".concat(numStates, \" state(s) but was passed \") + \"\".concat(initialState.length, \" initial state(s).\"));\n        }\n\n        if (_this4.unroll) {\n          console.warn('Ignoring unroll = true for RNN layer, due to imperative backend.');\n        }\n\n        var cellCallKwargs = {\n          training: training\n        }; // TODO(cais): Add support for constants.\n\n        var step = function step(inputs, states) {\n          // `inputs` and `states` are concatenated to form a single `Array` of\n          // `tf.Tensor`s as the input to `cell.call()`.\n          var outputs = _this4.cell.call([inputs].concat(states), cellCallKwargs); // Marshall the return value into output and new states.\n\n\n          return [outputs[0], outputs.slice(1)];\n        }; // TODO(cais): Add support for constants.\n\n\n        var rnnOutputs = rnn(step, inputs, initialState, _this4.goBackwards, mask, null, _this4.unroll, _this4.returnSequences);\n        var lastOutput = rnnOutputs[0];\n        var outputs = rnnOutputs[1];\n        var states = rnnOutputs[2];\n\n        if (_this4.stateful) {\n          _this4.resetStates(states, training);\n        }\n\n        var output = _this4.returnSequences ? outputs : lastOutput; // TODO(cais): Porperty set learning phase flag.\n\n        if (_this4.returnState) {\n          return [output].concat(states);\n        } else {\n          return output;\n        }\n      });\n    }\n  }, {\n    key: \"getInitialState\",\n    value: function getInitialState(inputs) {\n      var _this5 = this;\n\n      return tidy(function () {\n        // Build an all-zero tensor of shape [samples, outputDim].\n        // [Samples, timeSteps, inputDim].\n        var initialState = tfc.zeros(inputs.shape); // [Samples].\n\n        initialState = tfc.sum(initialState, [1, 2]);\n        initialState = K.expandDims(initialState); // [Samples, 1].\n\n        if (Array.isArray(_this5.cell.stateSize)) {\n          return _this5.cell.stateSize.map(function (dim) {\n            return dim > 1 ? K.tile(initialState, [1, dim]) : initialState;\n          });\n        } else {\n          return _this5.cell.stateSize > 1 ? [K.tile(initialState, [1, _this5.cell.stateSize])] : [initialState];\n        }\n      });\n    }\n  }, {\n    key: \"setFastWeightInitDuringBuild\",\n    value: function setFastWeightInitDuringBuild(value) {\n      _get(_getPrototypeOf(RNN.prototype), \"setFastWeightInitDuringBuild\", this).call(this, value);\n\n      if (this.cell != null) {\n        this.cell.setFastWeightInitDuringBuild(value);\n      }\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(RNN.prototype), \"getConfig\", this).call(this);\n\n      var config = {\n        returnSequences: this.returnSequences,\n        returnState: this.returnState,\n        goBackwards: this.goBackwards,\n        stateful: this.stateful,\n        unroll: this.unroll\n      };\n\n      if (this.numConstants != null) {\n        config['numConstants'] = this.numConstants;\n      }\n\n      var cellConfig = this.cell.getConfig();\n\n      if (this.getClassName() === RNN.className) {\n        config['cell'] = {\n          'className': this.cell.getClassName(),\n          'config': cellConfig\n        };\n      } // this order is necessary, to prevent cell name from replacing layer name\n\n\n      return Object.assign({}, cellConfig, baseConfig, config);\n    }\n    /** @nocollapse */\n\n  }, {\n    key: \"states\",\n    get: function get() {\n      if (this.states_ == null) {\n        var numStates = Array.isArray(this.cell.stateSize) ? this.cell.stateSize.length : 1;\n        var output = [];\n\n        for (var i = 0; i < numStates; ++i) {\n          output.push(null);\n        }\n\n        return output;\n      } else {\n        return this.states_;\n      }\n    },\n    set: function set(s) {\n      this.states_ = s;\n    }\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      if (!this.trainable) {\n        return [];\n      } // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n\n\n      return this.cell.trainableWeights;\n    }\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      // Porting Note: In TypeScript, `this` is always an instance of `Layer`.\n      if (!this.trainable) {\n        return this.cell.weights;\n      }\n\n      return this.cell.nonTrainableWeights;\n    }\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      var customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n      var cellConfig = config['cell'];\n      var cell = deserialize(cellConfig, customObjects);\n      return new cls(Object.assign(config, {\n        cell: cell\n      }));\n    }\n  }]);\n\n  return RNN;\n}(Layer);\n/** @nocollapse */\n\nRNN.className = 'RNN';\nserialization.registerClass(RNN); // Porting Note: This is a common parent class for RNN cells. There is no\n// equivalent of this in PyKeras. Having a common parent class forgoes the\n//  need for `has_attr(cell, ...)` checks or its TypeScript equivalent.\n\n/**\n * An RNNCell layer.\n *\n * @doc {heading: 'Layers', subheading: 'Classes'}\n */\n\nexport var RNNCell = /*#__PURE__*/function (_Layer2) {\n  _inherits(RNNCell, _Layer2);\n\n  function RNNCell() {\n    _classCallCheck(this, RNNCell);\n\n    return _possibleConstructorReturn(this, _getPrototypeOf(RNNCell).apply(this, arguments));\n  }\n\n  return RNNCell;\n}(Layer);\nexport var SimpleRNNCell = /*#__PURE__*/function (_RNNCell) {\n  _inherits(SimpleRNNCell, _RNNCell);\n\n  function SimpleRNNCell(args) {\n    var _this6;\n\n    _classCallCheck(this, SimpleRNNCell);\n\n    _this6 = _possibleConstructorReturn(this, _getPrototypeOf(SimpleRNNCell).call(this, args));\n    _this6.DEFAULT_ACTIVATION = 'tanh';\n    _this6.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    _this6.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    _this6.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    _this6.units = args.units;\n    assertPositiveInteger(_this6.units, \"units\");\n    _this6.activation = getActivation(args.activation == null ? _this6.DEFAULT_ACTIVATION : args.activation);\n    _this6.useBias = args.useBias == null ? true : args.useBias;\n    _this6.kernelInitializer = getInitializer(args.kernelInitializer || _this6.DEFAULT_KERNEL_INITIALIZER);\n    _this6.recurrentInitializer = getInitializer(args.recurrentInitializer || _this6.DEFAULT_RECURRENT_INITIALIZER);\n    _this6.biasInitializer = getInitializer(args.biasInitializer || _this6.DEFAULT_BIAS_INITIALIZER);\n    _this6.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    _this6.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    _this6.biasRegularizer = getRegularizer(args.biasRegularizer);\n    _this6.kernelConstraint = getConstraint(args.kernelConstraint);\n    _this6.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    _this6.biasConstraint = getConstraint(args.biasConstraint);\n    _this6.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    _this6.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    _this6.stateSize = _this6.units;\n    _this6.dropoutMask = null;\n    _this6.recurrentDropoutMask = null;\n    return _this6;\n  }\n\n  _createClass(SimpleRNNCell, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape); // TODO(cais): Use regularizer.\n\n      this.kernel = this.addWeight('kernel', [inputShape[inputShape.length - 1], this.units], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n      this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n\n      if (this.useBias) {\n        this.bias = this.addWeight('bias', [this.units], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n      } else {\n        this.bias = null;\n      }\n\n      this.built = true;\n    } // Porting Note: PyKeras' equivalent of this method takes two tensor inputs:\n    //   `inputs` and `states`. Here, the two tensors are combined into an\n    //   `Tensor[]` Array as the first input argument.\n    //   Similarly, PyKeras' equivalent of this method returns two values:\n    //    `output` and `[output]`. Here the two are combined into one length-2\n    //    `Tensor[]`, consisting of `output` repeated.\n\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this7 = this;\n\n      return tidy(function () {\n        inputs = inputs;\n\n        if (inputs.length !== 2) {\n          throw new ValueError(\"SimpleRNNCell expects 2 input Tensors, got \".concat(inputs.length, \".\"));\n        }\n\n        var prevOutput = inputs[1];\n        inputs = inputs[0];\n        var training = kwargs['training'] == null ? false : kwargs['training'];\n\n        if (0 < _this7.dropout && _this7.dropout < 1 && _this7.dropoutMask == null) {\n          _this7.dropoutMask = generateDropoutMask({\n            ones: function ones() {\n              return tfc.onesLike(inputs);\n            },\n            rate: _this7.dropout,\n            training: training\n          });\n        }\n\n        if (0 < _this7.recurrentDropout && _this7.recurrentDropout < 1 && _this7.recurrentDropoutMask == null) {\n          _this7.recurrentDropoutMask = generateDropoutMask({\n            ones: function ones() {\n              return tfc.onesLike(prevOutput);\n            },\n            rate: _this7.recurrentDropout,\n            training: training\n          });\n        }\n\n        var h;\n        var dpMask = _this7.dropoutMask;\n        var recDpMask = _this7.recurrentDropoutMask;\n\n        if (dpMask != null) {\n          h = K.dot(tfc.mul(inputs, dpMask), _this7.kernel.read());\n        } else {\n          h = K.dot(inputs, _this7.kernel.read());\n        }\n\n        if (_this7.bias != null) {\n          h = K.biasAdd(h, _this7.bias.read());\n        }\n\n        if (recDpMask != null) {\n          prevOutput = tfc.mul(prevOutput, recDpMask);\n        }\n\n        var output = tfc.add(h, K.dot(prevOutput, _this7.recurrentKernel.read()));\n\n        if (_this7.activation != null) {\n          output = _this7.activation.apply(output);\n        } // TODO(cais): Properly set learning phase on output tensor?\n\n\n        return [output, output];\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(SimpleRNNCell.prototype), \"getConfig\", this).call(this);\n\n      var config = {\n        units: this.units,\n        activation: serializeActivation(this.activation),\n        useBias: this.useBias,\n        kernelInitializer: serializeInitializer(this.kernelInitializer),\n        recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n        biasInitializer: serializeInitializer(this.biasInitializer),\n        kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n        recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n        biasRegularizer: serializeRegularizer(this.biasRegularizer),\n        activityRegularizer: serializeRegularizer(this.activityRegularizer),\n        kernelConstraint: serializeConstraint(this.kernelConstraint),\n        recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n        biasConstraint: serializeConstraint(this.biasConstraint),\n        dropout: this.dropout,\n        recurrentDropout: this.recurrentDropout\n      };\n      return Object.assign({}, baseConfig, config);\n    }\n  }]);\n\n  return SimpleRNNCell;\n}(RNNCell);\n/** @nocollapse */\n\nSimpleRNNCell.className = 'SimpleRNNCell';\nserialization.registerClass(SimpleRNNCell);\nexport var SimpleRNN = /*#__PURE__*/function (_RNN) {\n  _inherits(SimpleRNN, _RNN);\n\n  function SimpleRNN(args) {\n    _classCallCheck(this, SimpleRNN);\n\n    args.cell = new SimpleRNNCell(args);\n    return _possibleConstructorReturn(this, _getPrototypeOf(SimpleRNN).call(this, args)); // TODO(cais): Add activityRegularizer.\n  }\n\n  _createClass(SimpleRNN, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this8 = this;\n\n      return tidy(function () {\n        if (_this8.cell.dropoutMask != null) {\n          tfc.dispose(_this8.cell.dropoutMask);\n          _this8.cell.dropoutMask = null;\n        }\n\n        if (_this8.cell.recurrentDropoutMask != null) {\n          tfc.dispose(_this8.cell.recurrentDropoutMask);\n          _this8.cell.recurrentDropoutMask = null;\n        }\n\n        var mask = kwargs == null ? null : kwargs['mask'];\n        var training = kwargs == null ? null : kwargs['training'];\n        var initialState = kwargs == null ? null : kwargs['initialState'];\n        return _get(_getPrototypeOf(SimpleRNN.prototype), \"call\", _this8).call(_this8, inputs, {\n          mask: mask,\n          training: training,\n          initialState: initialState\n        });\n      });\n    }\n    /** @nocollapse */\n\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      return new cls(config);\n    }\n  }]);\n\n  return SimpleRNN;\n}(RNN);\n/** @nocollapse */\n\nSimpleRNN.className = 'SimpleRNN';\nserialization.registerClass(SimpleRNN);\nexport var GRUCell = /*#__PURE__*/function (_RNNCell2) {\n  _inherits(GRUCell, _RNNCell2);\n\n  function GRUCell(args) {\n    var _this9;\n\n    _classCallCheck(this, GRUCell);\n\n    _this9 = _possibleConstructorReturn(this, _getPrototypeOf(GRUCell).call(this, args));\n    _this9.DEFAULT_ACTIVATION = 'tanh';\n    _this9.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n    _this9.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    _this9.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    _this9.DEFAULT_BIAS_INITIALIZER = 'zeros';\n\n    if (args.resetAfter) {\n      throw new ValueError(\"GRUCell does not support reset_after parameter set to true.\");\n    }\n\n    _this9.units = args.units;\n    assertPositiveInteger(_this9.units, 'units');\n    _this9.activation = getActivation(args.activation === undefined ? _this9.DEFAULT_ACTIVATION : args.activation);\n    _this9.recurrentActivation = getActivation(args.recurrentActivation === undefined ? _this9.DEFAULT_RECURRENT_ACTIVATION : args.recurrentActivation);\n    _this9.useBias = args.useBias == null ? true : args.useBias;\n    _this9.kernelInitializer = getInitializer(args.kernelInitializer || _this9.DEFAULT_KERNEL_INITIALIZER);\n    _this9.recurrentInitializer = getInitializer(args.recurrentInitializer || _this9.DEFAULT_RECURRENT_INITIALIZER);\n    _this9.biasInitializer = getInitializer(args.biasInitializer || _this9.DEFAULT_BIAS_INITIALIZER);\n    _this9.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    _this9.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    _this9.biasRegularizer = getRegularizer(args.biasRegularizer);\n    _this9.kernelConstraint = getConstraint(args.kernelConstraint);\n    _this9.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    _this9.biasConstraint = getConstraint(args.biasConstraint);\n    _this9.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    _this9.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    _this9.implementation = args.implementation;\n    _this9.stateSize = _this9.units;\n    _this9.dropoutMask = null;\n    _this9.recurrentDropoutMask = null;\n    return _this9;\n  }\n\n  _createClass(GRUCell, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var inputDim = inputShape[inputShape.length - 1];\n      this.kernel = this.addWeight('kernel', [inputDim, this.units * 3], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n      this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 3], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n\n      if (this.useBias) {\n        this.bias = this.addWeight('bias', [this.units * 3], null, this.biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n      } else {\n        this.bias = null;\n      } // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n      //   of the weights and bias in the call() method, at execution time.\n\n\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this10 = this;\n\n      return tidy(function () {\n        inputs = inputs;\n\n        if (inputs.length !== 2) {\n          throw new ValueError(\"GRUCell expects 2 input Tensors (inputs, h, c), got \" + \"\".concat(inputs.length, \".\"));\n        }\n\n        var training = kwargs['training'] == null ? false : kwargs['training'];\n        var hTMinus1 = inputs[1]; // Previous memory state.\n\n        inputs = inputs[0]; // Note: For superior performance, TensorFlow.js always uses\n        // implementation 2, regardless of the actual value of\n        // config.implementation.\n\n        if (0 < _this10.dropout && _this10.dropout < 1 && _this10.dropoutMask == null) {\n          _this10.dropoutMask = generateDropoutMask({\n            ones: function ones() {\n              return tfc.onesLike(inputs);\n            },\n            rate: _this10.dropout,\n            training: training,\n            count: 3\n          });\n        }\n\n        if (0 < _this10.recurrentDropout && _this10.recurrentDropout < 1 && _this10.recurrentDropoutMask == null) {\n          _this10.recurrentDropoutMask = generateDropoutMask({\n            ones: function ones() {\n              return tfc.onesLike(hTMinus1);\n            },\n            rate: _this10.recurrentDropout,\n            training: training,\n            count: 3\n          });\n        }\n\n        var dpMask = _this10.dropoutMask;\n        var recDpMask = _this10.recurrentDropoutMask;\n        var z;\n        var r;\n        var hh;\n\n        if (0 < _this10.dropout && _this10.dropout < 1) {\n          inputs = tfc.mul(inputs, dpMask[0]);\n        }\n\n        var matrixX = K.dot(inputs, _this10.kernel.read());\n\n        if (_this10.useBias) {\n          matrixX = K.biasAdd(matrixX, _this10.bias.read());\n        }\n\n        if (0 < _this10.recurrentDropout && _this10.recurrentDropout < 1) {\n          hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n        }\n\n        var recurrentKernelValue = _this10.recurrentKernel.read();\n\n        var _tfc$split = tfc.split(recurrentKernelValue, [2 * _this10.units, _this10.units], recurrentKernelValue.rank - 1),\n            _tfc$split2 = _slicedToArray(_tfc$split, 2),\n            rk1 = _tfc$split2[0],\n            rk2 = _tfc$split2[1];\n\n        var matrixInner = K.dot(hTMinus1, rk1);\n\n        var _tfc$split3 = tfc.split(matrixX, 3, matrixX.rank - 1),\n            _tfc$split4 = _slicedToArray(_tfc$split3, 3),\n            xZ = _tfc$split4[0],\n            xR = _tfc$split4[1],\n            xH = _tfc$split4[2];\n\n        var _tfc$split5 = tfc.split(matrixInner, 2, matrixInner.rank - 1),\n            _tfc$split6 = _slicedToArray(_tfc$split5, 2),\n            recurrentZ = _tfc$split6[0],\n            recurrentR = _tfc$split6[1];\n\n        z = _this10.recurrentActivation.apply(tfc.add(xZ, recurrentZ));\n        r = _this10.recurrentActivation.apply(tfc.add(xR, recurrentR));\n        var recurrentH = K.dot(tfc.mul(r, hTMinus1), rk2);\n        hh = _this10.activation.apply(tfc.add(xH, recurrentH));\n        var h = tfc.add(tfc.mul(z, hTMinus1), tfc.mul(tfc.add(1, tfc.neg(z)), hh)); // TODO(cais): Add use_learning_phase flag properly.\n\n        return [h, h];\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(GRUCell.prototype), \"getConfig\", this).call(this);\n\n      var config = {\n        units: this.units,\n        activation: serializeActivation(this.activation),\n        recurrentActivation: serializeActivation(this.recurrentActivation),\n        useBias: this.useBias,\n        kernelInitializer: serializeInitializer(this.kernelInitializer),\n        recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n        biasInitializer: serializeInitializer(this.biasInitializer),\n        kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n        recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n        biasRegularizer: serializeRegularizer(this.biasRegularizer),\n        activityRegularizer: serializeRegularizer(this.activityRegularizer),\n        kernelConstraint: serializeConstraint(this.kernelConstraint),\n        recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n        biasConstraint: serializeConstraint(this.biasConstraint),\n        dropout: this.dropout,\n        recurrentDropout: this.recurrentDropout,\n        implementation: this.implementation,\n        resetAfter: false\n      };\n      return Object.assign({}, baseConfig, config);\n    }\n  }]);\n\n  return GRUCell;\n}(RNNCell);\n/** @nocollapse */\n\nGRUCell.className = 'GRUCell';\nserialization.registerClass(GRUCell);\nexport var GRU = /*#__PURE__*/function (_RNN2) {\n  _inherits(GRU, _RNN2);\n\n  function GRU(args) {\n    _classCallCheck(this, GRU);\n\n    if (args.implementation === 0) {\n      console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n    }\n\n    args.cell = new GRUCell(args);\n    return _possibleConstructorReturn(this, _getPrototypeOf(GRU).call(this, args)); // TODO(cais): Add activityRegularizer.\n  }\n\n  _createClass(GRU, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this11 = this;\n\n      return tidy(function () {\n        if (_this11.cell.dropoutMask != null) {\n          tfc.dispose(_this11.cell.dropoutMask);\n          _this11.cell.dropoutMask = null;\n        }\n\n        if (_this11.cell.recurrentDropoutMask != null) {\n          tfc.dispose(_this11.cell.recurrentDropoutMask);\n          _this11.cell.recurrentDropoutMask = null;\n        }\n\n        var mask = kwargs == null ? null : kwargs['mask'];\n        var training = kwargs == null ? null : kwargs['training'];\n        var initialState = kwargs == null ? null : kwargs['initialState'];\n        return _get(_getPrototypeOf(GRU.prototype), \"call\", _this11).call(_this11, inputs, {\n          mask: mask,\n          training: training,\n          initialState: initialState\n        });\n      });\n    }\n    /** @nocollapse */\n\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      if (config['implmentation'] === 0) {\n        config['implementation'] = 1;\n      }\n\n      return new cls(config);\n    }\n  }]);\n\n  return GRU;\n}(RNN);\n/** @nocollapse */\n\nGRU.className = 'GRU';\nserialization.registerClass(GRU);\nexport var LSTMCell = /*#__PURE__*/function (_RNNCell3) {\n  _inherits(LSTMCell, _RNNCell3);\n\n  function LSTMCell(args) {\n    var _this12;\n\n    _classCallCheck(this, LSTMCell);\n\n    _this12 = _possibleConstructorReturn(this, _getPrototypeOf(LSTMCell).call(this, args));\n    _this12.DEFAULT_ACTIVATION = 'tanh';\n    _this12.DEFAULT_RECURRENT_ACTIVATION = 'hardSigmoid';\n    _this12.DEFAULT_KERNEL_INITIALIZER = 'glorotNormal';\n    _this12.DEFAULT_RECURRENT_INITIALIZER = 'orthogonal';\n    _this12.DEFAULT_BIAS_INITIALIZER = 'zeros';\n    _this12.units = args.units;\n    assertPositiveInteger(_this12.units, 'units');\n    _this12.activation = getActivation(args.activation === undefined ? _this12.DEFAULT_ACTIVATION : args.activation);\n    _this12.recurrentActivation = getActivation(args.recurrentActivation === undefined ? _this12.DEFAULT_RECURRENT_ACTIVATION : args.recurrentActivation);\n    _this12.useBias = args.useBias == null ? true : args.useBias;\n    _this12.kernelInitializer = getInitializer(args.kernelInitializer || _this12.DEFAULT_KERNEL_INITIALIZER);\n    _this12.recurrentInitializer = getInitializer(args.recurrentInitializer || _this12.DEFAULT_RECURRENT_INITIALIZER);\n    _this12.biasInitializer = getInitializer(args.biasInitializer || _this12.DEFAULT_BIAS_INITIALIZER);\n    _this12.unitForgetBias = args.unitForgetBias;\n    _this12.kernelRegularizer = getRegularizer(args.kernelRegularizer);\n    _this12.recurrentRegularizer = getRegularizer(args.recurrentRegularizer);\n    _this12.biasRegularizer = getRegularizer(args.biasRegularizer);\n    _this12.kernelConstraint = getConstraint(args.kernelConstraint);\n    _this12.recurrentConstraint = getConstraint(args.recurrentConstraint);\n    _this12.biasConstraint = getConstraint(args.biasConstraint);\n    _this12.dropout = math_utils.min([1, math_utils.max([0, args.dropout == null ? 0 : args.dropout])]);\n    _this12.recurrentDropout = math_utils.min([1, math_utils.max([0, args.recurrentDropout == null ? 0 : args.recurrentDropout])]);\n    _this12.implementation = args.implementation;\n    _this12.stateSize = [_this12.units, _this12.units];\n    _this12.dropoutMask = null;\n    _this12.recurrentDropoutMask = null;\n    return _this12;\n  }\n\n  _createClass(LSTMCell, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      var _a;\n\n      inputShape = getExactlyOneShape(inputShape);\n      var inputDim = inputShape[inputShape.length - 1];\n      this.kernel = this.addWeight('kernel', [inputDim, this.units * 4], null, this.kernelInitializer, this.kernelRegularizer, true, this.kernelConstraint);\n      this.recurrentKernel = this.addWeight('recurrent_kernel', [this.units, this.units * 4], null, this.recurrentInitializer, this.recurrentRegularizer, true, this.recurrentConstraint);\n      var biasInitializer;\n\n      if (this.useBias) {\n        if (this.unitForgetBias) {\n          var capturedBiasInit = this.biasInitializer;\n          var capturedUnits = this.units;\n          biasInitializer = new (_a = /*#__PURE__*/function (_Initializer) {\n            _inherits(CustomInit, _Initializer);\n\n            function CustomInit() {\n              _classCallCheck(this, CustomInit);\n\n              return _possibleConstructorReturn(this, _getPrototypeOf(CustomInit).apply(this, arguments));\n            }\n\n            _createClass(CustomInit, [{\n              key: \"apply\",\n              value: function apply(shape, dtype) {\n                // TODO(cais): More informative variable names?\n                var bI = capturedBiasInit.apply([capturedUnits]);\n                var bF = new Ones().apply([capturedUnits]);\n                var bCAndH = capturedBiasInit.apply([capturedUnits * 2]);\n                return K.concatAlongFirstAxis(K.concatAlongFirstAxis(bI, bF), bCAndH);\n              }\n            }]);\n\n            return CustomInit;\n          }(Initializer),\n          /** @nocollapse */\n          _a.className = 'CustomInit', _a)();\n        } else {\n          biasInitializer = this.biasInitializer;\n        }\n\n        this.bias = this.addWeight('bias', [this.units * 4], null, biasInitializer, this.biasRegularizer, true, this.biasConstraint);\n      } else {\n        this.bias = null;\n      } // Porting Notes: Unlike the PyKeras implementation, we perform slicing\n      //   of the weights and bias in the call() method, at execution time.\n\n\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this13 = this;\n\n      return tidy(function () {\n        var training = kwargs['training'] == null ? false : kwargs['training'];\n        inputs = inputs;\n\n        if (inputs.length !== 3) {\n          throw new ValueError(\"LSTMCell expects 3 input Tensors (inputs, h, c), got \" + \"\".concat(inputs.length, \".\"));\n        }\n\n        var hTMinus1 = inputs[1]; // Previous memory state.\n\n        var cTMinus1 = inputs[2]; // Previous carry state.\n\n        inputs = inputs[0];\n\n        if (0 < _this13.dropout && _this13.dropout < 1 && _this13.dropoutMask == null) {\n          _this13.dropoutMask = generateDropoutMask({\n            ones: function ones() {\n              return tfc.onesLike(inputs);\n            },\n            rate: _this13.dropout,\n            training: training,\n            count: 4\n          });\n        }\n\n        if (0 < _this13.recurrentDropout && _this13.recurrentDropout < 1 && _this13.recurrentDropoutMask == null) {\n          _this13.recurrentDropoutMask = generateDropoutMask({\n            ones: function ones() {\n              return tfc.onesLike(hTMinus1);\n            },\n            rate: _this13.recurrentDropout,\n            training: training,\n            count: 4\n          });\n        }\n\n        var dpMask = _this13.dropoutMask;\n        var recDpMask = _this13.recurrentDropoutMask; // Note: For superior performance, TensorFlow.js always uses\n        // implementation 2 regardless of the actual value of\n        // config.implementation.\n\n        var i;\n        var f;\n        var c;\n        var o;\n\n        if (0 < _this13.dropout && _this13.dropout < 1) {\n          inputs = tfc.mul(inputs, dpMask[0]);\n        }\n\n        var z = K.dot(inputs, _this13.kernel.read());\n\n        if (0 < _this13.recurrentDropout && _this13.recurrentDropout < 1) {\n          hTMinus1 = tfc.mul(hTMinus1, recDpMask[0]);\n        }\n\n        z = tfc.add(z, K.dot(hTMinus1, _this13.recurrentKernel.read()));\n\n        if (_this13.useBias) {\n          z = K.biasAdd(z, _this13.bias.read());\n        }\n\n        var _tfc$split7 = tfc.split(z, 4, z.rank - 1),\n            _tfc$split8 = _slicedToArray(_tfc$split7, 4),\n            z0 = _tfc$split8[0],\n            z1 = _tfc$split8[1],\n            z2 = _tfc$split8[2],\n            z3 = _tfc$split8[3];\n\n        i = _this13.recurrentActivation.apply(z0);\n        f = _this13.recurrentActivation.apply(z1);\n        c = tfc.add(tfc.mul(f, cTMinus1), tfc.mul(i, _this13.activation.apply(z2)));\n        o = _this13.recurrentActivation.apply(z3);\n        var h = tfc.mul(o, _this13.activation.apply(c)); // TODO(cais): Add use_learning_phase flag properly.\n\n        return [h, h, c];\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(LSTMCell.prototype), \"getConfig\", this).call(this);\n\n      var config = {\n        units: this.units,\n        activation: serializeActivation(this.activation),\n        recurrentActivation: serializeActivation(this.recurrentActivation),\n        useBias: this.useBias,\n        kernelInitializer: serializeInitializer(this.kernelInitializer),\n        recurrentInitializer: serializeInitializer(this.recurrentInitializer),\n        biasInitializer: serializeInitializer(this.biasInitializer),\n        unitForgetBias: this.unitForgetBias,\n        kernelRegularizer: serializeRegularizer(this.kernelRegularizer),\n        recurrentRegularizer: serializeRegularizer(this.recurrentRegularizer),\n        biasRegularizer: serializeRegularizer(this.biasRegularizer),\n        activityRegularizer: serializeRegularizer(this.activityRegularizer),\n        kernelConstraint: serializeConstraint(this.kernelConstraint),\n        recurrentConstraint: serializeConstraint(this.recurrentConstraint),\n        biasConstraint: serializeConstraint(this.biasConstraint),\n        dropout: this.dropout,\n        recurrentDropout: this.recurrentDropout,\n        implementation: this.implementation\n      };\n      return Object.assign({}, baseConfig, config);\n    }\n  }]);\n\n  return LSTMCell;\n}(RNNCell);\n/** @nocollapse */\n\nLSTMCell.className = 'LSTMCell';\nserialization.registerClass(LSTMCell);\nexport var LSTM = /*#__PURE__*/function (_RNN3) {\n  _inherits(LSTM, _RNN3);\n\n  function LSTM(args) {\n    _classCallCheck(this, LSTM);\n\n    if (args.implementation === 0) {\n      console.warn('`implementation=0` has been deprecated, and now defaults to ' + '`implementation=1`. Please update your layer call.');\n    }\n\n    args.cell = new LSTMCell(args);\n    return _possibleConstructorReturn(this, _getPrototypeOf(LSTM).call(this, args)); // TODO(cais): Add activityRegularizer.\n  }\n\n  _createClass(LSTM, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this14 = this;\n\n      return tidy(function () {\n        if (_this14.cell.dropoutMask != null) {\n          tfc.dispose(_this14.cell.dropoutMask);\n          _this14.cell.dropoutMask = null;\n        }\n\n        if (_this14.cell.recurrentDropoutMask != null) {\n          tfc.dispose(_this14.cell.recurrentDropoutMask);\n          _this14.cell.recurrentDropoutMask = null;\n        }\n\n        var mask = kwargs == null ? null : kwargs['mask'];\n        var training = kwargs == null ? null : kwargs['training'];\n        var initialState = kwargs == null ? null : kwargs['initialState'];\n        return _get(_getPrototypeOf(LSTM.prototype), \"call\", _this14).call(_this14, inputs, {\n          mask: mask,\n          training: training,\n          initialState: initialState\n        });\n      });\n    }\n    /** @nocollapse */\n\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      if (config['implmentation'] === 0) {\n        config['implementation'] = 1;\n      }\n\n      return new cls(config);\n    }\n  }]);\n\n  return LSTM;\n}(RNN);\n/** @nocollapse */\n\nLSTM.className = 'LSTM';\nserialization.registerClass(LSTM);\nexport var StackedRNNCells = /*#__PURE__*/function (_RNNCell4) {\n  _inherits(StackedRNNCells, _RNNCell4);\n\n  function StackedRNNCells(args) {\n    var _this15;\n\n    _classCallCheck(this, StackedRNNCells);\n\n    _this15 = _possibleConstructorReturn(this, _getPrototypeOf(StackedRNNCells).call(this, args));\n    _this15.cells = args.cells;\n    return _this15;\n  }\n\n  _createClass(StackedRNNCells, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this16 = this;\n\n      return tidy(function () {\n        inputs = inputs;\n        var states = inputs.slice(1); // Recover per-cell states.\n\n        var nestedStates = [];\n\n        var _iterator3 = _createForOfIteratorHelper(_this16.cells.slice().reverse()),\n            _step3;\n\n        try {\n          for (_iterator3.s(); !(_step3 = _iterator3.n()).done;) {\n            var _cell = _step3.value;\n\n            if (Array.isArray(_cell.stateSize)) {\n              nestedStates.push(states.splice(0, _cell.stateSize.length));\n            } else {\n              nestedStates.push(states.splice(0, 1));\n            }\n          }\n        } catch (err) {\n          _iterator3.e(err);\n        } finally {\n          _iterator3.f();\n        }\n\n        nestedStates.reverse(); // Call the cells in order and store the returned states.\n\n        var newNestedStates = [];\n        var callInputs;\n\n        for (var i = 0; i < _this16.cells.length; ++i) {\n          var cell = _this16.cells[i];\n          states = nestedStates[i]; // TODO(cais): Take care of constants.\n\n          if (i === 0) {\n            callInputs = [inputs[0]].concat(states);\n          } else {\n            callInputs = [callInputs[0]].concat(states);\n          }\n\n          callInputs = cell.call(callInputs, kwargs);\n          newNestedStates.push(callInputs.slice(1));\n        } // Format the new states as a flat list in reverse cell order.\n\n\n        states = [];\n\n        var _iterator4 = _createForOfIteratorHelper(newNestedStates.slice().reverse()),\n            _step4;\n\n        try {\n          for (_iterator4.s(); !(_step4 = _iterator4.n()).done;) {\n            var _states;\n\n            var cellStates = _step4.value;\n\n            (_states = states).push.apply(_states, _toConsumableArray(cellStates));\n          }\n        } catch (err) {\n          _iterator4.e(err);\n        } finally {\n          _iterator4.f();\n        }\n\n        return [callInputs[0]].concat(states);\n      });\n    }\n  }, {\n    key: \"build\",\n    value: function build(inputShape) {\n      if (isArrayOfShapes(inputShape)) {\n        // TODO(cais): Take care of input constants.\n        // const constantShape = inputShape.slice(1);\n        inputShape = inputShape[0];\n      }\n\n      inputShape = inputShape;\n      var outputDim;\n      this.cells.forEach(function (cell, i) {\n        nameScope(\"RNNCell_\".concat(i), function () {\n          // TODO(cais): Take care of input constants.\n          cell.build(inputShape);\n\n          if (Array.isArray(cell.stateSize)) {\n            outputDim = cell.stateSize[0];\n          } else {\n            outputDim = cell.stateSize;\n          }\n\n          inputShape = [inputShape[0], outputDim];\n        });\n      });\n      this.built = true;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(StackedRNNCells.prototype), \"getConfig\", this).call(this);\n\n      var getCellConfig = function getCellConfig(cell) {\n        return {\n          'className': cell.getClassName(),\n          'config': cell.getConfig()\n        };\n      };\n\n      var cellConfigs = this.cells.map(getCellConfig);\n      var config = {\n        'cells': cellConfigs\n      };\n      return Object.assign({}, baseConfig, config);\n    }\n    /** @nocollapse */\n\n  }, {\n    key: \"getWeights\",\n\n    /**\n     * Retrieve the weights of a the model.\n     *\n     * @returns A flat `Array` of `tf.Tensor`s.\n     */\n    value: function getWeights() {\n      var weights = [];\n\n      var _iterator5 = _createForOfIteratorHelper(this.cells),\n          _step5;\n\n      try {\n        for (_iterator5.s(); !(_step5 = _iterator5.n()).done;) {\n          var cell = _step5.value;\n          weights.push.apply(weights, _toConsumableArray(cell.weights));\n        }\n      } catch (err) {\n        _iterator5.e(err);\n      } finally {\n        _iterator5.f();\n      }\n\n      return batchGetValue(weights);\n    }\n    /**\n     * Set the weights of the model.\n     *\n     * @param weights An `Array` of `tf.Tensor`s with shapes and types matching\n     *     the output of `getWeights()`.\n     */\n\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weights) {\n      var tuples = [];\n\n      var _iterator6 = _createForOfIteratorHelper(this.cells),\n          _step6;\n\n      try {\n        for (_iterator6.s(); !(_step6 = _iterator6.n()).done;) {\n          var cell = _step6.value;\n          var numParams = cell.weights.length;\n          var inputWeights = weights.splice(numParams);\n\n          for (var i = 0; i < cell.weights.length; ++i) {\n            tuples.push([cell.weights[i], inputWeights[i]]);\n          }\n        }\n      } catch (err) {\n        _iterator6.e(err);\n      } finally {\n        _iterator6.f();\n      }\n\n      batchSetValue(tuples);\n    }\n  }, {\n    key: \"stateSize\",\n    get: function get() {\n      // States are a flat list in reverse order of the cell stack.\n      // This allows perserving the requirement `stack.statesize[0] ===\n      // outputDim`. E.g., states of a 2-layer LSTM would be `[h2, c2, h1, c1]`,\n      // assuming one LSTM has states `[h, c]`.\n      var stateSize = [];\n\n      var _iterator7 = _createForOfIteratorHelper(this.cells.slice().reverse()),\n          _step7;\n\n      try {\n        for (_iterator7.s(); !(_step7 = _iterator7.n()).done;) {\n          var cell = _step7.value;\n\n          if (Array.isArray(cell.stateSize)) {\n            stateSize.push.apply(stateSize, _toConsumableArray(cell.stateSize));\n          } else {\n            stateSize.push(cell.stateSize);\n          }\n        }\n      } catch (err) {\n        _iterator7.e(err);\n      } finally {\n        _iterator7.f();\n      }\n\n      return stateSize;\n    }\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      if (!this.trainable) {\n        return [];\n      }\n\n      var weights = [];\n\n      var _iterator8 = _createForOfIteratorHelper(this.cells),\n          _step8;\n\n      try {\n        for (_iterator8.s(); !(_step8 = _iterator8.n()).done;) {\n          var cell = _step8.value;\n          weights.push.apply(weights, _toConsumableArray(cell.trainableWeights));\n        }\n      } catch (err) {\n        _iterator8.e(err);\n      } finally {\n        _iterator8.f();\n      }\n\n      return weights;\n    }\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      var weights = [];\n\n      var _iterator9 = _createForOfIteratorHelper(this.cells),\n          _step9;\n\n      try {\n        for (_iterator9.s(); !(_step9 = _iterator9.n()).done;) {\n          var _cell2 = _step9.value;\n          weights.push.apply(weights, _toConsumableArray(_cell2.nonTrainableWeights));\n        }\n      } catch (err) {\n        _iterator9.e(err);\n      } finally {\n        _iterator9.f();\n      }\n\n      if (!this.trainable) {\n        var trainableWeights = [];\n\n        var _iterator10 = _createForOfIteratorHelper(this.cells),\n            _step10;\n\n        try {\n          for (_iterator10.s(); !(_step10 = _iterator10.n()).done;) {\n            var cell = _step10.value;\n            trainableWeights.push.apply(trainableWeights, _toConsumableArray(cell.trainableWeights));\n          }\n        } catch (err) {\n          _iterator10.e(err);\n        } finally {\n          _iterator10.f();\n        }\n\n        return trainableWeights.concat(weights);\n      }\n\n      return weights;\n    }\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      var customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n      var cells = [];\n\n      var _iterator11 = _createForOfIteratorHelper(config['cells']),\n          _step11;\n\n      try {\n        for (_iterator11.s(); !(_step11 = _iterator11.n()).done;) {\n          var cellConfig = _step11.value;\n          cells.push(deserialize(cellConfig, customObjects));\n        }\n      } catch (err) {\n        _iterator11.e(err);\n      } finally {\n        _iterator11.f();\n      }\n\n      return new cls({\n        cells: cells\n      });\n    }\n  }]);\n\n  return StackedRNNCells;\n}(RNNCell);\n/** @nocollapse */\n\nStackedRNNCells.className = 'StackedRNNCells';\nserialization.registerClass(StackedRNNCells);\nexport function generateDropoutMask(args) {\n  var ones = args.ones,\n      rate = args.rate,\n      _args$training = args.training,\n      training = _args$training === void 0 ? false : _args$training,\n      _args$count = args.count,\n      count = _args$count === void 0 ? 1 : _args$count;\n\n  var droppedInputs = function droppedInputs() {\n    return K.dropout(ones(), rate);\n  };\n\n  var createMask = function createMask() {\n    return K.inTrainPhase(droppedInputs, ones, training);\n  }; // just in case count is provided with null or undefined\n\n\n  if (!count || count <= 1) {\n    return tfc.keep(createMask().clone());\n  }\n\n  var masks = Array(count).fill(undefined).map(createMask);\n  return masks.map(function (m) {\n    return tfc.keep(m.clone());\n  });\n}","map":null,"metadata":{},"sourceType":"module"}