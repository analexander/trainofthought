{"ast":null,"code":"import _classCallCheck from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass\";\nimport _possibleConstructorReturn from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";\nimport _get from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/get\";\nimport _getPrototypeOf from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";\nimport _inherits from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/inherits\";\n\nfunction _createForOfIteratorHelper(o, allowArrayLike) { var it; if (typeof Symbol === \"undefined\" || o[Symbol.iterator] == null) { if (Array.isArray(o) || (it = _unsupportedIterableToArray(o)) || allowArrayLike && o && typeof o.length === \"number\") { if (it) o = it; var i = 0; var F = function F() {}; return { s: F, n: function n() { if (i >= o.length) return { done: true }; return { done: false, value: o[i++] }; }, e: function e(_e) { throw _e; }, f: F }; } throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\"); } var normalCompletion = true, didErr = false, err; return { s: function s() { it = o[Symbol.iterator](); }, n: function n() { var step = it.next(); normalCompletion = step.done; return step; }, e: function e(_e2) { didErr = true; err = _e2; }, f: function f() { try { if (!normalCompletion && it.return != null) it.return(); } finally { if (didErr) throw err; } } }; }\n\nfunction _unsupportedIterableToArray(o, minLen) { if (!o) return; if (typeof o === \"string\") return _arrayLikeToArray(o, minLen); var n = Object.prototype.toString.call(o).slice(8, -1); if (n === \"Object\" && o.constructor) n = o.constructor.name; if (n === \"Map\" || n === \"Set\") return Array.from(o); if (n === \"Arguments\" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return _arrayLikeToArray(o, minLen); }\n\nfunction _arrayLikeToArray(arr, len) { if (len == null || len > arr.length) len = arr.length; for (var i = 0, arr2 = new Array(len); i < len; i++) { arr2[i] = arr[i]; } return arr2; }\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n *  Advanced activation layers.\n */\nimport { clipByValue, elu, leakyRelu, prelu, relu, serialization } from '@tensorflow/tfjs-core';\nimport { Softmax as softmaxActivation } from '../activations';\nimport { cast } from '../backend/tfjs_backend';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nexport var ReLU = /*#__PURE__*/function (_Layer) {\n  _inherits(ReLU, _Layer);\n\n  function ReLU(args) {\n    var _this;\n\n    _classCallCheck(this, ReLU);\n\n    _this = _possibleConstructorReturn(this, _getPrototypeOf(ReLU).call(this, args == null ? {} : args));\n    _this.supportsMasking = true;\n\n    if (args != null) {\n      _this.maxValue = args.maxValue;\n    }\n\n    return _this;\n  }\n\n  _createClass(ReLU, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      inputs = getExactlyOneTensor(inputs);\n      var output = relu(inputs);\n\n      if (this.maxValue != null) {\n        output = clipByValue(output, 0, this.maxValue);\n      }\n\n      return output;\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        maxValue: this.maxValue\n      };\n\n      var baseConfig = _get(_getPrototypeOf(ReLU.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n\n  return ReLU;\n}(Layer);\n/** @nocollapse */\n\nReLU.className = 'ReLU';\nserialization.registerClass(ReLU);\nexport var LeakyReLU = /*#__PURE__*/function (_Layer2) {\n  _inherits(LeakyReLU, _Layer2);\n\n  function LeakyReLU(args) {\n    var _this2;\n\n    _classCallCheck(this, LeakyReLU);\n\n    _this2 = _possibleConstructorReturn(this, _getPrototypeOf(LeakyReLU).call(this, args == null ? {} : args));\n    _this2.DEFAULT_ALPHA = 0.3;\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this2.alpha = args.alpha == null ? _this2.DEFAULT_ALPHA : args.alpha;\n    return _this2;\n  }\n\n  _createClass(LeakyReLU, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var x = getExactlyOneTensor(inputs);\n      return leakyRelu(x, this.alpha);\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        alpha: this.alpha\n      };\n\n      var baseConfig = _get(_getPrototypeOf(LeakyReLU.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n\n  return LeakyReLU;\n}(Layer);\n/** @nocollapse */\n\nLeakyReLU.className = 'LeakyReLU';\nserialization.registerClass(LeakyReLU);\nexport var PReLU = /*#__PURE__*/function (_Layer3) {\n  _inherits(PReLU, _Layer3);\n\n  function PReLU(args) {\n    var _this3;\n\n    _classCallCheck(this, PReLU);\n\n    _this3 = _possibleConstructorReturn(this, _getPrototypeOf(PReLU).call(this, args == null ? {} : args));\n    _this3.DEFAULT_ALPHA_INITIALIZER = 'zeros';\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this3.supportsMasking = true;\n    _this3.alphaInitializer = getInitializer(args.alphaInitializer || _this3.DEFAULT_ALPHA_INITIALIZER);\n    _this3.alphaRegularizer = getRegularizer(args.alphaRegularizer);\n    _this3.alphaConstraint = getConstraint(args.alphaConstraint);\n\n    if (args.sharedAxes == null) {\n      _this3.sharedAxes = null;\n    } else if (Array.isArray(args.sharedAxes)) {\n      _this3.sharedAxes = args.sharedAxes;\n    } else if (typeof args.sharedAxes === 'number') {\n      _this3.sharedAxes = [args.sharedAxes];\n    } else {\n      throw new ValueError(\"Expected sharedAxes to be a number or an array of numbers, \" + \"but got \".concat(args.sharedAxes));\n    }\n\n    return _this3;\n  }\n\n  _createClass(PReLU, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var paramShape = inputShape.slice(1);\n\n      if (this.sharedAxes != null) {\n        var _iterator = _createForOfIteratorHelper(this.sharedAxes),\n            _step;\n\n        try {\n          for (_iterator.s(); !(_step = _iterator.n()).done;) {\n            var i = _step.value;\n            paramShape[i - 1] = 1;\n          }\n        } catch (err) {\n          _iterator.e(err);\n        } finally {\n          _iterator.f();\n        }\n      }\n\n      this.alpha = this.addWeight('alpha', paramShape, 'float32', this.alphaInitializer, this.alphaRegularizer, true, this.alphaConstraint); // Set input spec.\n\n      var axes = {};\n\n      if (this.sharedAxes != null) {\n        for (var _i = 1; _i < inputShape.length; ++_i) {\n          axes[_i] = inputShape[_i];\n        }\n      }\n\n      this.inputSpec = [new InputSpec({\n        ndim: inputShape.length,\n        axes: axes\n      })];\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      inputs = getExactlyOneTensor(inputs);\n      return prelu(inputs, this.alpha.read());\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        alphaInitializer: serializeInitializer(this.alphaInitializer),\n        alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n        alphaConstraint: serializeConstraint(this.alphaConstraint),\n        sharedAxes: this.sharedAxes\n      };\n\n      var baseConfig = _get(_getPrototypeOf(PReLU.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n\n  return PReLU;\n}(Layer);\n/** @nocollapse */\n\nPReLU.className = 'PReLU';\nserialization.registerClass(PReLU);\nexport var ELU = /*#__PURE__*/function (_Layer4) {\n  _inherits(ELU, _Layer4);\n\n  function ELU(args) {\n    var _this4;\n\n    _classCallCheck(this, ELU);\n\n    _this4 = _possibleConstructorReturn(this, _getPrototypeOf(ELU).call(this, args == null ? {} : args));\n    _this4.DEFAULT_ALPHA = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    if (args.alpha != null && args.alpha !== _this4.DEFAULT_ALPHA) {\n      throw new NotImplementedError(\"Non-default alpha value (\".concat(args.alpha, \") is not supported by the \") + \"ELU layer yet.\");\n    }\n\n    _this4.alpha = args.alpha == null ? _this4.DEFAULT_ALPHA : args.alpha;\n    return _this4;\n  }\n\n  _createClass(ELU, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var x = getExactlyOneTensor(inputs);\n      return elu(x);\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        alpha: this.alpha\n      };\n\n      var baseConfig = _get(_getPrototypeOf(ELU.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n\n  return ELU;\n}(Layer);\n/** @nocollapse */\n\nELU.className = 'ELU';\nserialization.registerClass(ELU);\nexport var ThresholdedReLU = /*#__PURE__*/function (_Layer5) {\n  _inherits(ThresholdedReLU, _Layer5);\n\n  function ThresholdedReLU(args) {\n    var _this5;\n\n    _classCallCheck(this, ThresholdedReLU);\n\n    _this5 = _possibleConstructorReturn(this, _getPrototypeOf(ThresholdedReLU).call(this, args == null ? {} : args));\n    _this5.DEFAULT_THETA = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this5.theta = args.theta == null ? _this5.DEFAULT_THETA : args.theta;\n    return _this5;\n  }\n\n  _createClass(ThresholdedReLU, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var x = getExactlyOneTensor(inputs);\n      return x.mul(cast(x.greater(this.theta), 'float32'));\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        theta: this.theta\n      };\n\n      var baseConfig = _get(_getPrototypeOf(ThresholdedReLU.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n\n  return ThresholdedReLU;\n}(Layer);\n/** @nocollapse */\n\nThresholdedReLU.className = 'ThresholdedReLU';\nserialization.registerClass(ThresholdedReLU);\nexport var Softmax = /*#__PURE__*/function (_Layer6) {\n  _inherits(Softmax, _Layer6);\n\n  function Softmax(args) {\n    var _this6;\n\n    _classCallCheck(this, Softmax);\n\n    _this6 = _possibleConstructorReturn(this, _getPrototypeOf(Softmax).call(this, args == null ? {} : args));\n    _this6.DEFAULT_AXIS = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this6.softmax = new softmaxActivation().apply;\n    _this6.axis = args.axis == null ? _this6.DEFAULT_AXIS : args.axis;\n    return _this6;\n  }\n\n  _createClass(Softmax, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var x = getExactlyOneTensor(inputs);\n      return this.softmax(x, this.axis);\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        axis: this.axis\n      };\n\n      var baseConfig = _get(_getPrototypeOf(Softmax.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n\n  return Softmax;\n}(Layer);\n/** @nocollapse */\n\nSoftmax.className = 'Softmax';\nserialization.registerClass(Softmax);","map":null,"metadata":{},"sourceType":"module"}