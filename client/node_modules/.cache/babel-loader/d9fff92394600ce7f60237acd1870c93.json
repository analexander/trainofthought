{"ast":null,"code":"/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as util from './util';\n/**\n * Computes a list of TapeNodes that connect x to y, filtering everything else\n * out and preserving the order of the original tape elements.\n *\n * @param tape The tape elements to filter.\n * @param xs The input Tensors.\n * @param y The output Tensor.\n */\n\nexport function getFilteredNodesXToY(tape, xs, y) {\n  // Forward pass to compute all the nodes and Tensors that are transitively a\n  // function of x.\n  var tensorsFromX = {};\n  var nodesFromX = {};\n\n  for (var i = 0; i < xs.length; i++) {\n    tensorsFromX[xs[i].id] = true;\n  }\n\n  for (var _i = 0; _i < tape.length; _i++) {\n    var node = tape[_i];\n    var nodeInputs = node.inputs;\n\n    for (var inputName in nodeInputs) {\n      var input = nodeInputs[inputName];\n      var anyInputFromX = false;\n\n      for (var j = 0; j < xs.length; j++) {\n        if (tensorsFromX[input.id]) {\n          node.outputs.forEach(function (output) {\n            return tensorsFromX[output.id] = true;\n          });\n          anyInputFromX = true;\n          nodesFromX[node.id] = true;\n          break;\n        }\n      }\n\n      if (anyInputFromX) {\n        break;\n      }\n    }\n  } // Backward pass to find all of the nodes and Tensors that lead to y.\n\n\n  var tensorsLeadToY = {};\n  tensorsLeadToY[y.id] = true;\n  var nodesToY = {};\n\n  for (var _i2 = tape.length - 1; _i2 >= 0; _i2--) {\n    var _node = tape[_i2];\n    var _nodeInputs = _node.inputs; // If any of the outputs lead to y, mark all of the inputs as leading to y.\n\n    for (var _j = 0; _j < _node.outputs.length; _j++) {\n      if (tensorsLeadToY[_node.outputs[_j].id]) {\n        for (var _inputName in _nodeInputs) {\n          tensorsLeadToY[_nodeInputs[_inputName].id] = true;\n          nodesToY[_node.id] = true;\n        }\n\n        break;\n      }\n    }\n  } // Return the paths that come from x and lead to y.\n\n\n  var filteredTape = [];\n\n  for (var _i3 = 0; _i3 < tape.length; _i3++) {\n    var _node2 = tape[_i3];\n\n    if (nodesFromX[_node2.id] && nodesToY[_node2.id]) {\n      // Prune the inputs from the node that aren't a function of x.\n      var prunedInputs = {};\n\n      for (var _inputName2 in _node2.inputs) {\n        var nodeInput = _node2.inputs[_inputName2];\n\n        if (tensorsFromX[nodeInput.id]) {\n          prunedInputs[_inputName2] = nodeInput;\n        }\n      } // Copy the node and overwrite inputsAndArgs to the pruned version.\n\n\n      var prunedNode = Object.assign({}, _node2);\n      prunedNode.inputs = prunedInputs;\n      prunedNode.outputs = _node2.outputs;\n      filteredTape.push(prunedNode);\n    }\n  }\n\n  return filteredTape;\n}\n/**\n * Backpropagate gradients through the filtered TapeNodes.\n *\n * @param tensorAccumulatedGradientMap A map of Tensor to its gradient. This map\n * is mutated by this method.\n * @param filteredTape The filtered TapeNodes to backprop through.\n */\n\nexport function backpropagateGradients(tensorAccumulatedGradientMap, filteredTape, tidy, add) {\n  var _loop = function _loop(i) {\n    var node = filteredTape[i];\n    var dys = [];\n    node.outputs.forEach(function (o) {\n      var gradTensor = tensorAccumulatedGradientMap[o.id];\n\n      if (gradTensor != null) {\n        dys.push(gradTensor);\n      } else {\n        // This particular output is not in the back-propagation subgraph, so it\n        // does not affect the final output, thus we put null for its dy.\n        dys.push(null);\n      }\n    });\n\n    if (node.gradient == null) {\n      throw new Error(\"Cannot compute gradient: gradient function not found \" + \"for \".concat(node.kernelName, \".\"));\n    } // Backprop dy through this node and accumulate gradients over the inputs.\n\n\n    var inputGradients = node.gradient(dys);\n\n    var _loop2 = function _loop2(inputName) {\n      if (!(inputName in inputGradients)) {\n        throw new Error(\"Cannot backprop through input \".concat(inputName, \". \") + \"Available gradients found: \".concat(Object.keys(inputGradients), \".\"));\n      } // Call the gradient function.\n\n\n      var dx = tidy(function () {\n        return inputGradients[inputName]();\n      });\n\n      if (dx.dtype !== 'float32') {\n        throw new Error(\"Error in gradient for op \".concat(node.kernelName, \". The gradient of input \") + \"\".concat(inputName, \" must have 'float32' dtype, but has '\").concat(dx.dtype, \"'\"));\n      }\n\n      var x = node.inputs[inputName];\n\n      if (!util.arraysEqual(dx.shape, x.shape)) {\n        throw new Error(\"Error in gradient for op \".concat(node.kernelName, \". The gradient of input \") + \"'\".concat(inputName, \"' has shape '\").concat(dx.shape, \"', which does not match \") + \"the shape of the input '\".concat(x.shape, \"'\"));\n      }\n\n      if (tensorAccumulatedGradientMap[x.id] == null) {\n        tensorAccumulatedGradientMap[x.id] = dx;\n      } else {\n        var curGradient = tensorAccumulatedGradientMap[x.id];\n        tensorAccumulatedGradientMap[x.id] = add(curGradient, dx);\n        curGradient.dispose();\n      }\n    };\n\n    for (var inputName in node.inputs) {\n      _loop2(inputName);\n    }\n  };\n\n  // Walk the tape backward and keep a map of Tensor to its gradient.\n  for (var i = filteredTape.length - 1; i >= 0; i--) {\n    _loop(i);\n  }\n}","map":null,"metadata":{},"sourceType":"module"}