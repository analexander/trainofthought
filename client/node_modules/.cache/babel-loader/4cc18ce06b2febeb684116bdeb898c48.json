{"ast":null,"code":"import _toConsumableArray from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/toConsumableArray\";\nimport _classCallCheck from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass\";\nimport _possibleConstructorReturn from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";\nimport _get from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/get\";\nimport _getPrototypeOf from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";\nimport _inherits from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/inherits\";\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\n\nexport var Wrapper = /*#__PURE__*/function (_Layer) {\n  _inherits(Wrapper, _Layer);\n\n  function Wrapper(args) {\n    var _this;\n\n    _classCallCheck(this, Wrapper);\n\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    _this = _possibleConstructorReturn(this, _getPrototypeOf(Wrapper).call(this, args));\n    _this.layer = args.layer;\n    return _this;\n  }\n\n  _createClass(Wrapper, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      this.built = true;\n    } // TODO(cais): Implement activityRegularizer getter.\n\n  }, {\n    key: \"getWeights\",\n    // TODO(cais): Implement getLossesFor().\n    value: function getWeights() {\n      return this.layer.getWeights();\n    }\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weights) {\n      this.layer.setWeights(weights);\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        'layer': {\n          'className': this.layer.getClassName(),\n          'config': this.layer.getConfig()\n        }\n      };\n\n      var baseConfig = _get(_getPrototypeOf(Wrapper.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }, {\n    key: \"setFastWeightInitDuringBuild\",\n    value: function setFastWeightInitDuringBuild(value) {\n      _get(_getPrototypeOf(Wrapper.prototype), \"setFastWeightInitDuringBuild\", this).call(this, value);\n\n      if (this.layer != null) {\n        this.layer.setFastWeightInitDuringBuild(value);\n      }\n    }\n    /** @nocollapse */\n\n  }, {\n    key: \"trainable\",\n    get: function get() {\n      // Porting Note: the check of `this.layer` here is necessary due to the\n      //   way the `constructor` of this class is written (see Porting Note\n      //   above).\n      if (this.layer != null) {\n        return this.layer.trainable;\n      } else {\n        return false;\n      }\n    },\n    set: function set(value) {\n      // Porting Note: the check of `this.layer` here is necessary due to the\n      //   way the `constructor` of this class is written (see Porting Note\n      //   above).\n      if (this.layer != null) {\n        this.layer.trainable = value;\n      }\n    }\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      return this.layer.trainableWeights;\n    } // TODO(cais): Implement setter for trainableWeights.\n\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      return this.layer.nonTrainableWeights;\n    } // TODO(cais): Implement setter for nonTrainableWeights.\n\n  }, {\n    key: \"updates\",\n    get: function get() {\n      // tslint:disable-next-line:no-any\n      return this.layer._updates;\n    } // TODO(cais): Implement getUpdatesFor().\n\n  }, {\n    key: \"losses\",\n    get: function get() {\n      return this.layer.losses;\n    }\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      var customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n      var layerConfig = config['layer'];\n      var layer = deserialize(layerConfig, customObjects);\n      delete config['layer'];\n      var newConfig = {\n        layer: layer\n      };\n      Object.assign(newConfig, config);\n      return new cls(newConfig);\n    }\n  }]);\n\n  return Wrapper;\n}(Layer);\nexport var TimeDistributed = /*#__PURE__*/function (_Wrapper) {\n  _inherits(TimeDistributed, _Wrapper);\n\n  function TimeDistributed(args) {\n    var _this2;\n\n    _classCallCheck(this, TimeDistributed);\n\n    _this2 = _possibleConstructorReturn(this, _getPrototypeOf(TimeDistributed).call(this, args));\n    _this2.supportsMasking = true;\n    return _this2;\n  }\n\n  _createClass(TimeDistributed, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n\n      if (inputShape.length < 3) {\n        throw new ValueError(\"TimeDistributed layer expects an input shape >= 3D, but received \" + \"input shape \".concat(JSON.stringify(inputShape)));\n      }\n\n      this.inputSpec = [{\n        shape: inputShape\n      }];\n      var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n\n      if (!this.layer.built) {\n        this.layer.build(childInputShape);\n        this.layer.built = true;\n      }\n\n      _get(_getPrototypeOf(TimeDistributed.prototype), \"build\", this).call(this, inputShape);\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n      var childOutputShape = this.layer.computeOutputShape(childInputShape);\n      var timesteps = inputShape[1];\n      return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this3 = this;\n\n      return tidy(function () {\n        // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n        inputs = getExactlyOneTensor(inputs); // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n        // values. Hence the inputs can't have an undetermined first (batch)\n        // dimension, which is why we always use the K.rnn approach here.\n\n        var step = function step(inputs, states) {\n          // TODO(cais): Add useLearningPhase.\n          // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n          //   some cases (e.g., `layer` is a `Sequential` instance), which is\n          //   why `getExactlyOneTensor` is used below.\n          var output = getExactlyOneTensor(_this3.layer.call(inputs, kwargs));\n          return [output, []];\n        };\n\n        var rnnOutputs = rnn(step, inputs, [], false\n        /* goBackwards */\n        , null\n        /* mask */\n        , null\n        /* constants */\n        , false\n        /* unroll */\n        , true\n        /* needPerStepOutputs */\n        );\n        var y = rnnOutputs[1]; // TODO(cais): Add activity regularization.\n        // TODO(cais): Add useLearningPhase.\n\n        return y;\n      });\n    }\n  }]);\n\n  return TimeDistributed;\n}(Wrapper);\n/** @nocollapse */\n\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n  generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nvar DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport var Bidirectional = /*#__PURE__*/function (_Wrapper2) {\n  _inherits(Bidirectional, _Wrapper2);\n\n  function Bidirectional(args) {\n    var _this4;\n\n    _classCallCheck(this, Bidirectional);\n\n    _this4 = _possibleConstructorReturn(this, _getPrototypeOf(Bidirectional).call(this, args)); // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n\n    var layerConfig = args.layer.getConfig();\n    var forwDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    _this4.forwardLayer = deserialize(forwDict);\n    layerConfig['goBackwards'] = layerConfig['goBackwards'] === true ? false : true;\n    var backDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    _this4.backwardLayer = deserialize(backDict);\n    _this4.forwardLayer.name = 'forward_' + _this4.forwardLayer.name;\n    _this4.backwardLayer.name = 'backward_' + _this4.backwardLayer.name;\n    _this4.mergeMode = args.mergeMode === undefined ? DEFAULT_BIDIRECTIONAL_MERGE_MODE : args.mergeMode;\n    checkBidirectionalMergeMode(_this4.mergeMode);\n\n    if (args.weights) {\n      throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n    }\n\n    _this4._stateful = args.layer.stateful;\n    _this4.returnSequences = args.layer.returnSequences;\n    _this4.returnState = args.layer.returnState;\n    _this4.supportsMasking = true;\n    _this4._trainable = true;\n    _this4.inputSpec = args.layer.inputSpec;\n    _this4.numConstants = null;\n    return _this4;\n  }\n\n  _createClass(Bidirectional, [{\n    key: \"getWeights\",\n    value: function getWeights() {\n      return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n    }\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weights) {\n      var numWeights = weights.length;\n      var numeightsOver2 = Math.floor(numWeights / 2);\n      this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n      this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      var layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n\n      if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n        layerShapes = [layerShapes];\n      }\n\n      layerShapes = layerShapes;\n      var outputShape;\n      var outputShapes;\n      var stateShape;\n\n      if (this.returnState) {\n        stateShape = layerShapes.slice(1);\n        outputShape = layerShapes[0];\n      } else {\n        outputShape = layerShapes[0];\n      }\n\n      outputShape = outputShape;\n\n      if (this.mergeMode === 'concat') {\n        outputShape[outputShape.length - 1] *= 2;\n        outputShapes = [outputShape];\n      } else if (this.mergeMode == null) {\n        outputShapes = [outputShape, outputShape.slice()];\n      } else {\n        outputShapes = [outputShape];\n      }\n\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return outputShapes.concat(stateShape).concat(stateShape.slice());\n        }\n\n        return [outputShape].concat(stateShape).concat(stateShape.slice());\n      }\n\n      return generic_utils.singletonOrArray(outputShapes);\n    }\n  }, {\n    key: \"apply\",\n    value: function apply(inputs, kwargs) {\n      var initialState = kwargs == null ? null : kwargs['initialState'];\n      var constants = kwargs == null ? null : kwargs['constants'];\n\n      if (kwargs == null) {\n        kwargs = {};\n      }\n\n      var standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n      inputs = standardized.inputs;\n      initialState = standardized.initialState;\n      constants = standardized.constants;\n\n      if (Array.isArray(inputs)) {\n        initialState = inputs.slice(1);\n        inputs = inputs[0];\n      }\n\n      if ((initialState == null || initialState.length === 0) && constants == null) {\n        return _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, inputs, kwargs);\n      }\n\n      var additionalInputs = [];\n      var additionalSpecs = [];\n\n      if (initialState != null) {\n        var numStates = initialState.length;\n\n        if (numStates % 2 > 0) {\n          throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' + 'the state should be an Array containing the states of ' + 'the underlying RNNs.');\n        }\n\n        kwargs['initialState'] = initialState;\n        additionalInputs.push.apply(additionalInputs, _toConsumableArray(initialState));\n        var stateSpecs = initialState.map(function (state) {\n          return new InputSpec({\n            shape: state.shape\n          });\n        });\n        this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n        this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n        additionalSpecs.push.apply(additionalSpecs, _toConsumableArray(stateSpecs));\n      }\n\n      if (constants != null) {\n        throw new NotImplementedError('Support for constants in Bidirectional layers is not ' + 'implemented yet.');\n      }\n\n      var isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n\n      for (var _i = 0, _additionalInputs = additionalInputs; _i < _additionalInputs.length; _i++) {\n        var tensor = _additionalInputs[_i];\n\n        if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n          throw new ValueError('The initial state of a Bidirectional layer cannot be ' + 'specified as a mix of symbolic and non-symbolic tensors');\n        }\n      }\n\n      if (isSymbolicTensor) {\n        // Compute the full input and specs, including the states.\n        var fullInput = [inputs].concat(additionalInputs);\n        var fullInputSpec = this.inputSpec.concat(additionalSpecs); // Perform the call temporarily and replace inputSpec.\n        // Note: with initial states symbolic calls and non-symbolic calls to\n        // this method differ in how the initial states are passed. For\n        // symbolic calls, the initial states are passed in the first arg, as\n        // an Array of SymbolicTensors; for non-symbolic calls, they are\n        // passed in the second arg as a part of the kwargs. Hence the need to\n        // temporarily modify inputSpec here.\n        // TODO(cais): Make refactoring so that this hacky code below is no\n        // longer needed.\n\n        var originalInputSpec = this.inputSpec;\n        this.inputSpec = fullInputSpec;\n\n        var output = _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, fullInput, kwargs);\n\n        this.inputSpec = originalInputSpec;\n        return output;\n      } else {\n        return _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, inputs, kwargs);\n      }\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this5 = this;\n\n      return tidy(function () {\n        var initialState = kwargs['initialState'];\n        var y;\n        var yRev;\n\n        if (initialState == null) {\n          y = _this5.forwardLayer.call(inputs, kwargs);\n          yRev = _this5.backwardLayer.call(inputs, kwargs);\n        } else {\n          var forwardState = initialState.slice(0, initialState.length / 2);\n          var backwardState = initialState.slice(initialState.length / 2);\n          y = _this5.forwardLayer.call(inputs, Object.assign(kwargs, {\n            initialState: forwardState\n          }));\n          yRev = _this5.backwardLayer.call(inputs, Object.assign(kwargs, {\n            initialState: backwardState\n          }));\n        }\n\n        var states;\n\n        if (_this5.returnState) {\n          if (Array.isArray(y)) {\n            states = y.slice(1).concat(yRev.slice(1));\n          } else {}\n\n          y = y[0];\n          yRev = yRev[0];\n        }\n\n        if (_this5.returnSequences) {\n          yRev = tfc.reverse(yRev, 1);\n        }\n\n        var output;\n\n        if (_this5.mergeMode === 'concat') {\n          output = K.concatenate([y, yRev]);\n        } else if (_this5.mergeMode === 'sum') {\n          output = tfc.add(y, yRev);\n        } else if (_this5.mergeMode === 'ave') {\n          output = tfc.mul(.5, tfc.add(y, yRev));\n        } else if (_this5.mergeMode === 'mul') {\n          output = tfc.mul(y, yRev);\n        } else if (_this5.mergeMode == null) {\n          output = [y, yRev];\n        } // TODO(cais): Properly set learning phase.\n\n\n        if (_this5.returnState) {\n          if (_this5.mergeMode == null) {\n            return output.concat(states);\n          }\n\n          return [output].concat(states);\n        }\n\n        return output;\n      });\n    }\n  }, {\n    key: \"resetStates\",\n    value: function resetStates(states) {\n      this.forwardLayer.resetStates();\n      this.backwardLayer.resetStates();\n    }\n  }, {\n    key: \"build\",\n    value: function build(inputShape) {\n      var _this6 = this;\n\n      nameScope(this.forwardLayer.name, function () {\n        _this6.forwardLayer.build(inputShape);\n      });\n      nameScope(this.backwardLayer.name, function () {\n        _this6.backwardLayer.build(inputShape);\n      });\n      this.built = true;\n    }\n  }, {\n    key: \"computeMask\",\n    value: function computeMask(inputs, mask) {\n      if (Array.isArray(mask)) {\n        mask = mask[0];\n      }\n\n      var outputMask;\n\n      if (this.returnSequences) {\n        if (this.mergeMode == null) {\n          outputMask = [mask, mask];\n        } else {\n          outputMask = mask;\n        }\n      } else {\n        if (this.mergeMode == null) {\n          outputMask = [null, null];\n        } else {\n          outputMask = null;\n        }\n      }\n\n      if (this.returnState) {\n        var states = this.forwardLayer.states;\n        var stateMask = states.map(function (state) {\n          return null;\n        });\n\n        if (Array.isArray(outputMask)) {\n          return outputMask.concat(stateMask).concat(stateMask);\n        } else {\n          return [outputMask].concat(stateMask).concat(stateMask);\n        }\n      } else {\n        return outputMask;\n      }\n    }\n  }, {\n    key: \"setFastWeightInitDuringBuild\",\n    // TODO(cais): Implement constraints().\n    value: function setFastWeightInitDuringBuild(value) {\n      _get(_getPrototypeOf(Bidirectional.prototype), \"setFastWeightInitDuringBuild\", this).call(this, value);\n\n      if (this.forwardLayer != null) {\n        this.forwardLayer.setFastWeightInitDuringBuild(value);\n      }\n\n      if (this.backwardLayer != null) {\n        this.backwardLayer.setFastWeightInitDuringBuild(value);\n      }\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        'mergeMode': this.mergeMode\n      }; // TODO(cais): Add logic for `numConstants` once the property is added.\n\n      var baseConfig = _get(_getPrototypeOf(Bidirectional.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n    /** @nocollapse */\n\n  }, {\n    key: \"trainable\",\n    get: function get() {\n      return this._trainable;\n    },\n    set: function set(value) {\n      // Porting Note: the check of `this.layer` here is necessary due to the\n      //   way the `constructor` of this class is written (see Porting Note\n      //   above).\n      this._trainable = value;\n\n      if (this.forwardLayer != null) {\n        this.forwardLayer.trainable = value;\n      }\n\n      if (this.backwardLayer != null) {\n        this.backwardLayer.trainable = value;\n      }\n    }\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n    }\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n    }\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      var rnnLayer = deserialize(config['layer']);\n      delete config['layer']; // TODO(cais): Add logic for `numConstants` once the property is added.\n\n      if (config['numConstants'] != null) {\n        throw new NotImplementedError(\"Deserialization of a Bidirectional layer with numConstants \" + \"present is not supported yet.\");\n      } // tslint:disable-next-line:no-any\n\n\n      var newConfig = config;\n      newConfig['layer'] = rnnLayer;\n      return new cls(newConfig);\n    }\n  }]);\n\n  return Bidirectional;\n}(Wrapper);\n/** @nocollapse */\n\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);","map":null,"metadata":{},"sourceType":"module"}