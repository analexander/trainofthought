{"ast":null,"code":"import _slicedToArray from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/slicedToArray\";\n\n/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { _FusedMatMul } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { matMul as unfusedMatMul } from '../mat_mul';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n */\n\nfunction fusedMatMul_(_ref) {\n  var a = _ref.a,\n      b = _ref.b,\n      _ref$transposeA = _ref.transposeA,\n      transposeA = _ref$transposeA === void 0 ? false : _ref$transposeA,\n      _ref$transposeB = _ref.transposeB,\n      transposeB = _ref$transposeB === void 0 ? false : _ref$transposeB,\n      bias = _ref.bias,\n      _ref$activation = _ref.activation,\n      activation = _ref$activation === void 0 ? 'linear' : _ref$activation,\n      preluActivationWeights = _ref.preluActivationWeights;\n\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    var result = unfusedMatMul(a, b, transposeA, transposeB);\n\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(result, activation, preluActivationWeights);\n  }\n\n  var $a = convertToTensor(a, 'a', 'fused matMul');\n  var $b = convertToTensor(b, 'b', 'fused matMul');\n\n  var _makeTypesMatch = makeTypesMatch($a, $b);\n\n  var _makeTypesMatch2 = _slicedToArray(_makeTypesMatch, 2);\n\n  $a = _makeTypesMatch2[0];\n  $b = _makeTypesMatch2[1];\n  var innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n  var innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n  var outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n  var outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n  var outerDimsA = $a.shape.slice(0, -2);\n  var outerDimsB = $b.shape.slice(0, -2);\n  var batchDimA = util.sizeFromShape(outerDimsA);\n  var batchDimB = util.sizeFromShape(outerDimsB);\n  util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, function () {\n    return \"Error in fused matMul: inputs must have the same rank of at least \" + \"2, got ranks \".concat($a.rank, \" and \").concat($b.rank, \".\");\n  });\n  util.assert(util.arraysEqual(outerDimsA, outerDimsB), function () {\n    return \"Error in fused matMul: outer dimensions (\".concat(outerDimsA, \") and (\") + \"\".concat(outerDimsB, \") of Tensors with shapes \").concat($a.shape, \" and \") + \"\".concat($b.shape, \" must match.\");\n  });\n  util.assert(innerShapeA === innerShapeB, function () {\n    return \"Error in fused matMul: inner shapes (\".concat(innerShapeA, \") and (\") + \"\".concat(innerShapeB, \") of Tensors with shapes \").concat($a.shape, \" and \") + \"\".concat($b.shape, \" and transposeA=\").concat(transposeA) + \" and transposeB=\".concat(transposeB, \" must match.\");\n  });\n  var outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\n  var a3D = transposeA ? reshape($a, [batchDimA, innerShapeA, outerShapeA]) : reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n  var b3D = transposeB ? reshape($b, [batchDimB, outerShapeB, innerShapeB]) : reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n  var $bias;\n\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused matMul');\n\n    var _makeTypesMatch3 = makeTypesMatch($bias, $a);\n\n    var _makeTypesMatch4 = _slicedToArray(_makeTypesMatch3, 1);\n\n    $bias = _makeTypesMatch4[0];\n    broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n  }\n\n  var $preluActivationWeights;\n\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n  }\n\n  var grad = function grad(dy, saved) {\n    var _saved = _slicedToArray(saved, 4),\n        a3D = _saved[0],\n        b3D = _saved[1],\n        y = _saved[2],\n        $bias = _saved[3]; // we reshape dy because the result of the forward is not\n    // necessarily going to be a 3d tensor due to a reshape done at the end of\n    // the customOp.\n\n\n    var dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);\n    var aDer;\n    var bDer;\n\n    if (!transposeA && !transposeB) {\n      aDer = unfusedMatMul(dyActivation, b3D, false, true);\n      bDer = unfusedMatMul(a3D, dyActivation, true, false);\n    } else if (!transposeA && transposeB) {\n      aDer = unfusedMatMul(dyActivation, b3D, false, false);\n      bDer = unfusedMatMul(dyActivation, a3D, true, false);\n    } else if (transposeA && !transposeB) {\n      aDer = unfusedMatMul(b3D, dyActivation, false, true);\n      bDer = unfusedMatMul(a3D, dyActivation, false, false);\n    } else {\n      aDer = unfusedMatMul(b3D, dyActivation, true, true);\n      bDer = unfusedMatMul(dyActivation, a3D, true, true);\n    }\n\n    if (bias != null) {\n      var biasDer = getFusedBiasGradient($bias, dyActivation);\n      return [aDer, bDer, biasDer];\n    } else {\n      return [aDer, bDer];\n    }\n  };\n\n  var forward = function forward(backend) {\n    var y = backend.fusedBatchMatMul({\n      a: a3D,\n      b: b3D,\n      transposeA: transposeA,\n      transposeB: transposeB,\n      bias: $bias,\n      activation: activation,\n      preluActivationWeights: $preluActivationWeights\n    });\n    return y;\n  };\n\n  var inputs = {\n    a: a3D,\n    b: b3D,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  var attrs = {\n    transposeA: transposeA,\n    transposeB: transposeB,\n    activation: activation\n  }; // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n\n  if (bias == null) {\n    var customOp = customGrad(function (a3D, b3D, save) {\n      var res = ENGINE.runKernelFunc(forward, inputs, null\n      /* grad */\n      , _FusedMatMul, attrs);\n      save([a3D, b3D, res]);\n      return {\n        value: reshape(res, outShape),\n        gradFunc: grad\n      };\n    });\n    return customOp(a3D, b3D);\n  } else {\n    var customOpWithBias = customGrad(function (a3D, b3D, $bias, save) {\n      var res = ENGINE.runKernelFunc(forward, inputs, null\n      /* grad */\n      , _FusedMatMul, attrs);\n      save([a3D, b3D, res, $bias]);\n      return {\n        value: reshape(res, outShape),\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(a3D, b3D, $bias);\n  }\n}\n\nexport var matMul = op({\n  fusedMatMul_: fusedMatMul_\n});","map":null,"metadata":{},"sourceType":"module"}