{"ast":null,"code":"import _slicedToArray from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/slicedToArray\";\nimport _toConsumableArray from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/toConsumableArray\";\nimport _regeneratorRuntime from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/regenerator\";\nimport _asyncToGenerator from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\nimport _classCallCheck from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass\";\nimport _possibleConstructorReturn from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/possibleConstructorReturn\";\nimport _get from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/get\";\nimport _getPrototypeOf from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";\nimport _inherits from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/inherits\";\nimport _assertThisInitialized from \"/Users/angeldiscopanda/Trilogy-2020/Projects/trainofthought/client/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/assertThisInitialized\";\n\n/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as tf from '@tensorflow/tfjs-core';\nimport { backend_util, DataStorage, engine, env, kernel_impls, KernelBackend, max, slice_util, TensorBuffer, upcastType, util } from '@tensorflow/tfjs-core';\nvar nonMaxSuppressionV3Impl = kernel_impls.nonMaxSuppressionV3Impl;\nvar _split = kernel_impls.split;\nvar _tile = kernel_impls.tile;\nvar topkImpl = kernel_impls.topkImpl;\nvar whereImpl = kernel_impls.whereImpl;\nimport * as seedrandom from 'seedrandom';\nimport { assertNotComplex } from './cpu_util';\nexport var MathBackendCPU = /*#__PURE__*/function (_KernelBackend) {\n  _inherits(MathBackendCPU, _KernelBackend);\n\n  function MathBackendCPU() {\n    var _this;\n\n    _classCallCheck(this, MathBackendCPU);\n\n    _this = _possibleConstructorReturn(this, _getPrototypeOf(MathBackendCPU).call(this));\n    _this.blockSize = 48;\n    _this.firstUse = true;\n    _this.data = new DataStorage(_assertThisInitialized(_assertThisInitialized(_this)), engine());\n    return _this;\n  }\n\n  _createClass(MathBackendCPU, [{\n    key: \"write\",\n    value: function write(values, shape, dtype) {\n      if (this.firstUse) {\n        this.firstUse = false;\n\n        if (env().get('IS_NODE')) {\n          backend_util.warn('\\n============================\\n' + 'Hi there ðŸ‘‹. Looks like you are running TensorFlow.js in ' + 'Node.js. To speed things up dramatically, install our node ' + 'backend, which binds to TensorFlow C++, by running ' + 'npm i @tensorflow/tfjs-node, ' + 'or npm i @tensorflow/tfjs-node-gpu if you have CUDA. ' + 'Then call require(\\'@tensorflow/tfjs-node\\'); (-gpu ' + 'suffix for CUDA) at the start of your program. ' + 'Visit https://github.com/tensorflow/tfjs-node for more details.' + '\\n============================');\n        }\n      }\n\n      var dataId = {};\n      this.data.set(dataId, {\n        values: values,\n        dtype: dtype,\n        refCount: 1\n      });\n      return dataId;\n    }\n    /**\n     * Create a data bucket in cpu backend.\n     * @param shape Shape of the `TensorInfo`.\n     * @param dtype DType of the `TensorInfo`.\n     * @param values The value of the `TensorInfo` stored as a flattened array.\n     */\n\n  }, {\n    key: \"makeTensorInfo\",\n    value: function makeTensorInfo(shape, dtype, values) {\n      var outId;\n\n      if (dtype === 'string' && values != null && values.length > 0 && util.isString(values[0])) {\n        var encodedValues = values.map(function (d) {\n          return util.encodeString(d);\n        });\n        outId = this.write(encodedValues, shape, dtype);\n      } else {\n        outId = this.write(values, shape, dtype);\n      }\n\n      return {\n        dataId: outId,\n        shape: shape,\n        dtype: dtype\n      };\n    }\n    /** Increase refCount of a `TensorData`. */\n\n  }, {\n    key: \"incRef\",\n    value: function incRef(dataId) {\n      var tensorData = this.data.get(dataId);\n      tensorData.refCount++;\n    }\n    /** Decrease refCount of a `TensorData`. */\n\n  }, {\n    key: \"decRef\",\n    value: function decRef(dataId) {\n      if (this.data.has(dataId)) {\n        var tensorData = this.data.get(dataId);\n        tensorData.refCount--;\n      }\n    }\n  }, {\n    key: \"move\",\n    value: function move(dataId, values, shape, dtype) {\n      this.data.set(dataId, {\n        values: values,\n        dtype: dtype,\n        refCount: 1\n      });\n    }\n  }, {\n    key: \"numDataIds\",\n    value: function numDataIds() {\n      return this.data.numDataIds();\n    }\n  }, {\n    key: \"read\",\n    value: function () {\n      var _read = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee(dataId) {\n        return _regeneratorRuntime.wrap(function _callee$(_context) {\n          while (1) {\n            switch (_context.prev = _context.next) {\n              case 0:\n                return _context.abrupt(\"return\", this.readSync(dataId));\n\n              case 1:\n              case \"end\":\n                return _context.stop();\n            }\n          }\n        }, _callee, this);\n      }));\n\n      function read(_x) {\n        return _read.apply(this, arguments);\n      }\n\n      return read;\n    }()\n  }, {\n    key: \"readSync\",\n    value: function readSync(dataId) {\n      var _this$data$get = this.data.get(dataId),\n          dtype = _this$data$get.dtype,\n          complexTensorInfos = _this$data$get.complexTensorInfos;\n\n      if (dtype === 'complex64') {\n        var realValues = this.readSync(complexTensorInfos.real.dataId);\n        var imagValues = this.readSync(complexTensorInfos.imag.dataId);\n        return backend_util.mergeRealAndImagArrays(realValues, imagValues);\n      }\n\n      return this.data.get(dataId).values;\n    }\n  }, {\n    key: \"bufferSync\",\n    value: function bufferSync(t) {\n      var data = this.readSync(t.dataId);\n      var decodedData = data;\n\n      if (t.dtype === 'string') {\n        try {\n          // Decode the bytes into string.\n          decodedData = data.map(function (d) {\n            return util.decodeString(d);\n          });\n        } catch (_a) {\n          throw new Error('Failed to decode encoded string bytes into utf-8');\n        }\n      }\n\n      return tf.buffer(t.shape, t.dtype, decodedData);\n    }\n  }, {\n    key: \"makeOutput\",\n    value: function makeOutput(values, shape, dtype) {\n      var dataId = this.write(values, shape, dtype);\n      return engine().makeTensorFromDataId(dataId, shape, dtype, this);\n    }\n  }, {\n    key: \"disposeData\",\n    value: function disposeData(dataId) {\n      if (this.data.has(dataId)) {\n        var _this$data$get2 = this.data.get(dataId),\n            complexTensorInfos = _this$data$get2.complexTensorInfos;\n\n        if (complexTensorInfos != null) {\n          this.disposeData(complexTensorInfos.real.dataId);\n          this.disposeData(complexTensorInfos.imag.dataId);\n        }\n\n        this.data.delete(dataId);\n      }\n    }\n  }, {\n    key: \"disposeIntermediateTensorInfo\",\n    value: function disposeIntermediateTensorInfo(tensorInfo) {\n      var dataId = tensorInfo.dataId;\n\n      if (this.data.has(dataId)) {\n        var tensorData = this.data.get(dataId);\n        tensorData.refCount--;\n\n        if (tensorData.refCount < 1) {\n          this.disposeData(dataId);\n        }\n      }\n    }\n  }, {\n    key: \"time\",\n    value: function () {\n      var _time = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2(f) {\n        var start, kernelMs;\n        return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n          while (1) {\n            switch (_context2.prev = _context2.next) {\n              case 0:\n                start = util.now();\n                f();\n                kernelMs = util.now() - start;\n                return _context2.abrupt(\"return\", {\n                  kernelMs: kernelMs\n                });\n\n              case 4:\n              case \"end\":\n                return _context2.stop();\n            }\n          }\n        }, _callee2);\n      }));\n\n      function time(_x2) {\n        return _time.apply(this, arguments);\n      }\n\n      return time;\n    }()\n  }, {\n    key: \"memory\",\n    value: function memory() {\n      return {\n        // Unreliable due to automatic gc. The numbers above are cumulative.\n        unreliable: true,\n        reasons: ['The reported memory is an upper bound. Due to automatic garbage ' + 'collection, the true allocated memory may be less.']\n      };\n    }\n  }, {\n    key: \"stridedSlice\",\n    value: function stridedSlice(x, begin, end, strides) {\n      assertNotComplex(x, 'stridedSlice');\n      var outShape = slice_util.computeOutShape(begin, end, strides);\n\n      if (outShape.some(function (axis) {\n        return axis === 0;\n      })) {\n        return tf.tensor([], outShape);\n      }\n\n      var buffer = tf.buffer(outShape, x.dtype);\n      var xBuf = this.bufferSync(x);\n\n      for (var i = 0; i < buffer.size; i++) {\n        var loc = buffer.indexToLoc(i);\n        var newLoc = new Array(loc.length);\n\n        for (var j = 0; j < newLoc.length; j++) {\n          newLoc[j] = loc[j] * strides[j] + begin[j];\n        }\n\n        buffer.set.apply(buffer, [xBuf.get.apply(xBuf, newLoc)].concat(_toConsumableArray(loc)));\n      }\n\n      return buffer.toTensor();\n    }\n  }, {\n    key: \"diag\",\n    value: function diag(x) {\n      var xVals = this.readSync(x.dataId);\n      var buffer = tf.buffer([x.size, x.size], x.dtype);\n      var vals = buffer.values;\n\n      for (var i = 0; i < xVals.length; i++) {\n        vals[i * x.size + i] = xVals[i];\n      }\n\n      return buffer.toTensor();\n    }\n  }, {\n    key: \"unstack\",\n    value: function unstack(x, axis) {\n      var num = x.shape[axis];\n      var outShape = new Array(x.rank - 1);\n      var outIndex = 0;\n\n      for (var i = 0; i < x.rank; i++) {\n        if (i !== axis) {\n          outShape[outIndex++] = x.shape[i];\n        }\n      }\n\n      var begin = new Array(x.rank).fill(0);\n      var size = x.shape.slice();\n      size[axis] = 1;\n      var res = new Array(num);\n\n      for (var _i = 0; _i < res.length; _i++) {\n        begin[axis] = _i;\n        res[_i] = tf.slice(x, begin, size).reshape(outShape);\n      }\n\n      return res;\n    }\n  }, {\n    key: \"reverse\",\n    value: function reverse(x, axis) {\n      assertNotComplex(x, 'reverse');\n      var buffer = tf.buffer(x.shape, x.dtype);\n      var xBuf = this.bufferSync(x);\n\n      var _loop = function _loop(i) {\n        var outLoc = buffer.indexToLoc(i);\n        var inLoc = outLoc.slice();\n        axis.forEach(function (ax) {\n          return inLoc[ax] = x.shape[ax] - 1 - inLoc[ax];\n        });\n        buffer.set.apply(buffer, [xBuf.get.apply(xBuf, _toConsumableArray(inLoc))].concat(_toConsumableArray(outLoc)));\n      };\n\n      for (var i = 0; i < buffer.size; i++) {\n        _loop(i);\n      }\n\n      return buffer.toTensor();\n    }\n  }, {\n    key: \"neg\",\n    value: function neg(x) {\n      assertNotComplex(x, 'neg'); // TODO(lina128): Use mul directly once neg is modularized.\n\n      return tf.mul(tf.scalar(-1), x);\n    }\n  }, {\n    key: \"addN\",\n    value: function addN(tensors) {\n      var _this2 = this;\n\n      assertNotComplex(tensors, 'addN');\n      var vals = tensors.map(function (t) {\n        return _this2.readSync(t.dataId);\n      });\n      var result = tf.buffer(tensors[0].shape, tensors[0].dtype);\n      var resultVals = result.values;\n\n      for (var i = 0; i < tensors.length; i++) {\n        var currVals = vals[i];\n\n        for (var j = 0; j < resultVals.length; j++) {\n          resultVals[j] += currVals[j];\n        }\n      }\n\n      return result.toTensor();\n    }\n  }, {\n    key: \"softmax\",\n    value: function softmax(logits, dim) {\n      var axes = util.parseAxisParam([dim], logits.shape); // TODO(annxingyuan): Call maxImpl rather than op as part of softmax kernel\n      // modularization.\n\n      var maxLogit = max(logits, axes);\n      var expandedShape = backend_util.expandShapeToKeepDim(maxLogit.shape, axes); // TODO(lina128): Use sub directly once softmax is modularized.\n\n      var a = tf.sub(logits, maxLogit.reshape(expandedShape));\n      var b = tf.exp(a);\n      var sumExp = this.sum(b, axes).reshape(expandedShape); // TODO(annxingyuan): Call divImpl rather than op as part of softmax\n      // kernel modularization.\n\n      return tf.div(b, sumExp);\n    }\n  }, {\n    key: \"pow\",\n    value: function pow(a, b) {\n      assertNotComplex([a, b], 'pow');\n      return this.broadcastedBinaryOp(a, b, a.dtype, function (aValue, bValue) {\n        return Math.pow(aValue, bValue);\n      });\n    }\n  }, {\n    key: \"floorDiv\",\n    value: function floorDiv(a, b) {\n      assertNotComplex([a, b], 'floorDiv');\n\n      var op = function op(a, b) {\n        return Math.floor(a / b);\n      };\n\n      var outputDtype = 'int32';\n      return this.broadcastedBinaryOp(a, b, outputDtype, op);\n    }\n  }, {\n    key: \"sum\",\n    value: function sum(x, axes) {\n      assertNotComplex(x, 'sum');\n      backend_util.assertAxesAreInnerMostDims('sum', axes, x.rank);\n\n      var _backend_util$compute = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute2 = _slicedToArray(_backend_util$compute, 2),\n          outShape = _backend_util$compute2[0],\n          reduceShape = _backend_util$compute2[1];\n\n      var resultDtype = upcastType(x.dtype, 'int32');\n      var result = tf.zeros(outShape, resultDtype);\n      var reduceSize = util.sizeFromShape(reduceShape);\n      var vals = this.readSync(result.dataId);\n      var aVals = this.readSync(x.dataId);\n\n      for (var i = 0; i < vals.length; ++i) {\n        var offset = i * reduceSize;\n        var sum = 0;\n\n        for (var j = 0; j < reduceSize; ++j) {\n          sum += aVals[offset + j];\n        }\n\n        vals[i] = sum;\n      }\n\n      return result;\n    }\n  }, {\n    key: \"prod\",\n    value: function prod(x, axes) {\n      assertNotComplex(x, 'sum');\n\n      var _backend_util$compute3 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute4 = _slicedToArray(_backend_util$compute3, 2),\n          outShape = _backend_util$compute4[0],\n          reduceShape = _backend_util$compute4[1];\n\n      var resultDtype = upcastType(x.dtype, 'int32');\n      var result = tf.zeros(outShape, resultDtype);\n      var reduceSize = util.sizeFromShape(reduceShape);\n      var vals = this.readSync(result.dataId);\n      var aVals = this.readSync(x.dataId);\n\n      for (var i = 0; i < vals.length; ++i) {\n        var offset = i * reduceSize;\n        var prod = 1;\n\n        for (var j = 0; j < reduceSize; ++j) {\n          prod *= aVals[offset + j];\n        }\n\n        vals[i] = prod;\n      }\n\n      return result;\n    }\n  }, {\n    key: \"unsortedSegmentSum\",\n    value: function unsortedSegmentSum(x, segmentIds, numSegments) {\n      assertNotComplex(x, 'unsortedSegmentSum');\n      var res = []; // Reshape the segment id's so that they can be broadcast with\n      // x. The new shape should be [segmentIds.shape, 1, ..., 1]\n\n      var numIters = x.rank - segmentIds.rank;\n\n      for (var i = 0; i < numIters; ++i) {\n        segmentIds = segmentIds.expandDims(i + 1);\n      }\n\n      for (var _i2 = 0; _i2 < numSegments; ++_i2) {\n        var segmentId = tf.scalar(_i2, 'int32');\n        var mask = tf.equal(segmentId, segmentIds).asType('float32');\n        var sum = mask.mul(x).sum(0);\n        res.push(sum);\n      }\n\n      return tf.stack(res);\n    }\n  }, {\n    key: \"argMin\",\n    value: function argMin(x, axis) {\n      assertNotComplex(x, 'argMin');\n      var axes = [axis];\n      backend_util.assertAxesAreInnerMostDims('argMin', axes, x.rank);\n\n      var _backend_util$compute5 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute6 = _slicedToArray(_backend_util$compute5, 2),\n          outShape = _backend_util$compute6[0],\n          reduceShape = _backend_util$compute6[1];\n\n      var result = tf.zeros(outShape, 'int32');\n      var reduceSize = util.sizeFromShape(reduceShape);\n      var vals = this.readSync(result.dataId);\n      var aVals = this.readSync(x.dataId);\n\n      for (var i = 0; i < vals.length; ++i) {\n        var offset = i * reduceSize;\n        var min = aVals[offset];\n        var minIndex = 0;\n\n        for (var j = 0; j < reduceSize; ++j) {\n          var value = aVals[offset + j];\n\n          if (value < min) {\n            min = value;\n            minIndex = j;\n          }\n        }\n\n        vals[i] = minIndex;\n      }\n\n      return result;\n    }\n  }, {\n    key: \"argMax\",\n    value: function argMax(x, axis) {\n      assertNotComplex(x, 'argMax');\n      var axes = [axis];\n      backend_util.assertAxesAreInnerMostDims('argMax', axes, x.rank);\n\n      var _backend_util$compute7 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute8 = _slicedToArray(_backend_util$compute7, 2),\n          outShape = _backend_util$compute8[0],\n          reduceShape = _backend_util$compute8[1];\n\n      var result = tf.zeros(outShape, 'int32');\n      var reduceSize = util.sizeFromShape(reduceShape);\n      var vals = this.readSync(result.dataId);\n      var aVals = this.readSync(x.dataId);\n\n      for (var i = 0; i < vals.length; ++i) {\n        var offset = i * reduceSize;\n        var _max = aVals[offset];\n        var maxIndex = 0;\n\n        for (var j = 0; j < reduceSize; ++j) {\n          var value = aVals[offset + j];\n\n          if (value > _max) {\n            _max = value;\n            maxIndex = j;\n          }\n        }\n\n        vals[i] = maxIndex;\n      }\n\n      return result;\n    }\n  }, {\n    key: \"cumsum\",\n    value: function cumsum(x, axis, exclusive, reverse) {\n      assertNotComplex(x, 'cumsum');\n\n      if (axis !== x.rank - 1) {\n        throw new Error(\"backend.cumsum in CPU expects an inner-most axis=\".concat(x.rank - 1, \" \") + \"but got axis=\".concat(axis));\n      }\n\n      var resultDtype = upcastType(x.dtype, 'int32');\n      var result = tf.zeros(x.shape, resultDtype);\n      var vals = this.readSync(result.dataId);\n      var aVals = this.readSync(x.dataId);\n      var finalDim = x.shape[x.rank - 1];\n      var indexAdjuster = reverse ? function (i, j) {\n        return i + finalDim - j - 1;\n      } : function (i, j) {\n        return i + j;\n      };\n\n      for (var i = 0; i < aVals.length; i += finalDim) {\n        for (var j = 0; j < finalDim; j++) {\n          var idx = indexAdjuster(i, j);\n\n          if (j === 0) {\n            vals[idx] = exclusive ? 0 : aVals[idx];\n          } else {\n            var prevIdx = indexAdjuster(i, j - 1);\n            vals[idx] = exclusive ? aVals[prevIdx] + vals[prevIdx] : aVals[idx] + vals[prevIdx];\n          }\n        }\n      }\n\n      return result;\n    }\n  }, {\n    key: \"equal\",\n    value: function equal(a, b) {\n      assertNotComplex([a, b], 'equal');\n      return this.broadcastedBinaryOp(a, b, 'bool', function (aVal, bVal) {\n        return aVal === bVal ? 1 : 0;\n      });\n    }\n  }, {\n    key: \"notEqual\",\n    value: function notEqual(a, b) {\n      assertNotComplex([a, b], 'notEqual');\n      return this.broadcastedBinaryOp(a, b, 'bool', function (aVal, bVal) {\n        return aVal !== bVal ? 1 : 0;\n      });\n    }\n  }, {\n    key: \"less\",\n    value: function less(a, b) {\n      assertNotComplex([a, b], 'less');\n      return this.broadcastedBinaryOp(a, b, 'bool', function (aVal, bVal) {\n        return aVal < bVal ? 1 : 0;\n      });\n    }\n  }, {\n    key: \"lessEqual\",\n    value: function lessEqual(a, b) {\n      assertNotComplex([a, b], 'lessEqual');\n      return this.broadcastedBinaryOp(a, b, 'bool', function (aVal, bVal) {\n        return aVal <= bVal ? 1 : 0;\n      });\n    }\n  }, {\n    key: \"greater\",\n    value: function greater(a, b) {\n      assertNotComplex([a, b], 'greater');\n      return this.broadcastedBinaryOp(a, b, 'bool', function (aVal, bVal) {\n        return aVal > bVal ? 1 : 0;\n      });\n    }\n  }, {\n    key: \"greaterEqual\",\n    value: function greaterEqual(a, b) {\n      assertNotComplex([a, b], 'greaterEqual');\n      return this.broadcastedBinaryOp(a, b, 'bool', function (aVal, bVal) {\n        return aVal >= bVal ? 1 : 0;\n      });\n    }\n  }, {\n    key: \"logicalAnd\",\n    value: function logicalAnd(a, b) {\n      assertNotComplex([a, b], 'logicalAnd');\n      return this.broadcastedBinaryOp(a, b, 'bool', function (aVal, bVal) {\n        return aVal && bVal;\n      });\n    }\n  }, {\n    key: \"logicalOr\",\n    value: function logicalOr(a, b) {\n      assertNotComplex([a, b], 'logicalOr');\n      return this.broadcastedBinaryOp(a, b, 'bool', function (aVal, bVal) {\n        return aVal || bVal;\n      });\n    }\n  }, {\n    key: \"select\",\n    value: function select(condition, a, b) {\n      assertNotComplex([condition, a, b], 'select');\n      var values = this.readSync(condition.dataId);\n      var aValues = this.readSync(a.dataId);\n      var bValues = this.readSync(b.dataId);\n      var result = tf.zeros(a.shape, upcastType(a.dtype, b.dtype));\n      var newValues = this.readSync(result.dataId);\n      var index = 0;\n      var offset = condition.rank === 0 || condition.rank > 1 || a.rank === 1 ? 1 : util.sizeFromShape(a.shape.slice(1));\n\n      for (var i = 0; i < values.length; i++) {\n        for (var j = 0; j < offset; j++) {\n          if (values[i] === 1) {\n            newValues[index++] = aValues[i];\n          } else {\n            newValues[index++] = bValues[i];\n          }\n        }\n      }\n\n      return result;\n    }\n  }, {\n    key: \"where\",\n    value: function where(condition) {\n      assertNotComplex([condition], 'where');\n      var condVals = this.readSync(condition.dataId);\n      return whereImpl(condition.shape, condVals);\n    }\n  }, {\n    key: \"topk\",\n    value: function topk(x, k, sorted) {\n      assertNotComplex(x, 'topk');\n      var xVals = this.readSync(x.dataId);\n      return topkImpl(xVals, x.shape, x.dtype, k, sorted);\n    }\n  }, {\n    key: \"min\",\n    value: function min(x, axes) {\n      assertNotComplex(x, 'min');\n      backend_util.assertAxesAreInnerMostDims('min', axes, x.rank);\n\n      var _backend_util$compute9 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute10 = _slicedToArray(_backend_util$compute9, 2),\n          outShape = _backend_util$compute10[0],\n          reduceShape = _backend_util$compute10[1];\n\n      var result = tf.zeros(outShape, x.dtype);\n      var reduceSize = util.sizeFromShape(reduceShape);\n      var vals = this.readSync(result.dataId);\n      var aVals = this.readSync(x.dataId);\n\n      for (var i = 0; i < vals.length; ++i) {\n        var offset = i * reduceSize;\n        var min = aVals[offset];\n\n        for (var j = 0; j < reduceSize; ++j) {\n          var value = aVals[offset + j];\n\n          if (value < min) {\n            min = value;\n          }\n        }\n\n        vals[i] = min;\n      }\n\n      return result;\n    }\n  }, {\n    key: \"minimum\",\n    value: function minimum(a, b) {\n      assertNotComplex([a, b], 'minimum');\n      return this.broadcastedBinaryOp(a, b, a.dtype, function (aVal, bVal) {\n        return Math.min(aVal, bVal);\n      });\n    }\n  }, {\n    key: \"mod\",\n    value: function mod(a, b) {\n      assertNotComplex([a, b], 'mod');\n      return this.broadcastedBinaryOp(a, b, a.dtype, function (aVal, bVal) {\n        var rem = aVal % bVal;\n\n        if (aVal < 0 && bVal < 0 || aVal >= 0 && bVal >= 0) {\n          return rem;\n        } else {\n          return (rem + bVal) % bVal;\n        }\n      });\n    }\n  }, {\n    key: \"maximum\",\n    value: function maximum(a, b) {\n      assertNotComplex([a, b], 'maximum');\n      return this.broadcastedBinaryOp(a, b, a.dtype, function (aVal, bVal) {\n        return Math.max(aVal, bVal);\n      });\n    }\n  }, {\n    key: \"all\",\n    value: function all(x, axes) {\n      assertNotComplex(x, 'all');\n      backend_util.assertAxesAreInnerMostDims('all', axes, x.rank);\n\n      var _backend_util$compute11 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute12 = _slicedToArray(_backend_util$compute11, 2),\n          outShape = _backend_util$compute12[0],\n          reduceShape = _backend_util$compute12[1];\n\n      var result = tf.zeros(outShape, x.dtype);\n      var reduceSize = util.sizeFromShape(reduceShape);\n      var vals = this.readSync(result.dataId);\n      var aVals = this.readSync(x.dataId);\n\n      for (var i = 0; i < vals.length; ++i) {\n        var offset = i * reduceSize;\n        var all = aVals[offset];\n\n        for (var j = 0; j < reduceSize; ++j) {\n          var value = aVals[offset + j];\n          all = all && value;\n        }\n\n        vals[i] = all;\n      }\n\n      return result;\n    }\n  }, {\n    key: \"any\",\n    value: function any(x, axes) {\n      assertNotComplex(x, 'any');\n      backend_util.assertAxesAreInnerMostDims('any', axes, x.rank);\n\n      var _backend_util$compute13 = backend_util.computeOutAndReduceShapes(x.shape, axes),\n          _backend_util$compute14 = _slicedToArray(_backend_util$compute13, 2),\n          outShape = _backend_util$compute14[0],\n          reduceShape = _backend_util$compute14[1];\n\n      var result = tf.zeros(outShape, x.dtype);\n      var reduceSize = util.sizeFromShape(reduceShape);\n      var vals = this.readSync(result.dataId);\n      var aVals = this.readSync(x.dataId);\n\n      for (var i = 0; i < vals.length; ++i) {\n        var offset = i * reduceSize;\n        var anyVal = aVals[offset];\n\n        for (var j = 0; j < reduceSize; ++j) {\n          var value = aVals[offset + j];\n          anyVal = anyVal || value;\n        }\n\n        vals[i] = anyVal;\n      }\n\n      return result;\n    }\n  }, {\n    key: \"squaredDifference\",\n    value: function squaredDifference(a, b) {\n      assertNotComplex([a, b], 'squaredDifference');\n      return this.broadcastedBinaryOp(a, b, a.dtype, function (aVal, bVal) {\n        var diff = aVal - bVal;\n        return diff * diff;\n      });\n    }\n  }, {\n    key: \"eluDer\",\n    value: function eluDer(dy, y) {\n      assertNotComplex([dy, y], 'eluDer');\n      var resultValues = new Float32Array(y.size);\n      var values = this.readSync(y.dataId);\n      var dyValues = this.readSync(dy.dataId);\n\n      for (var i = 0; i < values.length; ++i) {\n        var v = values[i];\n\n        if (v >= 1) {\n          resultValues[i] = dyValues[i];\n        } else {\n          resultValues[i] = dyValues[i] * (v + 1);\n        }\n      }\n\n      return this.makeOutput(resultValues, y.shape, 'float32');\n    }\n  }, {\n    key: \"atan2\",\n    value: function atan2(a, b) {\n      assertNotComplex([a, b], 'atan2');\n      return this.broadcastedBinaryOp(a, b, a.dtype, function (aValue, bValue) {\n        return Math.atan2(aValue, bValue);\n      });\n    }\n  }, {\n    key: \"tile\",\n    value: function tile(x, reps) {\n      assertNotComplex(x, 'tile');\n      return _tile(this.bufferSync(x), reps);\n    }\n  }, {\n    key: \"gather\",\n    value: function gather(x, indices, axis) {\n      assertNotComplex([x, indices], 'gather');\n      var newShape = x.shape.slice();\n      var indicesValues = this.readSync(indices.dataId);\n      newShape[axis] = indicesValues.length;\n      var result = tf.buffer(newShape, x.dtype);\n      var xBuf = this.bufferSync(x);\n\n      for (var i = 0; i < result.size; ++i) {\n        var newLoc = result.indexToLoc(i);\n        var originalLoc = newLoc.slice();\n        originalLoc[axis] = indicesValues[newLoc[axis]];\n        var originalIndex = xBuf.locToIndex(originalLoc);\n        result.values[i] = xBuf.values[originalIndex];\n      }\n\n      return result.toTensor();\n    }\n  }, {\n    key: \"batchToSpaceND\",\n    value: function batchToSpaceND(x, blockShape, crops) {\n      assertNotComplex([x], 'batchToSpaceND');\n      var prod = blockShape.reduce(function (a, b) {\n        return a * b;\n      });\n      var reshaped = backend_util.getReshaped(x.shape, blockShape, prod);\n      var permuted = backend_util.getPermuted(reshaped.length, blockShape.length);\n      var reshapedPermuted = backend_util.getReshapedPermuted(x.shape, blockShape, prod);\n      var sliceBeginCoords = backend_util.getSliceBeginCoords(crops, blockShape.length);\n      var sliceSize = backend_util.getSliceSize(reshapedPermuted, crops, blockShape.length);\n      return tf.transpose(x.reshape(reshaped), permuted).reshape(reshapedPermuted).slice(sliceBeginCoords, sliceSize);\n    }\n  }, {\n    key: \"pool3d\",\n    value: function pool3d(x, convInfo, poolType) {\n      assertNotComplex(x, 'pool3d');\n      var strideDepth = convInfo.strideDepth;\n      var strideHeight = convInfo.strideHeight;\n      var strideWidth = convInfo.strideWidth;\n      var dilationDepth = convInfo.dilationDepth;\n      var dilationHeight = convInfo.dilationHeight;\n      var dilationWidth = convInfo.dilationWidth;\n      var effectiveFilterDepth = convInfo.effectiveFilterDepth;\n      var effectiveFilterHeight = convInfo.effectiveFilterHeight;\n      var effectiveFilterWidth = convInfo.effectiveFilterWidth;\n      var padFront = convInfo.padInfo.front;\n      var padTop = convInfo.padInfo.top;\n      var padLeft = convInfo.padInfo.left;\n      var initialValue = poolType === 'max' ? Number.NEGATIVE_INFINITY : Number.POSITIVE_INFINITY;\n      var xValues = this.readSync(x.dataId);\n      var output = tf.buffer(convInfo.outShape, x.dtype);\n      var outputVals = output.values;\n      var outputBatchStrides = convInfo.outShape[1] * convInfo.outShape[2] * convInfo.outShape[3] * convInfo.outShape[4];\n      var outputDepthStrides = convInfo.outShape[2] * convInfo.outShape[3] * convInfo.outShape[4];\n      var outputRowStrides = convInfo.outShape[3] * convInfo.outShape[4];\n      var outputColStrides = convInfo.outShape[4];\n\n      for (var batch = 0; batch < convInfo.batchSize; ++batch) {\n        var outputBatchOffset = batch * outputBatchStrides;\n        var inputBatchOffset = batch * x.strides[0];\n\n        for (var channel = 0; channel < convInfo.inChannels; ++channel) {\n          for (var yDepth = 0; yDepth < convInfo.outDepth; ++yDepth) {\n            var xDepthCorner = yDepth * strideDepth - padFront;\n            var xDepthMin = xDepthCorner;\n\n            while (xDepthMin < 0) {\n              xDepthMin += dilationDepth;\n            }\n\n            var xDepthMax = Math.min(convInfo.inDepth, effectiveFilterDepth + xDepthCorner);\n            var outputDepthOffset = outputBatchOffset + yDepth * outputDepthStrides;\n\n            for (var yRow = 0; yRow < convInfo.outHeight; ++yRow) {\n              var xRowCorner = yRow * strideHeight - padTop;\n              var xRowMin = xRowCorner;\n\n              while (xRowMin < 0) {\n                xRowMin += dilationHeight;\n              }\n\n              var xRowMax = Math.min(convInfo.inHeight, effectiveFilterHeight + xRowCorner);\n              var outputRowOffset = outputDepthOffset + yRow * outputRowStrides;\n\n              for (var yCol = 0; yCol < convInfo.outWidth; ++yCol) {\n                var xColCorner = yCol * strideWidth - padLeft;\n                var xColMin = xColCorner;\n\n                while (xColMin < 0) {\n                  xColMin += dilationWidth;\n                }\n\n                var xColMax = Math.min(convInfo.inWidth, effectiveFilterWidth + xColCorner); // Shader code begins\n\n                var outputColOffset = outputRowOffset + yCol * outputColStrides;\n                var minMaxValue = initialValue;\n                var avgValue = 0;\n                var count = 0;\n\n                for (var xDepth = xDepthMin; xDepth < xDepthMax; xDepth += dilationDepth) {\n                  var xDepthOffset = inputBatchOffset + xDepth * x.strides[1];\n\n                  for (var xRow = xRowMin; xRow < xRowMax; xRow += dilationHeight) {\n                    var xRowOffset = xDepthOffset + xRow * x.strides[2];\n\n                    for (var xCol = xColMin; xCol < xColMax; xCol += dilationWidth) {\n                      var xColOffset = xRowOffset + xCol * x.strides[3];\n                      var pixel = xValues[xColOffset + channel];\n\n                      if (poolType === 'max' && pixel > minMaxValue) {\n                        minMaxValue = pixel;\n                      } else if (poolType === 'avg') {\n                        avgValue += pixel;\n                        count++;\n                      }\n\n                      if (isNaN(minMaxValue)) {\n                        break;\n                      }\n                    }\n\n                    if (isNaN(minMaxValue)) {\n                      break;\n                    }\n                  }\n\n                  if (isNaN(minMaxValue)) {\n                    break;\n                  }\n                }\n\n                var outputOffset = outputColOffset + channel;\n                outputVals[outputOffset] = poolType === 'avg' ? avgValue / count : minMaxValue;\n              }\n            }\n          }\n        }\n      }\n\n      return output.toTensor();\n    }\n  }, {\n    key: \"avgPool3d\",\n    value: function avgPool3d(x, convInfo) {\n      assertNotComplex(x, 'avgPool3d');\n      return this.pool3d(x, convInfo, 'avg').toFloat();\n    }\n  }, {\n    key: \"avgPool3dBackprop\",\n    value: function avgPool3dBackprop(dy, x, convInfo) {\n      assertNotComplex([dy, x], 'avgPool3dBackprop');\n      var strideDepth = convInfo.strideDepth;\n      var strideHeight = convInfo.strideHeight;\n      var strideWidth = convInfo.strideWidth;\n      var filterDepth = convInfo.filterDepth;\n      var filterHeight = convInfo.filterHeight;\n      var filterWidth = convInfo.filterWidth;\n      var dilationDepth = convInfo.dilationDepth;\n      var dilationHeight = convInfo.dilationHeight;\n      var dilationWidth = convInfo.dilationWidth;\n      var effectiveFilterDepth = convInfo.effectiveFilterDepth;\n      var effectiveFilterHeight = convInfo.effectiveFilterHeight;\n      var effectiveFilterWidth = convInfo.effectiveFilterWidth;\n      var padFront = effectiveFilterDepth - 1 - convInfo.padInfo.front;\n      var padLeft = effectiveFilterWidth - 1 - convInfo.padInfo.left;\n      var padTop = effectiveFilterHeight - 1 - convInfo.padInfo.top;\n      var dx = tf.buffer(x.shape, 'float32');\n      var avgMultiplier = 1 / (filterDepth * filterHeight * filterWidth);\n      var dyBuf = this.bufferSync(dy);\n\n      for (var batch = 0; batch < convInfo.batchSize; ++batch) {\n        for (var channel = 0; channel < convInfo.inChannels; ++channel) {\n          for (var dxDepth = 0; dxDepth < convInfo.inDepth; ++dxDepth) {\n            for (var dxRow = 0; dxRow < convInfo.inHeight; ++dxRow) {\n              for (var dxCol = 0; dxCol < convInfo.inWidth; ++dxCol) {\n                // Shader code begins.\n                var dyDepthCorner = dxDepth - padFront;\n                var dyRowCorner = dxRow - padTop;\n                var dyColCorner = dxCol - padLeft;\n                var dotProd = 0;\n\n                for (var wDepth = 0; wDepth < effectiveFilterDepth; wDepth += dilationDepth) {\n                  var dyDepth = (dyDepthCorner + wDepth) / strideDepth;\n\n                  if (dyDepth < 0 || dyDepth >= convInfo.outDepth || Math.floor(dyDepth) !== dyDepth) {\n                    continue;\n                  }\n\n                  for (var wRow = 0; wRow < effectiveFilterHeight; wRow += dilationHeight) {\n                    var dyRow = (dyRowCorner + wRow) / strideHeight;\n\n                    if (dyRow < 0 || dyRow >= convInfo.outHeight || Math.floor(dyRow) !== dyRow) {\n                      continue;\n                    }\n\n                    for (var wCol = 0; wCol < effectiveFilterWidth; wCol += dilationWidth) {\n                      var dyCol = (dyColCorner + wCol) / strideWidth;\n\n                      if (dyCol < 0 || dyCol >= convInfo.outWidth || Math.floor(dyCol) !== dyCol) {\n                        continue;\n                      }\n\n                      var pixel = dyBuf.get(batch, dyDepth, dyRow, dyCol, channel);\n                      dotProd += pixel;\n                    }\n                  }\n                }\n\n                dx.set(dotProd * avgMultiplier, batch, dxDepth, dxRow, dxCol, channel);\n              }\n            }\n          }\n        }\n      }\n\n      return dx.toTensor();\n    }\n  }, {\n    key: \"maxPool3d\",\n    value: function maxPool3d(x, convInfo) {\n      assertNotComplex(x, 'maxPool3d');\n      return this.pool3d(x, convInfo, 'max').toFloat();\n    }\n  }, {\n    key: \"maxPool3dPositions\",\n    value: function maxPool3dPositions(x, convInfo) {\n      var maxPositions = tf.buffer(convInfo.outShape, 'int32');\n      var strideDepth = convInfo.strideDepth;\n      var strideHeight = convInfo.strideHeight;\n      var strideWidth = convInfo.strideWidth;\n      var dilationDepth = convInfo.dilationDepth;\n      var dilationHeight = convInfo.dilationHeight;\n      var dilationWidth = convInfo.dilationWidth;\n      var effectiveFilterDepth = convInfo.effectiveFilterDepth;\n      var effectiveFilterHeight = convInfo.effectiveFilterHeight;\n      var effectiveFilterWidth = convInfo.effectiveFilterWidth;\n      var padFront = convInfo.padInfo.front;\n      var padTop = convInfo.padInfo.top;\n      var padLeft = convInfo.padInfo.left;\n      var xBuf = this.bufferSync(x);\n\n      for (var batch = 0; batch < convInfo.batchSize; ++batch) {\n        for (var channel = 0; channel < convInfo.inChannels; ++channel) {\n          for (var yDepth = 0; yDepth < convInfo.outDepth; ++yDepth) {\n            var xDepthCorner = yDepth * strideDepth - padFront;\n            var xDepthMin = xDepthCorner;\n\n            while (xDepthMin < 0) {\n              xDepthMin += dilationDepth;\n            }\n\n            var xDepthMax = Math.min(convInfo.inDepth, effectiveFilterDepth + xDepthCorner);\n\n            for (var yRow = 0; yRow < convInfo.outHeight; ++yRow) {\n              var xRowCorner = yRow * strideHeight - padTop;\n              var xRowMin = xRowCorner;\n\n              while (xRowMin < 0) {\n                xRowMin += dilationHeight;\n              }\n\n              var xRowMax = Math.min(convInfo.inHeight, effectiveFilterHeight + xRowCorner);\n\n              for (var yCol = 0; yCol < convInfo.outWidth; ++yCol) {\n                var xColCorner = yCol * strideWidth - padLeft;\n                var xColMin = xColCorner;\n\n                while (xColMin < 0) {\n                  xColMin += dilationWidth;\n                }\n\n                var xColMax = Math.min(convInfo.inWidth, effectiveFilterWidth + xColCorner); // Shader code begins\n\n                var maxValue = Number.NEGATIVE_INFINITY;\n                var maxPosition = -1;\n\n                for (var xDepth = xDepthMin; xDepth < xDepthMax; xDepth += dilationDepth) {\n                  var wDepth = xDepth - xDepthCorner;\n\n                  for (var xRow = xRowMin; xRow < xRowMax; xRow += dilationHeight) {\n                    var wRow = xRow - xRowCorner;\n\n                    for (var xCol = xColMin; xCol < xColMax; xCol += dilationWidth) {\n                      var wCol = xCol - xColCorner;\n                      var pixel = xBuf.get(batch, xDepth, xRow, xCol, channel);\n\n                      if (pixel >= maxValue) {\n                        maxValue = pixel;\n                        maxPosition = wDepth * effectiveFilterHeight * effectiveFilterWidth + wRow * effectiveFilterHeight + wCol;\n                      }\n                    }\n                  }\n                }\n\n                maxPositions.set(maxPosition, batch, yDepth, yRow, yCol, channel);\n              }\n            }\n          }\n        }\n      }\n\n      return maxPositions.toTensor();\n    }\n  }, {\n    key: \"maxPool3dBackprop\",\n    value: function maxPool3dBackprop(dy, x, y, convInfo) {\n      assertNotComplex([x, y], 'maxPool3dBackprop');\n      var maxPositions = this.maxPool3dPositions(x, convInfo);\n      var strideDepth = convInfo.strideDepth;\n      var strideHeight = convInfo.strideHeight;\n      var strideWidth = convInfo.strideWidth;\n      var dilationDepth = convInfo.dilationDepth;\n      var dilationHeight = convInfo.dilationHeight;\n      var dilationWidth = convInfo.dilationWidth;\n      var effectiveFilterDepth = convInfo.effectiveFilterDepth;\n      var effectiveFilterHeight = convInfo.effectiveFilterHeight;\n      var effectiveFilterWidth = convInfo.effectiveFilterWidth;\n      var padFront = effectiveFilterDepth - 1 - convInfo.padInfo.front;\n      var padLeft = effectiveFilterWidth - 1 - convInfo.padInfo.left;\n      var padTop = effectiveFilterHeight - 1 - convInfo.padInfo.top;\n      var dx = tf.buffer(x.shape, 'float32');\n      var maxPosBuf = this.bufferSync(maxPositions);\n      var dyBuf = this.bufferSync(dy);\n\n      for (var batch = 0; batch < convInfo.batchSize; ++batch) {\n        for (var channel = 0; channel < convInfo.inChannels; ++channel) {\n          for (var dxDepth = 0; dxDepth < convInfo.inDepth; ++dxDepth) {\n            for (var dxRow = 0; dxRow < convInfo.inHeight; ++dxRow) {\n              for (var dxCol = 0; dxCol < convInfo.inWidth; ++dxCol) {\n                // Shader code begins\n                var dyDepthCorner = dxDepth - padFront;\n                var dyRowCorner = dxRow - padTop;\n                var dyColCorner = dxCol - padLeft;\n                var dotProd = 0;\n\n                for (var wDepth = 0; wDepth < effectiveFilterDepth; wDepth += dilationDepth) {\n                  var dyDepth = (dyDepthCorner + wDepth) / strideDepth;\n\n                  if (dyDepth < 0 || dyDepth >= convInfo.outDepth || Math.floor(dyDepth) !== dyDepth) {\n                    continue;\n                  }\n\n                  for (var wRow = 0; wRow < effectiveFilterHeight; wRow += dilationHeight) {\n                    var dyRow = (dyRowCorner + wRow) / strideHeight;\n\n                    if (dyRow < 0 || dyRow >= convInfo.outHeight || Math.floor(dyRow) !== dyRow) {\n                      continue;\n                    }\n\n                    for (var wCol = 0; wCol < effectiveFilterWidth; wCol += dilationWidth) {\n                      var dyCol = (dyColCorner + wCol) / strideWidth;\n\n                      if (dyCol < 0 || dyCol >= convInfo.outWidth || Math.floor(dyCol) !== dyCol) {\n                        continue;\n                      }\n\n                      var maxPos = effectiveFilterDepth * effectiveFilterHeight * effectiveFilterWidth - 1 - maxPosBuf.get(batch, dyDepth, dyRow, dyCol, channel);\n                      var curPos = wDepth * effectiveFilterHeight * effectiveFilterWidth + wRow * effectiveFilterWidth + wCol;\n                      var mask = maxPos === curPos ? 1 : 0;\n\n                      if (mask === 0) {\n                        continue;\n                      }\n\n                      var pixel = dyBuf.get(batch, dyDepth, dyRow, dyCol, channel);\n                      dotProd += pixel * mask;\n                    }\n                  }\n                }\n\n                dx.set(dotProd, batch, dxDepth, dxRow, dxCol, channel);\n              }\n            }\n          }\n        }\n      }\n\n      return dx.toTensor();\n    }\n  }, {\n    key: \"resizeBilinear\",\n    value: function resizeBilinear(x, newHeight, newWidth, alignCorners) {\n      assertNotComplex(x, 'resizeBilinear');\n\n      var _x$shape = _slicedToArray(x.shape, 4),\n          batch = _x$shape[0],\n          oldHeight = _x$shape[1],\n          oldWidth = _x$shape[2],\n          numChannels = _x$shape[3];\n\n      var xValues = this.readSync(x.dataId);\n      var result = new Float32Array(util.sizeFromShape([batch, newHeight, newWidth, numChannels]));\n      var effectiveInputSize = [alignCorners && newHeight > 1 ? oldHeight - 1 : oldHeight, alignCorners && newWidth > 1 ? oldWidth - 1 : oldWidth];\n      var effectiveOutputSize = [alignCorners && newHeight > 1 ? newHeight - 1 : newHeight, alignCorners && newWidth > 1 ? newWidth - 1 : newWidth];\n      var outputIdx = 0;\n      var effectiveRowSizeRatio = effectiveInputSize[0] / effectiveOutputSize[0];\n      var effectiveColSizeRatio = effectiveInputSize[1] / effectiveOutputSize[1];\n\n      for (var b = 0; b < batch; b++) {\n        for (var r = 0; r < newHeight; r++) {\n          var sourceFracRow = effectiveRowSizeRatio * r;\n          var sourceRowFloor = Math.floor(sourceFracRow);\n          var rowFrac = sourceFracRow - sourceRowFloor;\n          var sourceRowCeil = Math.min(oldHeight - 1, Math.ceil(sourceFracRow));\n          var topRowOffset = b * x.strides[0] + sourceRowFloor * x.strides[1];\n          var botRowOffset = b * x.strides[0] + sourceRowCeil * x.strides[1];\n\n          for (var c = 0; c < newWidth; c++) {\n            var sourceFracCol = effectiveColSizeRatio * c;\n            var sourceColFloor = Math.floor(sourceFracCol);\n            var colFrac = sourceFracCol - sourceColFloor;\n            var sourceColCeil = Math.min(oldWidth - 1, Math.ceil(sourceFracCol));\n            var topLeftOffest = topRowOffset + sourceColFloor * x.strides[2];\n            var botLeftOffset = botRowOffset + sourceColFloor * x.strides[2];\n            var topRightOffset = topRowOffset + sourceColCeil * x.strides[2];\n            var botRightOffest = botRowOffset + sourceColCeil * x.strides[2];\n\n            for (var d = 0; d < numChannels; d++) {\n              // Begin shader.\n              // Compute the fractional index of the source.\n              var topLeft = xValues[topLeftOffest + d];\n              var bottomLeft = xValues[botLeftOffset + d];\n              var topRight = xValues[topRightOffset + d];\n              var bottomRight = xValues[botRightOffest + d];\n              var top = topLeft + (topRight - topLeft) * colFrac;\n              var bottom = bottomLeft + (bottomRight - bottomLeft) * colFrac;\n              var newValue = top + (bottom - top) * rowFrac;\n              result[outputIdx++] = newValue;\n            }\n          }\n        }\n      }\n\n      return tf.tensor(result, [batch, newHeight, newWidth, numChannels]);\n    }\n  }, {\n    key: \"resizeBilinearBackprop\",\n    value: function resizeBilinearBackprop(dy, x, alignCorners) {\n      assertNotComplex([dy, x], 'resizeBilinearBackprop');\n\n      var _x$shape2 = _slicedToArray(x.shape, 4),\n          batch = _x$shape2[0],\n          xHeight = _x$shape2[1],\n          xWidth = _x$shape2[2],\n          depth = _x$shape2[3];\n\n      var _dy$shape = _slicedToArray(dy.shape, 3),\n          yHeight = _dy$shape[1],\n          yWidth = _dy$shape[2];\n\n      var output = new Float32Array(batch * xHeight * xWidth * depth); // In the backwards pass, we want to find the pixels that were generated\n      // for each pixel in the input image the forward pass and add the\n      // corresponding coefficient from dy to the gradient (with some\n      // interpolation).\n\n      var effectiveXSize = [alignCorners && yHeight > 1 ? xHeight - 1 : xHeight, alignCorners && yWidth > 1 ? xWidth - 1 : xWidth];\n      var effectiveYSize = [alignCorners && yHeight > 1 ? yHeight - 1 : yHeight, alignCorners && yWidth > 1 ? yWidth - 1 : yWidth];\n      var heightScale = effectiveXSize[0] / effectiveYSize[0];\n      var widthScale = effectiveXSize[1] / effectiveYSize[1]; // Reference implementation\n      // tslint:disable-next-line:max-line-length\n      // https://github.com/tensorflow/tensorflow/blob/3039375c86a5bbc9610c7725dcaa95d635f87ba2/tensorflow/core/kernels/resize_bilinear_op.cc#L275\n\n      var dyValues = this.readSync(dy.dataId);\n      var offset = 0;\n\n      for (var b = 0; b < batch; b++) {\n        var bOffset = b * x.strides[0];\n\n        for (var r = 0; r < yHeight; r++) {\n          var dxR = r * heightScale;\n          var topDxRIndex = Math.floor(dxR);\n          var bottomDxRIndex = Math.min(Math.ceil(dxR), xHeight - 1);\n          var topDxROffset = bOffset + topDxRIndex * x.strides[1];\n          var bottomDxROffset = bOffset + bottomDxRIndex * x.strides[1];\n          var dxRLerp = dxR - topDxRIndex;\n          var inverseDxRLerp = 1.0 - dxRLerp;\n\n          for (var c = 0; c < yWidth; c++) {\n            var dxC = c * widthScale;\n            var leftDxCIndex = Math.floor(dxC);\n            var rightDxCIndex = Math.min(Math.ceil(dxC), xWidth - 1);\n            var dxCLerp = dxC - leftDxCIndex;\n            var inverseDxCLerp = 1.0 - dxCLerp;\n            var topLeftRCOffset = topDxROffset + leftDxCIndex * x.strides[2];\n            var topRightRCOffset = topDxROffset + rightDxCIndex * x.strides[2];\n            var bottomLeftRCOffset = bottomDxROffset + leftDxCIndex * x.strides[2];\n            var bottomRightRCOffset = bottomDxROffset + rightDxCIndex * x.strides[2];\n            var inverseDxRLerpTimesInverseDxCLerp = inverseDxRLerp * inverseDxCLerp;\n            var inverseDxRLerpTimesDxCLerp = inverseDxRLerp * dxCLerp;\n            var dxRLerpTimesInverseDxCLerp = dxRLerp * inverseDxCLerp;\n            var dxRLerpTimesDxCLerp = dxRLerp * dxCLerp;\n\n            for (var d = 0; d < depth; d++) {\n              var dyVal = dyValues[offset++];\n              output[topLeftRCOffset + d] += dyVal * inverseDxRLerpTimesInverseDxCLerp;\n              output[topRightRCOffset + d] += dyVal * inverseDxRLerpTimesDxCLerp;\n              output[bottomLeftRCOffset + d] += dyVal * dxRLerpTimesInverseDxCLerp;\n              output[bottomRightRCOffset + d] += dyVal * dxRLerpTimesDxCLerp;\n            }\n          }\n        }\n      }\n\n      return tf.tensor4d(output, [batch, xWidth, xHeight, depth], x.dtype);\n    }\n  }, {\n    key: \"resizeNearestNeighbor\",\n    value: function resizeNearestNeighbor(x, newHeight, newWidth, alignCorners) {\n      assertNotComplex(x, 'resizeNearestNeighbor');\n\n      var _x$shape3 = _slicedToArray(x.shape, 4),\n          batch = _x$shape3[0],\n          oldHeight = _x$shape3[1],\n          oldWidth = _x$shape3[2],\n          numChannels = _x$shape3[3];\n\n      var xValues = this.readSync(x.dataId);\n      var output = new Float32Array(batch * newHeight * newWidth * numChannels);\n      var effectiveInputSize = [alignCorners && newHeight > 1 ? oldHeight - 1 : oldHeight, alignCorners && newWidth > 1 ? oldWidth - 1 : oldWidth];\n      var effectiveOutputSize = [alignCorners && newHeight > 1 ? newHeight - 1 : newHeight, alignCorners && newWidth > 1 ? newWidth - 1 : newWidth];\n      var effectiveRowSizeRatio = effectiveInputSize[0] / effectiveOutputSize[0];\n      var effectiveColSizeRatio = effectiveInputSize[1] / effectiveOutputSize[1];\n      var outputOffset = 0;\n\n      for (var b = 0; b < batch; b++) {\n        var batchOffset = b * x.strides[0];\n\n        for (var r = 0; r < newHeight; r++) {\n          var sourceFracRow = effectiveRowSizeRatio * r;\n          var sourceNearestRow = Math.min(oldHeight - 1, alignCorners ? Math.round(sourceFracRow) : Math.floor(sourceFracRow));\n          var rowOffset = batchOffset + sourceNearestRow * x.strides[1];\n\n          for (var c = 0; c < newWidth; c++) {\n            var sourceFracCol = effectiveColSizeRatio * c;\n            var sourceNearestCol = Math.min(oldWidth - 1, alignCorners ? Math.round(sourceFracCol) : Math.floor(sourceFracCol));\n            var colOffset = rowOffset + sourceNearestCol * x.strides[2];\n\n            for (var d = 0; d < numChannels; d++) {\n              // Begin shader.\n              // Compute the fractional index of the source.\n              var newVal = xValues[colOffset + d];\n              output[outputOffset++] = newVal;\n            }\n          }\n        }\n      }\n\n      return tf.tensor(output, [batch, newHeight, newWidth, numChannels], x.dtype);\n    }\n  }, {\n    key: \"resizeNearestNeighborBackprop\",\n    value: function resizeNearestNeighborBackprop(dy, x, alignCorners) {\n      assertNotComplex([dy, x], 'resizeNearestNeighborBackprop');\n\n      var _x$shape4 = _slicedToArray(x.shape, 4),\n          batch = _x$shape4[0],\n          xHeight = _x$shape4[1],\n          xWidth = _x$shape4[2],\n          depth = _x$shape4[3];\n\n      var _dy$shape2 = _slicedToArray(dy.shape, 3),\n          yHeight = _dy$shape2[1],\n          yWidth = _dy$shape2[2];\n\n      var output = new Float32Array(batch * xHeight * xWidth * depth);\n      var dyValues = this.readSync(dy.dataId); // In the backwards pass, we want to find the pixels that were generated\n      // for each pixel in the input image the forward pass\n\n      var effectiveXSize = [alignCorners && yHeight > 1 ? xHeight - 1 : xHeight, alignCorners && yWidth > 1 ? xWidth - 1 : xWidth];\n      var effectiveYSize = [alignCorners && yHeight > 1 ? yHeight - 1 : yHeight, alignCorners && yWidth > 1 ? yWidth - 1 : yWidth];\n      var heightScale = effectiveXSize[0] / effectiveYSize[0];\n      var widthScale = effectiveXSize[1] / effectiveYSize[1];\n      var invHeightScale = 1 / heightScale;\n      var invWidthScale = 1 / widthScale; // This defines the size of the window of values around a particular\n      // index in dy that we want to search for contributions to dx.\n\n      var winHeight = Math.ceil(invHeightScale) * 2 + 2;\n      var winWidth = Math.ceil(invWidthScale) * 2 + 2; // Loop over the output space.\n\n      for (var b = 0; b < batch; b++) {\n        var batchOffset = b * x.strides[0];\n\n        for (var r = 0; r < xHeight; r++) {\n          var rowOffset = batchOffset + r * x.strides[1]; // Compute bounds for where in dy we will look\n\n          var startRLerp = Math.floor(r * invHeightScale);\n          var startDyR = Math.floor(startRLerp - winHeight / 2);\n\n          for (var c = 0; c < xWidth; c++) {\n            var colOffset = rowOffset + c * x.strides[2]; // Compute bounds for where in dy we will look\n\n            var startCLerp = Math.floor(c * invWidthScale);\n            var startDyC = Math.floor(startCLerp - winWidth / 2);\n\n            for (var d = 0; d < depth; d++) {\n              var accum = 0; // loop over dy\n\n              for (var dyRIndex = 0; dyRIndex < winHeight; dyRIndex++) {\n                var dyR = dyRIndex + startDyR; // Guard against the window exceeding the bounds of dy\n\n                if (dyR < 0 || dyR >= yHeight) {\n                  continue;\n                }\n\n                var dyROffset = batchOffset + dyR * dy.strides[1];\n                var sourceFracRow = dyR * heightScale;\n                var sourceNearestRow = Math.min(xHeight - 1, alignCorners ? Math.round(sourceFracRow) : Math.floor(sourceFracRow));\n\n                if (r !== sourceNearestRow) {\n                  continue;\n                }\n\n                for (var dyCIndex = 0; dyCIndex < winWidth; dyCIndex++) {\n                  var dyC = dyCIndex + startDyC; // Guard against the window exceeding the bounds of dy\n\n                  if (dyC < 0 || dyC >= yWidth) {\n                    continue;\n                  }\n\n                  var dyCOffset = dyROffset + dyC * dy.strides[2];\n                  var sourceFracCol = dyC * widthScale;\n                  var sourceNearestCol = Math.min(xWidth - 1, alignCorners ? Math.round(sourceFracCol) : Math.floor(sourceFracCol));\n\n                  if (c === sourceNearestCol) {\n                    accum += dyValues[dyCOffset + d];\n                  }\n                }\n              }\n\n              output[colOffset + d] = accum;\n            }\n          }\n        }\n      }\n\n      return tf.tensor4d(output, x.shape, x.dtype);\n    }\n  }, {\n    key: \"localResponseNormalization4D\",\n    value: function localResponseNormalization4D(x, depthRadius, bias, alpha, beta) {\n      assertNotComplex(x, 'localResponseNormalization4D');\n      var channels = x.shape[3];\n      var maxD = channels - 1;\n      var xValues = this.readSync(x.dataId);\n      var size = x.size;\n      var result = new Float32Array(size);\n\n      function sumAcrossChannels(offset) {\n        var currentChannel = offset % channels;\n        var beginSumOffset = offset - currentChannel + Math.max(0, currentChannel - depthRadius);\n        var endSumOffset = offset - currentChannel + Math.min(currentChannel + depthRadius, maxD);\n        var sum = 0.0;\n\n        for (; beginSumOffset <= endSumOffset; beginSumOffset++) {\n          var z = xValues[beginSumOffset];\n          sum += z * z;\n        }\n\n        return sum;\n      }\n\n      for (var offset = 0; offset < size; offset++) {\n        var sum = sumAcrossChannels(offset);\n        var val = xValues[offset] * Math.pow(bias + alpha * sum, -beta);\n        result[offset] = val;\n      }\n\n      return tf.tensor4d(result, x.shape);\n    }\n  }, {\n    key: \"LRNGrad\",\n    value: function LRNGrad(dy, inputImage, outputImage, depthRadius, bias, alpha, beta) {\n      assertNotComplex(dy, 'LRNGrad');\n      var channels = dy.shape[3];\n      var dyValues = this.readSync(dy.dataId);\n      var inputImageValues = this.readSync(inputImage.dataId);\n      var outputImageValues = this.readSync(outputImage.dataId);\n      var result = new Float32Array(dy.size);\n      var size = dy.size;\n\n      for (var offset = 0; offset < size; offset++) {\n        var currentChannel = offset % channels;\n        var depthBegin = offset - currentChannel + Math.max(0, currentChannel - depthRadius);\n        var depthEnd = offset - currentChannel + Math.min(channels, currentChannel + depthRadius + 1);\n        var norm = 0;\n\n        for (var k = depthBegin; k < depthEnd; k++) {\n          norm += Math.pow(inputImageValues[k], 2);\n        }\n\n        norm = alpha * norm + bias;\n\n        for (var _k = depthBegin; _k < depthEnd; _k++) {\n          var dyi = -2 * alpha * beta * inputImageValues[_k] * outputImageValues[offset] / norm;\n\n          if (offset === _k) {\n            dyi += Math.pow(norm, -beta);\n          }\n\n          dyi *= dyValues[offset];\n          result[_k] += dyi;\n        }\n      }\n\n      return tf.tensor4d(result, dy.shape);\n    }\n  }, {\n    key: \"multinomial\",\n    value: function multinomial(logits, normalized, numSamples, seed) {\n      assertNotComplex(logits, 'multinomial');\n      var probabilities = normalized ? logits : tf.softmax(logits);\n      var batchSize = probabilities.shape[0];\n      var numEvents = probabilities.shape[1];\n      var res = tf.zeros([batchSize, numSamples], 'int32');\n      var resVals = this.readSync(res.dataId);\n      var probVals = this.readSync(probabilities.dataId);\n\n      for (var b = 0; b < batchSize; ++b) {\n        var offset = b * numEvents; // The cdf won't include the last event. It will be implicit if no other\n        // event happened.\n\n        var cdf = new Float32Array(numEvents - 1);\n        cdf[0] = probVals[offset];\n\n        for (var event = 1; event < cdf.length; ++event) {\n          cdf[event] = cdf[event - 1] + probVals[offset + event];\n        }\n\n        var random = seedrandom.alea(seed.toString());\n        var outOffset = b * numSamples;\n\n        for (var sampleId = 0; sampleId < numSamples; ++sampleId) {\n          var r = random(); // Assume last event happened by default.\n\n          resVals[outOffset + sampleId] = cdf.length;\n\n          for (var _event = 0; _event < cdf.length; _event++) {\n            if (r < cdf[_event]) {\n              resVals[outOffset + sampleId] = _event;\n              break;\n            }\n          }\n        }\n      }\n\n      return res;\n    }\n  }, {\n    key: \"oneHot\",\n    value: function oneHot(indices, depth, onValue, offValue) {\n      assertNotComplex(indices, 'oneHot');\n      var res = new Float32Array(indices.size * depth);\n      res.fill(offValue);\n      var indicesVal = this.readSync(indices.dataId);\n\n      for (var event = 0; event < indices.size; ++event) {\n        if (indicesVal[event] >= 0 && indicesVal[event] < depth) {\n          res[event * depth + indicesVal[event]] = onValue;\n        }\n      }\n\n      return tf.tensor2d(res, [indices.size, depth], 'int32');\n    }\n  }, {\n    key: \"nonMaxSuppression\",\n    value: function nonMaxSuppression(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold) {\n      assertNotComplex(boxes, 'nonMaxSuppression');\n      var boxesVals = this.readSync(boxes.dataId);\n      var scoresVals = this.readSync(scores.dataId);\n      return nonMaxSuppressionV3Impl(boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold);\n    }\n  }, {\n    key: \"depthToSpace\",\n    value: function depthToSpace(x, blockSize, dataFormat) {\n      util.assert(dataFormat === 'NHWC', function () {\n        return \"Only NHWC dataFormat supported on CPU for depthToSpace. Got \".concat(dataFormat);\n      });\n      util.assert(blockSize > 1, function () {\n        return \"blockSize should be > 1 for depthToSpace, but was: \".concat(blockSize);\n      });\n      var batchSize = x.shape[0];\n      var inputHeight = x.shape[1];\n      var inputWidth = x.shape[2];\n      var inputDepth = x.shape[3];\n      var outputHeight = inputHeight * blockSize;\n      var outputWidth = inputWidth * blockSize;\n      var outputDepth = inputDepth / (blockSize * blockSize);\n      var xValues = this.readSync(x.dataId);\n      var result = new Float32Array(batchSize * outputHeight * outputWidth * outputDepth);\n      var outputIdx = 0;\n\n      for (var b = 0; b < batchSize; ++b) {\n        for (var h = 0; h < outputHeight; ++h) {\n          var inH = Math.floor(h / blockSize);\n          var offsetH = h % blockSize;\n\n          for (var w = 0; w < outputWidth; ++w) {\n            var inW = Math.floor(w / blockSize);\n            var offsetW = w % blockSize;\n            var offsetD = (offsetH * blockSize + offsetW) * outputDepth;\n\n            for (var d = 0; d < outputDepth; ++d) {\n              var inD = d + offsetD;\n              var inputIdx = inD + inputDepth * (inW + inputWidth * (inH + inputHeight * b));\n              result[outputIdx++] = xValues[inputIdx];\n            }\n          }\n        }\n      }\n\n      return tf.tensor4d(result, [batchSize, outputHeight, outputWidth, outputDepth]);\n    }\n  }, {\n    key: \"broadcastedBinaryOp\",\n    value: function broadcastedBinaryOp(a, b, dtype, op) {\n      var newShape = backend_util.assertAndGetBroadcastShape(a.shape, b.shape);\n      var result = tf.buffer(newShape, dtype);\n      var aVals = this.readSync(a.dataId);\n      var bVals = this.readSync(b.dataId);\n      var aBroadcastDims = backend_util.getBroadcastDims(a.shape, newShape);\n      var bBroadcastDims = backend_util.getBroadcastDims(b.shape, newShape);\n      var resVals = result.values;\n\n      if (aBroadcastDims.length + bBroadcastDims.length === 0) {\n        for (var i = 0; i < resVals.length; ++i) {\n          resVals[i] = op(aVals[i % aVals.length], bVals[i % bVals.length]);\n        }\n      } else {\n        var aBuf = this.bufferSync(a);\n        var bBuf = this.bufferSync(b);\n\n        var _loop2 = function _loop2(_i3) {\n          var loc = result.indexToLoc(_i3);\n          var aLoc = loc.slice(-a.rank);\n          aBroadcastDims.forEach(function (d) {\n            return aLoc[d] = 0;\n          });\n          var aIndex = aBuf.locToIndex(aLoc);\n          var bLoc = loc.slice(-b.rank);\n          bBroadcastDims.forEach(function (d) {\n            return bLoc[d] = 0;\n          });\n          var bIndex = bBuf.locToIndex(bLoc);\n          resVals[_i3] = op(aVals[aIndex], bVals[bIndex]);\n        };\n\n        for (var _i3 = 0; _i3 < resVals.length; ++_i3) {\n          _loop2(_i3);\n        }\n      }\n\n      return result.toTensor();\n    }\n  }, {\n    key: \"split\",\n    value: function split(x, sizeSplits, axis) {\n      return _split(x, sizeSplits, axis);\n    }\n  }, {\n    key: \"dispose\",\n    value: function dispose() {}\n  }, {\n    key: \"floatPrecision\",\n    value: function floatPrecision() {\n      return 32;\n    }\n    /** Returns the smallest representable number.  */\n\n  }, {\n    key: \"epsilon\",\n    value: function epsilon() {\n      return _get(_getPrototypeOf(MathBackendCPU.prototype), \"epsilon\", this).call(this);\n    }\n  }, {\n    key: \"cropAndResize\",\n    value: function cropAndResize(images, boxes, boxIndex, cropSize, method, extrapolationValue) {\n      var _images$shape = _slicedToArray(images.shape, 4),\n          batch = _images$shape[0],\n          imageHeight = _images$shape[1],\n          imageWidth = _images$shape[2],\n          numChannels = _images$shape[3];\n\n      var numBoxes = boxes.shape[0];\n\n      var _cropSize = _slicedToArray(cropSize, 2),\n          cropHeight = _cropSize[0],\n          cropWidth = _cropSize[1];\n\n      var output = tf.buffer([numBoxes, cropHeight, cropWidth, numChannels], 'float32');\n      var boxVals = this.readSync(boxes.dataId);\n      var boxIndVals = this.readSync(boxIndex.dataId);\n      var imageVals = this.readSync(images.dataId);\n      var inStride = images.strides; // to calculate flat indexes into image\n\n      var outStride = output.strides; // to calculate flat indexes into output\n      // Reference implementation\n      // tslint:disable-next-line:max-line-length\n      // https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/crop_and_resize_op.cc\n\n      for (var b = 0; b < numBoxes; b++) {\n        var startInd = b * 4;\n        var y1 = boxVals[startInd];\n        var x1 = boxVals[startInd + 1];\n        var y2 = boxVals[startInd + 2];\n        var x2 = boxVals[startInd + 3];\n        var bInd = boxIndVals[b];\n\n        if (bInd >= batch) {\n          continue;\n        }\n\n        var heightScale = cropHeight > 1 ? (y2 - y1) * (imageHeight - 1) / (cropHeight - 1) : 0;\n        var widthScale = cropWidth > 1 ? (x2 - x1) * (imageWidth - 1) / (cropWidth - 1) : 0;\n\n        for (var y = 0; y < cropHeight; y++) {\n          var yInd = cropHeight > 1 ? y1 * (imageHeight - 1) + y * heightScale : 0.5 * (y1 + y2) * (imageHeight - 1);\n\n          if (yInd < 0 || yInd > imageHeight - 1) {\n            for (var x = 0; x < cropWidth; x++) {\n              for (var c = 0; c < numChannels; c++) {\n                var ind = c + x * outStride[2] + y * outStride[1] + b * outStride[0];\n                output.values[ind] = extrapolationValue;\n              }\n            }\n\n            continue;\n          }\n\n          if (method === 'bilinear') {\n            var topInd = Math.floor(yInd);\n            var bottomInd = Math.ceil(yInd);\n            var yLerp = yInd - topInd;\n\n            for (var _x3 = 0; _x3 < cropWidth; _x3++) {\n              var xInd = cropWidth > 1 ? x1 * (imageWidth - 1) + _x3 * widthScale : 0.5 * (x1 + x2) * (imageWidth - 1);\n\n              if (xInd < 0 || xInd > imageWidth - 1) {\n                for (var _c = 0; _c < numChannels; _c++) {\n                  var _ind = _c + _x3 * outStride[2] + y * outStride[1] + b * outStride[0];\n\n                  output.values[_ind] = extrapolationValue;\n                }\n\n                continue;\n              }\n\n              var leftInd = Math.floor(xInd);\n              var rightInd = Math.ceil(xInd);\n              var xLerp = xInd - leftInd;\n\n              for (var _c2 = 0; _c2 < numChannels; _c2++) {\n                var _ind2 = _c2 + leftInd * inStride[2] + topInd * inStride[1] + bInd * inStride[0];\n\n                var topLeft = imageVals[_ind2];\n                _ind2 = _c2 + rightInd * inStride[2] + topInd * inStride[1] + bInd * inStride[0];\n                var topRight = imageVals[_ind2];\n                _ind2 = _c2 + leftInd * inStride[2] + bottomInd * inStride[1] + bInd * inStride[0];\n                var bottomLeft = imageVals[_ind2];\n                _ind2 = _c2 + rightInd * inStride[2] + bottomInd * inStride[1] + bInd * inStride[0];\n                var bottomRight = imageVals[_ind2];\n                var top = topLeft + (topRight - topLeft) * xLerp;\n                var bottom = bottomLeft + (bottomRight - bottomLeft) * xLerp;\n                _ind2 = _c2 + _x3 * outStride[2] + y * outStride[1] + b * outStride[0];\n                output.values[_ind2] = top + (bottom - top) * yLerp;\n              }\n            }\n          } else {\n            // method == \"nearest\"\n            for (var _x4 = 0; _x4 < cropWidth; ++_x4) {\n              var _xInd = cropWidth > 1 ? x1 * (imageWidth - 1) + _x4 * widthScale : 0.5 * (x1 + x2) * (imageWidth - 1);\n\n              if (_xInd < 0 || _xInd > imageWidth - 1) {\n                for (var _c3 = 0; _c3 < numChannels; _c3++) {\n                  var _ind3 = _c3 + _x4 * outStride[2] + y * outStride[1] + b * outStride[0];\n\n                  output.values[_ind3] = extrapolationValue;\n                }\n\n                continue;\n              }\n\n              var closestX = Math.round(_xInd);\n              var closestY = Math.round(yInd);\n\n              for (var _c4 = 0; _c4 < numChannels; _c4++) {\n                var inInd = _c4 + closestX * inStride[2] + closestY * inStride[1] + bInd * inStride[0];\n                var outInd = _c4 + _x4 * outStride[2] + y * outStride[1] + b * outStride[0];\n                output.values[outInd] = imageVals[inInd];\n              }\n            }\n          }\n        }\n      }\n\n      return output.toTensor();\n    }\n  }, {\n    key: \"sparseToDense\",\n    value: function sparseToDense(sparseIndices, sparseValues, outputShape, defaultValue) {\n      var _backend_util$calcula = backend_util.calculateShapes(sparseValues, sparseIndices, outputShape),\n          sliceRank = _backend_util$calcula.sliceRank,\n          numUpdates = _backend_util$calcula.numUpdates,\n          sliceSize = _backend_util$calcula.sliceSize,\n          strides = _backend_util$calcula.strides,\n          outputSize = _backend_util$calcula.outputSize;\n\n      var sumDupeIndices = false;\n      return this.scatter(sparseIndices, sparseValues, outputShape, outputSize, sliceSize, numUpdates, sliceRank, strides, defaultValue, sumDupeIndices);\n    }\n  }, {\n    key: \"gatherND\",\n    value: function gatherND(x, indices) {\n      var indicesShape = indices.shape;\n      var sliceRank = indicesShape[indicesShape.length - 1];\n\n      var _backend_util$prepare = backend_util.prepareAndValidate(x, indices),\n          _backend_util$prepare2 = _slicedToArray(_backend_util$prepare, 4),\n          resultShape = _backend_util$prepare2[0],\n          numSlices = _backend_util$prepare2[1],\n          sliceSize = _backend_util$prepare2[2],\n          strides = _backend_util$prepare2[3];\n\n      if (numSlices === 0) {\n        return tf.tensor([], resultShape, x.dtype);\n      }\n\n      var buffer = new TensorBuffer([numSlices, sliceSize], x.dtype);\n      var indicesData = this.readSync(indices.dataId);\n      var xData = this.readSync(x.dataId);\n\n      for (var i = 0; i < numSlices; i++) {\n        var index = [];\n        var flattenIndex = 0;\n\n        for (var j = 0; j < sliceRank; j++) {\n          var dim = indicesData[i * sliceRank + j];\n          flattenIndex += dim * strides[j];\n          index.push(dim);\n        }\n\n        if (flattenIndex < 0 || flattenIndex >= x.size / sliceSize) {\n          throw new Error(\"Invalid indices: \".concat(index, \" does not index into \").concat(x.shape));\n        }\n\n        for (var k = 0; k < sliceSize; k++) {\n          buffer.values[i * sliceSize + k] = xData[flattenIndex * sliceSize + k];\n        }\n      }\n\n      return buffer.toTensor().reshape(resultShape);\n    }\n  }, {\n    key: \"scatterND\",\n    value: function scatterND(indices, updates, shape) {\n      var _backend_util$calcula2 = backend_util.calculateShapes(updates, indices, shape),\n          sliceRank = _backend_util$calcula2.sliceRank,\n          numUpdates = _backend_util$calcula2.numUpdates,\n          sliceSize = _backend_util$calcula2.sliceSize,\n          strides = _backend_util$calcula2.strides,\n          outputSize = _backend_util$calcula2.outputSize;\n\n      var defaultValue = tf.scalar(0);\n      var sumDupeIndices = true;\n      return this.scatter(indices, updates, shape, outputSize, sliceSize, numUpdates, sliceRank, strides, defaultValue, sumDupeIndices);\n    }\n  }, {\n    key: \"onesLike\",\n    value: function onesLike(x) {\n      if (x.dtype === 'string') {\n        throw new Error('onesLike is not supported for string tensors');\n      } else {\n        // TODO(lina128): Use fill kernel directly once this kernel is\n        // modularized.\n        return tf.fill(x.shape, 1, x.dtype);\n      }\n    }\n  }, {\n    key: \"zerosLike\",\n    value: function zerosLike(x) {\n      var values = util.getArrayFromDType(x.dtype, util.sizeFromShape(x.shape));\n      return this.makeOutput(values, x.shape, x.dtype);\n    }\n  }, {\n    key: \"linspace\",\n    value: function linspace(start, stop, num) {\n      return backend_util.linspaceImpl(start, stop, num);\n    }\n  }, {\n    key: \"scatter\",\n    value: function scatter(indices, updates, shape, outputSize, sliceSize, numUpdates, sliceRank, strides, defaultValue, sumDupeIndices) {\n      var flattenShape = [outputSize / sliceSize, sliceSize];\n      var indicesData = this.readSync(indices.dataId);\n      var updatesData = this.readSync(updates.dataId);\n\n      if (outputSize === 0) {\n        return tf.tensor([], shape, updates.dtype);\n      }\n\n      var buffer = new TensorBuffer(flattenShape, updates.dtype);\n      buffer.values.fill(this.readSync(defaultValue.dataId)[0]);\n\n      for (var i = 0; i < numUpdates; i++) {\n        var index = [];\n        var flattenIndex = 0;\n\n        for (var j = 0; j < sliceRank; j++) {\n          var dim = indicesData[i * sliceRank + j];\n          index.push(dim);\n          flattenIndex += dim * strides[j];\n        }\n\n        if (flattenIndex < 0 || flattenIndex >= outputSize / sliceSize) {\n          throw new Error(\"Invalid indices: \".concat(index, \" does not index into \").concat(shape));\n        }\n\n        for (var k = 0; k < sliceSize; k++) {\n          if (sumDupeIndices) {\n            buffer.values[flattenIndex * sliceSize + k] += updatesData[i * sliceSize + k];\n          } else {\n            buffer.values[flattenIndex * sliceSize + k] = updates.rank === 0 ? updatesData[0] : updatesData[i * sliceSize + k];\n          }\n        }\n      }\n\n      return buffer.toTensor().reshape(shape);\n    }\n  }]);\n\n  return MathBackendCPU;\n}(KernelBackend);","map":null,"metadata":{},"sourceType":"module"}